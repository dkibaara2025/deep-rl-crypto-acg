{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOqFdhgMwUKNqZT98ZDU0LH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VsdXzSlMTZFV","executionInfo":{"status":"ok","timestamp":1761561541451,"user_tz":-180,"elapsed":24920,"user":{"displayName":"David Kibaara","userId":"07870126442739887481"}},"outputId":"44fbb6e1-2a58-46ec-9ad8-3cbd239f8c72"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import os, re, json, glob\n","from dataclasses import dataclass, asdict\n","import numpy as np\n","import pandas as pd\n","\n","# -----------------------------\n","# 0) Reproducibility & folders\n","# -----------------------------\n","np.random.seed(1337)\n","BASE_DIR = \"/content\"\n","EXPORT_DIRS = [f\"{BASE_DIR}/export/tables\", f\"{BASE_DIR}/export/figures\", f\"{BASE_DIR}/export/logs\"]\n","for d in EXPORT_DIRS:\n","    os.makedirs(d, exist_ok=True)\n","\n","# -----------------------------\n","# 1) Minimal experiment config\n","# -----------------------------\n","@dataclass\n","class ExpConfig:\n","    assets: tuple = tuple([\"BTC\",\"ETH\",\"BNB\",\"XRP\",\"ADA\",\"SOL\",\"DOGE\",\"TRX\",\"DOT\",\"LTC\"])\n","    freq: str = \"1D\"\n","    tz: str = \"UTC\"\n","    t_train_end: str = \"2022-12-31\"\n","    t_val_end: str = \"2023-12-31\"\n","    t_test_end: str = \"2025-09-30\"\n","    min_rows_per_asset: int = 500\n","    expect_cols: tuple = tuple([\"timestamp\",\"symbol\",\"open\",\"high\",\"low\",\"close\",\"volume\"])\n","    data_paths: tuple = tuple([\n","        \"/content/drive/My Drive/PHD/Sep 2025/Dataset/archive (9)/*.csv\"\n","    ])\n","\n","CFG = ExpConfig()\n","print(\"Experiment Config:\\n\", json.dumps(asdict(CFG), indent=2))\n","\n","# -----------------------------\n","# 2) Helpers\n","# -----------------------------\n","def _parse_timestamp(s):\n","    try:\n","        return pd.to_datetime(s, utc=True, infer_datetime_format=True)\n","    except Exception:\n","        return pd.NaT\n","\n","def _infer_symbol_from_filename(path, fallback=\"UNK\"):\n","    stem = os.path.splitext(os.path.basename(path))[0]\n","    tokens = re.findall(r'[A-Z0-9]{2,10}', stem.upper())\n","    return (tokens[0] if tokens else fallback)\n","\n","def _standardize_cols(df: pd.DataFrame) -> pd.DataFrame:\n","    df = df.rename(columns={c: c.lower().strip() for c in df.columns})\n","    alias = {'date':'timestamp','datetime':'timestamp','time':'timestamp',\n","             'asset':'symbol','ticker':'symbol','coin':'symbol'}\n","    for k,v in alias.items():\n","        if k in df.columns:\n","            df = df.rename(columns={k:v})\n","    for k in ['close','adj_close','close_price','price']:\n","        if k in df.columns:\n","            df = df.rename(columns={k:'close'})\n","            break\n","    for need in ['timestamp','symbol','open','high','low','close','volume']:\n","        if need not in df.columns:\n","            if need in ['open','high','low','volume']:\n","                df[need] = np.nan\n","            else:\n","                raise ValueError(f\"Missing required column: {need}\")\n","    return df\n","\n","def _to_long(df0: pd.DataFrame, path: str) -> pd.DataFrame:\n","    lc = [c.lower() for c in df0.columns]\n","    if 'symbol' in lc:\n","        return df0\n","    ts_candidates = [c for c in df0.columns if c.lower() in ['timestamp','date','datetime','time']]\n","    if not ts_candidates:\n","        raise ValueError(f\"Cannot infer timestamp column in: {path}\")\n","    ts_col = ts_candidates[0]\n","    df0 = df0.rename(columns={ts_col:'timestamp'})\n","    non_ts = [c for c in df0.columns if c != 'timestamp']\n","    ohlcv_set = {'open','high','low','close','volume'}\n","    if not set([c.lower() for c in non_ts]).issubset(ohlcv_set):\n","        vname = '_melt_value_'\n","        df0 = df0.melt(id_vars='timestamp', var_name='symbol', value_name=vname)\n","        df0 = df0.rename(columns={vname:'close'})\n","        return df0\n","    sym = _infer_symbol_from_filename(path)\n","    df0['symbol'] = sym\n","    return df0\n","\n","def load_any(paths) -> pd.DataFrame:\n","    if not paths:\n","        raise SystemExit(\"Set CFG.data_paths to your CSV path(s).\")\n","    files = []\n","    for p in paths:\n","        files.extend(glob.glob(p))\n","    if not files:\n","        raise SystemExit(f\"No files matched: {paths}\")\n","    frames = []\n","    for f in files:\n","        df0 = pd.read_csv(f)\n","        df0 = _to_long(df0, f)\n","        df0 = _standardize_cols(df0)\n","        frames.append(df0)\n","    df = pd.concat(frames, ignore_index=True)\n","    df['timestamp'] = df['timestamp'].apply(_parse_timestamp)\n","    df = df.dropna(subset=['timestamp','symbol','close'])\n","    df['symbol'] = df['symbol'].astype(str).str.upper().str.replace(r'[^A-Z0-9]', '', regex=True)\n","    df = df.sort_values('timestamp')\n","    return df\n","\n","def resample_ohlcv(df, freq):\n","    df = df.set_index('timestamp').sort_index()\n","    agg = {'open':'first','high':'max','low':'min','close':'last','volume':'sum'}\n","    out = (df.groupby('symbol')\n","             .apply(lambda g: g.resample(freq).agg(agg).dropna(subset=['close']))\n","             .reset_index())\n","    return out\n","\n","# -----------------------------\n","# 3) Load & filter\n","# -----------------------------\n","raw = load_any(CFG.data_paths)\n","\n","asset_counts = raw['symbol'].value_counts()\n","wanted = [a for a in CFG.assets if a in asset_counts.index]\n","if len(wanted) < len(CFG.assets):\n","    for a in asset_counts.index:\n","        if a not in wanted and len(wanted) < len(CFG.assets):\n","            wanted.append(a)\n","data = raw[raw['symbol'].isin(wanted)].copy()\n","\n","data = resample_ohlcv(data, CFG.freq)\n","\n","keep_assets = [a for a, g in data.groupby('symbol') if len(g) >= CFG.min_rows_per_asset]\n","data = data[data['symbol'].isin(keep_assets)]\n","print(f\"Kept assets ({len(keep_assets)}): {sorted(keep_assets)}\")\n","\n","# -----------------------------\n","# 4) Temporal splits\n","# -----------------------------\n","t_train_end = pd.Timestamp(CFG.t_train_end, tz='UTC')\n","t_val_end   = pd.Timestamp(CFG.t_val_end, tz='UTC')\n","t_test_end  = pd.Timestamp(CFG.t_test_end, tz='UTC')\n","\n","def _label_split(ts):\n","    if ts <= t_train_end: return \"train\"\n","    if ts <= t_val_end:   return \"val\"\n","    if ts <= t_test_end:  return \"test\"\n","    return \"holdout_future\"\n","\n","data['split'] = data['timestamp'].apply(_label_split)\n","data = data[data['split'] != \"holdout_future\"]\n","\n","# -----------------------------\n","# 5) Summary & exports\n","# -----------------------------\n","summary = {\n","    \"date_range\": {\n","        \"min\": str(data['timestamp'].min()),\n","        \"max\": str(data['timestamp'].max()),\n","        \"freq\": CFG.freq\n","    },\n","    \"assets\": sorted(data['symbol'].unique().tolist()),\n","    \"rows_total\": int(len(data)),\n","    \"rows_by_split\": data['split'].value_counts().to_dict(),\n","    \"rows_by_asset\": data.groupby('symbol').size().sort_values(ascending=False).to_dict(),\n","    \"missing_by_col\": {c:int(data[c].isna().sum()) for c in ['open','high','low','close','volume']},\n","}\n","print(\"\\n=== DATASET SUMMARY ===\")\n","print(json.dumps(summary, indent=2))\n","\n","data_out = f\"{BASE_DIR}/export/tables/dataset_long_{CFG.freq}.csv\"\n","json_out = f\"{BASE_DIR}/export/tables/dataset_summary.json\"\n","data.to_csv(data_out, index=False)\n","with open(json_out, \"w\") as f:\n","    json.dump(summary, f, indent=2)\n","print(f\"\\nSaved: {data_out}\")\n","print(f\"Saved: {json_out}\")\n","\n","pivot = (data\n","         .groupby(['symbol','split'])\n","         .size()\n","         .unstack(fill_value=0)\n","         .reset_index()\n","         .sort_values(by=['test','val','train'], ascending=False))\n","pivot_out = f\"{BASE_DIR}/export/tables/coverage_by_asset_split.csv\"\n","pivot.to_csv(pivot_out, index=False)\n","print(f\"Saved: {pivot_out}\")\n","print(pivot.head(20))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5IOfC-JiQKv0","executionInfo":{"status":"ok","timestamp":1761562601498,"user_tz":-180,"elapsed":566188,"user":{"displayName":"David Kibaara","userId":"07870126442739887481"}},"outputId":"f3639c67-3c58-41ae-83d1-aafaf2860b3b"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Experiment Config:\n"," {\n","  \"assets\": [\n","    \"BTC\",\n","    \"ETH\",\n","    \"BNB\",\n","    \"XRP\",\n","    \"ADA\",\n","    \"SOL\",\n","    \"DOGE\",\n","    \"TRX\",\n","    \"DOT\",\n","    \"LTC\"\n","  ],\n","  \"freq\": \"1D\",\n","  \"tz\": \"UTC\",\n","  \"t_train_end\": \"2022-12-31\",\n","  \"t_val_end\": \"2023-12-31\",\n","  \"t_test_end\": \"2025-09-30\",\n","  \"min_rows_per_asset\": 500,\n","  \"expect_cols\": [\n","    \"timestamp\",\n","    \"symbol\",\n","    \"open\",\n","    \"high\",\n","    \"low\",\n","    \"close\",\n","    \"volume\"\n","  ],\n","  \"data_paths\": [\n","    \"/content/drive/My Drive/PHD/Sep 2025/Dataset/archive (9)/*.csv\"\n","  ]\n","}\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1906947698.py:41: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n","  return pd.to_datetime(s, utc=True, infer_datetime_format=True)\n","/tmp/ipython-input-1906947698.py:114: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  .apply(lambda g: g.resample(freq).agg(agg).dropna(subset=['close']))\n"]},{"output_type":"stream","name":"stdout","text":["Kept assets (5): ['CLOSE', 'HIGH', 'LOW', 'OPEN', 'TICKER']\n","\n","=== DATASET SUMMARY (for §4.1 paste) ===\n","{\n","  \"date_range\": {\n","    \"min\": \"2010-07-17 00:00:00+00:00\",\n","    \"max\": \"2025-09-30 00:00:00+00:00\",\n","    \"freq\": \"1D\"\n","  },\n","  \"assets\": [\n","    \"CLOSE\",\n","    \"HIGH\",\n","    \"LOW\",\n","    \"OPEN\",\n","    \"TICKER\"\n","  ],\n","  \"rows_total\": 27535,\n","  \"rows_by_split\": {\n","    \"train\": 22755,\n","    \"test\": 2955,\n","    \"val\": 1825\n","  },\n","  \"rows_by_asset\": {\n","    \"CLOSE\": 5507,\n","    \"HIGH\": 5507,\n","    \"LOW\": 5507,\n","    \"OPEN\": 5507,\n","    \"TICKER\": 5507\n","  },\n","  \"missing_by_col\": {\n","    \"open\": 27535,\n","    \"high\": 27535,\n","    \"low\": 27535,\n","    \"close\": 0,\n","    \"volume\": 0\n","  }\n","}\n","\n","Saved: /content/export/tables/dataset_long_1D.csv\n","Saved: /content/export/tables/dataset_summary.json\n","Saved: /content/export/tables/coverage_by_asset_split.csv\n","split  symbol  test  train  val\n","0       CLOSE   591   4551  365\n","1        HIGH   591   4551  365\n","2         LOW   591   4551  365\n","3        OPEN   591   4551  365\n","4      TICKER   591   4551  365\n"]}]},{"cell_type":"code","source":["!pip -q install torchinfo torchviz > /dev/null\n","\n","import os, json, warnings\n","from dataclasses import dataclass, asdict\n","from typing import Dict, List, Tuple\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torchinfo import summary as torch_summary\n","\n","try:\n","    from torchviz import make_dot\n","    TORCHVIZ_OK = True\n","except Exception:\n","    TORCHVIZ_OK = False\n","\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","\n","BASE_DIR = \"/content\"\n","TBL_DIR  = f\"{BASE_DIR}/export/tables\"\n","FIG_DIR  = f\"{BASE_DIR}/export/figures\"\n","os.makedirs(TBL_DIR, exist_ok=True)\n","os.makedirs(FIG_DIR, exist_ok=True)\n","\n","@dataclass\n","class ExpConfig:\n","    ds_csv: str = f\"{BASE_DIR}/export/tables/dataset_long_1D.csv\"\n","    assets_max: int = 5\n","    horizons: List[int] = (1, 3, 7)\n","    win: int = 64\n","    sma_window: int = 5\n","    batch_size: int = 128\n","    epochs: int = 10\n","    patience: int = 3\n","    lr: float = 1e-3\n","    lstm_hidden: int = 64\n","    lstm_layers: int = 2\n","    lstm_dropout: float = 0.1\n","    spline_K: int = 16\n","    seed: int = 1337\n","    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","CFG = ExpConfig()\n","np.random.seed(CFG.seed)\n","torch.manual_seed(CFG.seed)\n","print(\"Baseline Config\\n\", json.dumps(asdict(CFG), indent=2))\n","\n","df = pd.read_csv(CFG.ds_csv, parse_dates=[\"timestamp\"])\n","assert {\"timestamp\",\"symbol\",\"close\",\"split\"}.issubset(df.columns)\n","df[\"close\"] = pd.to_numeric(df[\"close\"], errors=\"coerce\")\n","df = df.dropna(subset=[\"close\"]).reset_index(drop=True)\n","\n","top_assets = (df.groupby(\"symbol\").size()\n","              .sort_values(ascending=False)\n","              .head(CFG.assets_max).index.tolist())\n","df = df[df[\"symbol\"].isin(top_assets)].copy()\n","\n","scalers: Dict[str, Tuple[float,float]] = {}\n","for a, g in df[df[\"split\"]==\"train\"].groupby(\"symbol\"):\n","    mu = float(g[\"close\"].mean())\n","    sd = float(g[\"close\"].std(ddof=0))\n","    scalers[a] = (mu, sd if sd > 0 else 1.0)\n","\n","def zscore(a, x):\n","    mu, sd = scalers[a]\n","    return (x - mu) / sd\n","\n","def make_xy(series: pd.Series, horizon: int, win: int) -> Tuple[np.ndarray,np.ndarray]:\n","    x_list, y_list = [], []\n","    vals = series.values\n","    for t in range(win-1, len(vals)-horizon):\n","        x = vals[t-win+1:t+1]\n","        y = vals[t+horizon]\n","        x_list.append(x.astype(np.float32))\n","        y_list.append(np.float32(y))\n","    return np.stack(x_list), np.stack(y_list)\n","\n","class SeqDataset(Dataset):\n","    def __init__(self, X, Y):\n","        self.X = torch.from_numpy(X)[:, :, None]\n","        self.Y = torch.from_numpy(Y)[:, None]\n","    def __len__(self): return self.X.shape[0]\n","    def __getitem__(self, i): return self.X[i], self.Y[i]\n","\n","data_xy = {}\n","coverage = []\n","for a in top_assets:\n","    g = df[df[\"symbol\"]==a].sort_values(\"timestamp\").copy()\n","    if a not in scalers:\n","        continue\n","    z = g[\"close\"].map(lambda v: zscore(a, v))\n","    g = g.assign(z=z.values)\n","    for split in [\"train\",\"val\",\"test\"]:\n","        gz = g[g[\"split\"]==split][\"z\"]\n","        for h in CFG.horizons:\n","            if len(gz) >= CFG.win + h + 1:\n","                X, Y = make_xy(gz.reset_index(drop=True), h, CFG.win)\n","                data_xy[(a, split, h)] = (X, Y)\n","                coverage.append((a, split, h, len(Y)))\n","            else:\n","                coverage.append((a, split, h, 0))\n","\n","pd.DataFrame(coverage, columns=[\"asset\",\"split\",\"horizon\",\"samples\"]).to_csv(\n","    f\"{TBL_DIR}/coverage_windows.csv\", index=False\n",")\n","print(\"Saved:\", f\"{TBL_DIR}/coverage_windows.csv\")\n","\n","def predict_naive(x_batch):\n","    return x_batch[:, -1, 0:1]\n","\n","def predict_sma(x_batch, k=5):\n","    xs = x_batch[:, -k:, 0]\n","    return xs.mean(dim=1, keepdim=True)\n","\n","class LSTMForecast(nn.Module):\n","    def __init__(self, hidden=64, layers=2, dropout=0.1):\n","        super().__init__()\n","        self.lstm = nn.LSTM(input_size=1, hidden_size=hidden,\n","                            num_layers=layers, dropout=dropout, batch_first=True)\n","        self.head = nn.Sequential(nn.Linear(hidden, hidden), nn.ReLU(), nn.Linear(hidden, 1))\n","    def forward(self, x):\n","        out, _ = self.lstm(x)\n","        last = out[:, -1, :]\n","        return self.head(last)\n","\n","class TriangularSpline(nn.Module):\n","    def __init__(self, K=16, xmin=-4.0, xmax=4.0):\n","        super().__init__()\n","        self.K = K\n","        centers = torch.linspace(xmin, xmax, K)\n","        self.register_buffer(\"centers\", centers)\n","        self.delta = (xmax - xmin) / (K - 1 + 1e-6)\n","    def forward(self, x):\n","        B, W, _ = x.shape\n","        xexp = x.expand(-1, -1, self.K)\n","        cexp = self.centers.view(1,1,-1).expand(B, W, -1)\n","        return torch.relu(1.0 - torch.abs((xexp - cexp) / self.delta))\n","\n","class KANForecast(nn.Module):\n","    def __init__(self, K=16):\n","        super().__init__()\n","        self.spline = TriangularSpline(K=K)\n","        self.head = nn.Sequential(nn.Linear(K, K), nn.ReLU(), nn.Linear(K, 1))\n","    def forward(self, x):\n","        phi = self.spline(x)\n","        pooled = phi.mean(dim=1)\n","        return self.head(pooled)\n","\n","def count_params(m: nn.Module) -> int:\n","    return sum(p.numel() for p in m.parameters() if p.requires_grad)\n","\n","def train_model(model, train_loader, val_loader, epochs, lr, patience, device):\n","    model = model.to(device)\n","    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n","    loss_fn = nn.MSELoss()\n","    best_state, best_val = None, float(\"inf\")\n","    hist = {\"train\": [], \"val\": []}\n","    patience_left = patience\n","    for _ in range(1, epochs+1):\n","        model.train()\n","        tr_loss = 0.0\n","        for xb, yb in train_loader:\n","            xb, yb = xb.to(device), yb.to(device)\n","            opt.zero_grad()\n","            yhat = model(xb)\n","            loss = loss_fn(yhat, yb)\n","            loss.backward()\n","            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","            opt.step()\n","            tr_loss += loss.item() * xb.size(0)\n","        tr_loss /= max(1, len(train_loader.dataset))\n","        model.eval()\n","        va_loss = 0.0\n","        with torch.no_grad():\n","            for xb, yb in val_loader:\n","                xb, yb = xb.to(device), yb.to(device)\n","                yhat = model(xb)\n","                va_loss += loss_fn(yhat, yb).item() * xb.size(0)\n","        va_loss /= max(1, len(val_loader.dataset))\n","        hist[\"train\"].append(tr_loss); hist[\"val\"].append(va_loss)\n","        if va_loss + 1e-9 < best_val:\n","            best_val = va_loss\n","            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n","            patience_left = patience\n","        else:\n","            patience_left -= 1\n","            if patience_left <= 0:\n","                break\n","    if best_state is not None:\n","        model.load_state_dict(best_state)\n","    return model.cpu(), hist\n","\n","def mae(y, yhat): return float(np.mean(np.abs(y - yhat)))\n","def rmse(y, yhat): return float(np.sqrt(np.mean((y - yhat)**2)))\n","def smape(y, yhat, eps=1e-8):\n","    denom = (np.abs(y) + np.abs(yhat) + eps) / 2.0\n","    return float(np.mean(np.abs(y - yhat) / denom) * 100.0)\n","def dir_acc(y_prev, y_true, yhat):\n","    s_true = np.sign(y_true - y_prev)\n","    s_pred = np.sign(yhat - y_prev)\n","    return float(np.mean((s_true == s_pred).astype(np.float32)))\n","\n","results = []\n","model_summaries = []\n","\n","for a in top_assets:\n","    for h in CFG.horizons:\n","        key_tr, key_va, key_te = (a, \"train\", h), (a, \"val\", h), (a, \"test\", h)\n","        if key_tr not in data_xy or key_va not in data_xy or key_te not in data_xy:\n","            continue\n","        Xtr, Ytr = data_xy[key_tr]\n","        Xva, Yva = data_xy[key_va]\n","        Xte, Yte = data_xy[key_te]\n","        ds_tr, ds_va, ds_te = SeqDataset(Xtr, Ytr), SeqDataset(Xva, Yva), SeqDataset(Xte, Yte)\n","        dl_tr = DataLoader(ds_tr, batch_size=CFG.batch_size, shuffle=True)\n","        dl_va = DataLoader(ds_va, batch_size=CFG.batch_size, shuffle=False)\n","        dl_te = DataLoader(ds_te, batch_size=CFG.batch_size, shuffle=False)\n","\n","        with torch.no_grad():\n","            xvb = ds_te.X\n","            y_prev = xvb[:, -1, 0].numpy()\n","            y_true = ds_te.Y[:, 0].numpy()\n","            y_naive = predict_naive(ds_te.X).numpy()[:,0]\n","            y_sma   = predict_sma(ds_te.X, k=CFG.sma_window).numpy()[:,0]\n","\n","        results.append(dict(model=\"Naive\", asset=a, horizon=h,\n","                            MAE=mae(y_true, y_naive), RMSE=rmse(y_true, y_naive),\n","                            sMAPE=smape(y_true, y_naive), DA=dir_acc(y_prev, y_true, y_naive)))\n","        results.append(dict(model=\"SMA\", asset=a, horizon=h,\n","                            MAE=mae(y_true, y_sma), RMSE=rmse(y_true, y_sma),\n","                            sMAPE=smape(y_true, y_sma), DA=dir_acc(y_prev, y_true, y_sma)))\n","\n","        lstm = LSTMForecast(hidden=CFG.lstm_hidden, layers=CFG.lstm_layers, dropout=CFG.lstm_dropout)\n","        lstm, hist_lstm = train_model(lstm, dl_tr, dl_va, CFG.epochs, CFG.lr, CFG.patience, CFG.device)\n","        with torch.no_grad():\n","            yhat_lstm = []\n","            for xb, _ in dl_te:\n","                yhat_lstm.append(lstm(xb.to(CFG.device)).detach().cpu().numpy())\n","            yhat_lstm = np.concatenate(yhat_lstm, axis=0)[:,0]\n","        results.append(dict(model=\"LSTM\", asset=a, horizon=h,\n","                            MAE=mae(y_true, yhat_lstm), RMSE=rmse(y_true, yhat_lstm),\n","                            sMAPE=smape(y_true, yhat_lstm), DA=dir_acc(y_prev, y_true, yhat_lstm)))\n","\n","        kan = KANForecast(K=CFG.spline_K)\n","        kan, hist_kan = train_model(kan, dl_tr, dl_va, CFG.epochs, CFG.lr, CFG.patience, CFG.device)\n","        with torch.no_grad():\n","            yhat_kan = []\n","            for xb, _ in dl_te:\n","                yhat_kan.append(kan(xb.to(CFG.device)).detach().cpu().numpy())\n","            yhat_kan = np.concatenate(yhat_kan, axis=0)[:,0]\n","        results.append(dict(model=\"KAN-spline\", asset=a, horizon=h,\n","                            MAE=mae(y_true, yhat_kan), RMSE=rmse(y_true, yhat_kan),\n","                            sMAPE=smape(y_true, yhat_kan), DA=dir_acc(y_prev, y_true, yhat_kan)))\n","\n","        def plot_loss(hist, title, fname):\n","            plt.figure(figsize=(6,4))\n","            plt.plot(hist[\"train\"], label=\"train\")\n","            plt.plot(hist[\"val\"], label=\"val\")\n","            plt.xlabel(\"Epoch\"); plt.ylabel(\"MSE loss\"); plt.title(title); plt.legend()\n","            plt.tight_layout()\n","            pth = os.path.join(FIG_DIR, fname)\n","            plt.savefig(pth, dpi=150); plt.close()\n","            return pth\n","\n","        plot_loss(hist_lstm, f\"LSTM Loss — {a}, H={h}\", f\"loss_lstm_{a}_H{h}.png\")\n","        plot_loss(hist_kan,  f\"KAN-spline Loss — {a}, H={h}\", f\"loss_kan_{a}_H{h}.png\")\n","\n","        Nplot = min(400, len(y_true))\n","        def plot_trace(y_true, yhat, title, fname):\n","            plt.figure(figsize=(8,4))\n","            plt.plot(y_true[:Nplot], label=\"true\")\n","            plt.plot(yhat[:Nplot],  label=\"pred\")\n","            plt.xlabel(\"Test sample index\"); plt.ylabel(\"Standardized close\")\n","            plt.title(title); plt.legend(); plt.tight_layout()\n","            pth = os.path.join(FIG_DIR, fname)\n","            plt.savefig(pth, dpi=150); plt.close()\n","            return pth\n","\n","        plot_trace(y_true, yhat_lstm, f\"LSTM Test Trace — {a}, H={h}\", f\"trace_lstm_{a}_H{h}.png\")\n","        plot_trace(y_true, yhat_kan,  f\"KAN-spline Test Trace — {a}, H={h}\", f\"trace_kan_{a}_H{h}.png\")\n","\n","        def plot_residuals(y_true, yhat, title, fname):\n","            plt.figure(figsize=(6,4))\n","            resid = y_true - yhat\n","            plt.hist(resid, bins=40)\n","            plt.xlabel(\"Residual\"); plt.ylabel(\"Count\"); plt.title(title); plt.tight_layout()\n","            pth = os.path.join(FIG_DIR, fname)\n","            plt.savefig(pth, dpi=150); plt.close()\n","            return pth\n","\n","        plot_residuals(y_true, yhat_lstm, f\"Residuals LSTM — {a}, H={h}\", f\"resid_lstm_{a}_H{h}.png\")\n","        plot_residuals(y_true, yhat_kan,  f\"Residuals KAN — {a}, H={h}\", f\"resid_kan_{a}_H{h}.png\")\n","\n","        lstm_params = count_params(lstm)\n","        ms_lstm = torch_summary(lstm, input_size=(1, CFG.win, 1), verbose=0)\n","        with open(os.path.join(TBL_DIR, f\"model_summary_lstm_{a}_H{h}.txt\"), \"w\") as f:\n","            f.write(str(ms_lstm)); f.write(f\"\\nTotal trainable parameters: {lstm_params}\\n\")\n","        kan_params = count_params(kan)\n","        ms_kan = torch_summary(kan, input_size=(1, CFG.win, 1), verbose=0)\n","        with open(os.path.join(TBL_DIR, f\"model_summary_kan_{a}_H{h}.txt\"), \"w\") as f:\n","            f.write(str(ms_kan)); f.write(f\"\\nTotal trainable parameters: {kan_params}\\n\")\n","\n","        model_summaries.append({\"asset\":a,\"horizon\":h,\"model\":\"LSTM\",\"params\":lstm_params})\n","        model_summaries.append({\"asset\":a,\"horizon\":h,\"model\":\"KAN-spline\",\"params\":kan_params})\n","\n","        if TORCHVIZ_OK:\n","            try:\n","                xdummy = torch.randn(1, CFG.win, 1)\n","                make_dot(lstm(xdummy), params=dict(list(lstm.named_parameters()))).render(\n","                    os.path.join(FIG_DIR, f\"graph_lstm_{a}_H{h}\"), format=\"png\", cleanup=True)\n","                make_dot(kan(xdummy), params=dict(list(kan.named_parameters()))).render(\n","                    os.path.join(FIG_DIR, f\"graph_kan_{a}_H{h}\"), format=\"png\", cleanup=True)\n","            except Exception as e:\n","                print(\"torchviz failed for\", a, h, \":\", e)\n","\n","        xx = np.linspace(-4, 4, 400, dtype=np.float32)\n","        with torch.no_grad():\n","            x_t = torch.from_numpy(xx).view(1, -1, 1)\n","            phi = kan.spline(x_t).squeeze(0).numpy()\n","        plt.figure(figsize=(7,4))\n","        for k in range(phi.shape[1]):\n","            plt.plot(xx, phi[:,k])\n","        plt.title(f\"KAN Triangular Bases (K={CFG.spline_K}) — {a}, H={h}\")\n","        plt.xlabel(\"Standardized input\"); plt.ylabel(\"Activation\")\n","        plt.tight_layout()\n","        plt.savefig(os.path.join(FIG_DIR, f\"kan_bases_{a}_H{h}.png\"), dpi=150)\n","        plt.close()\n","\n","res_df = pd.DataFrame(results).sort_values([\"asset\",\"horizon\",\"model\"])\n","res_csv = os.path.join(TBL_DIR, \"baseline_results_per_asset_horizon.csv\")\n","res_df.to_csv(res_csv, index=False)\n","\n","agg_df = (res_df.groupby([\"model\",\"horizon\"])\n","          .agg(MAE=(\"MAE\",\"mean\"), RMSE=(\"RMSE\",\"mean\"), sMAPE=(\"sMAPE\",\"mean\"), DA=(\"DA\",\"mean\"))\n","          .reset_index()\n","          .sort_values([\"horizon\",\"model\"]))\n","agg_csv = os.path.join(TBL_DIR, \"baseline_results_agg_by_horizon.csv\")\n","agg_df.to_csv(agg_csv, index=False)\n","\n","param_df = pd.DataFrame(model_summaries)\n","param_csv = os.path.join(TBL_DIR, \"model_param_counts.csv\")\n","param_df.to_csv(param_csv, index=False)\n","\n","def barplot_param_counts(df, fname):\n","    piv = df.pivot_table(index=[\"model\"], values=\"params\", aggfunc=\"mean\").reset_index()\n","    plt.figure(figsize=(6,4))\n","    plt.bar(piv[\"model\"], piv[\"params\"])\n","    plt.ylabel(\"Parameters (mean across assets & horizons)\")\n","    plt.title(\"Model Parameter Counts\")\n","    plt.tight_layout()\n","    pth = os.path.join(FIG_DIR, fname); plt.savefig(pth, dpi=150); plt.close(); return pth\n","\n","def barplot_metric(df, metric, fname, title):\n","    plt.figure(figsize=(7,4))\n","    labels = sorted(df[\"horizon\"].unique())\n","    models = df[\"model\"].unique().tolist()\n","    width = 0.15\n","    idx = np.arange(len(labels))\n","    for i, m in enumerate(models):\n","        sub = df[df[\"model\"]==m].set_index(\"horizon\").reindex(labels)\n","        plt.bar(idx + i*width, sub[metric].values, width=width, label=m)\n","    plt.xticks(idx + width*(len(models)-1)/2, [f\"H={h}\" for h in labels])\n","    plt.ylabel(metric); plt.title(title); plt.legend()\n","    plt.tight_layout()\n","    pth = os.path.join(FIG_DIR, fname); plt.savefig(pth, dpi=150); plt.close(); return pth\n","\n","barplot_param_counts(param_df, \"param_counts_bar.png\")\n","barplot_metric(agg_df, \"DA\",    \"directional_accuracy_bars.png\", \"Directional Accuracy by Model & Horizon\")\n","barplot_metric(agg_df, \"MAE\",   \"mae_bars.png\", \"MAE by Model & Horizon\")\n","barplot_metric(agg_df, \"RMSE\",  \"rmse_bars.png\",\"RMSE by Model & Horizon\")\n","barplot_metric(agg_df, \"sMAPE\", \"smape_bars.png\",\"sMAPE by Model & Horizon\")\n","\n","print(\"\\nRESULTS SUMMARY\")\n","print(\"Per-asset × horizon results CSV:\", res_csv)\n","print(res_df.head(12).to_string(index=False))\n","print(\"\\nAggregated by horizon (means across assets):\", agg_csv)\n","print(agg_df.to_string(index=False))\n","\n","figs = sorted([os.path.join(FIG_DIR, f) for f in os.listdir(FIG_DIR) if f.endswith(\".png\")])\n","print(\"\\nFIGURES SAVED\")\n","for p in figs[:30]:\n","    print(p)\n","if len(figs) > 30:\n","    print(f\"... and {len(figs)-30} more figures\")\n","\n","print(\"\\nArtifacts:\")\n","print(\" - Model summaries (*.txt) in\", TBL_DIR)\n","print(\" - Parameter counts:\", param_csv)\n","print(\" - Coverage windows:\", f\"{TBL_DIR}/coverage_windows.csv\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w66tTPeISHfi","executionInfo":{"status":"ok","timestamp":1761563938066,"user_tz":-180,"elapsed":471974,"user":{"displayName":"David Kibaara","userId":"07870126442739887481"}},"outputId":"b1bdeef3-0bbb-4485-f91d-096349d9112e"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Baseline Config\n"," {\n","  \"ds_csv\": \"/content/export/tables/dataset_long_1D.csv\",\n","  \"assets_max\": 5,\n","  \"horizons\": [\n","    1,\n","    3,\n","    7\n","  ],\n","  \"win\": 64,\n","  \"sma_window\": 5,\n","  \"batch_size\": 128,\n","  \"epochs\": 10,\n","  \"patience\": 3,\n","  \"lr\": 0.001,\n","  \"lstm_hidden\": 64,\n","  \"lstm_layers\": 2,\n","  \"lstm_dropout\": 0.1,\n","  \"spline_K\": 16,\n","  \"seed\": 1337,\n","  \"device\": \"cpu\"\n","}\n","Saved: /content/export/tables/coverage_windows.csv\n","\n","RESULTS SUMMARY\n","Per-asset × horizon results CSV: /content/export/tables/baseline_results_per_asset_horizon.csv\n","     model asset  horizon      MAE     RMSE      sMAPE       DA\n","KAN-spline CLOSE        1 0.362014 3.041022 199.553375 0.548387\n","      LSTM CLOSE        1 0.395214 3.040620 197.622009 0.544592\n","     Naive CLOSE        1 0.536916 4.310656  30.184805 0.000000\n","       SMA CLOSE        1 0.538268 3.339615  59.478020 0.639469\n","KAN-spline CLOSE        3 0.363296 3.046820 199.423523 0.544762\n","      LSTM CLOSE        3 0.363687 3.048740 161.150421 0.544762\n","     Naive CLOSE        3 0.540753 4.319302  30.749044 0.000000\n","       SMA CLOSE        3 0.535805 3.340200  59.217781 0.630476\n","KAN-spline CLOSE        7 0.417606 3.055876 196.893387 0.552783\n","      LSTM CLOSE        7 0.373221 3.058179 198.717606 0.554703\n","     Naive CLOSE        7 0.539027 4.311964  30.126255 0.000000\n","       SMA CLOSE        7 0.538646 3.350779  58.881390 0.666027\n","\n","Aggregated by horizon (means across assets): /content/export/tables/baseline_results_agg_by_horizon.csv\n","     model  horizon      MAE     RMSE      sMAPE       DA\n","KAN-spline        1 0.675449 4.886481 194.942509 0.570209\n","      LSTM        1 0.700699 4.883918 194.234871 0.566888\n","     Naive        1 1.158371 6.925767  37.626258 0.000000\n","       SMA        1 1.137369 5.305524  64.427356 0.655598\n","KAN-spline        3 0.674546 4.896242 195.448318 0.561905\n","      LSTM        3 0.686383 4.894893 184.443340 0.560952\n","     Naive        3 1.160192 6.932469  37.581491 0.000476\n","       SMA        3 1.142168 5.317496  64.306325 0.653333\n","KAN-spline        7 0.688681 4.914742 192.496319 0.558061\n","      LSTM        7 0.697420 4.912454 185.853386 0.554703\n","     Naive        7 1.167578 6.960510  37.626885 0.000480\n","       SMA        7 1.144823 5.335130  63.666493 0.666027\n","\n","FIGURES SAVED\n","/content/export/figures/directional_accuracy_bars.png\n","/content/export/figures/graph_kan_CLOSE_H1.png\n","/content/export/figures/graph_kan_CLOSE_H3.png\n","/content/export/figures/graph_kan_CLOSE_H7.png\n","/content/export/figures/graph_kan_HIGH_H1.png\n","/content/export/figures/graph_kan_HIGH_H3.png\n","/content/export/figures/graph_kan_HIGH_H7.png\n","/content/export/figures/graph_kan_LOW_H1.png\n","/content/export/figures/graph_kan_LOW_H3.png\n","/content/export/figures/graph_kan_LOW_H7.png\n","/content/export/figures/graph_kan_OPEN_H1.png\n","/content/export/figures/graph_kan_OPEN_H3.png\n","/content/export/figures/graph_kan_OPEN_H7.png\n","/content/export/figures/graph_lstm_CLOSE_H1.png\n","/content/export/figures/graph_lstm_CLOSE_H3.png\n","/content/export/figures/graph_lstm_CLOSE_H7.png\n","/content/export/figures/graph_lstm_HIGH_H1.png\n","/content/export/figures/graph_lstm_HIGH_H3.png\n","/content/export/figures/graph_lstm_HIGH_H7.png\n","/content/export/figures/graph_lstm_LOW_H1.png\n","/content/export/figures/graph_lstm_LOW_H3.png\n","/content/export/figures/graph_lstm_LOW_H7.png\n","/content/export/figures/graph_lstm_OPEN_H1.png\n","/content/export/figures/graph_lstm_OPEN_H3.png\n","/content/export/figures/graph_lstm_OPEN_H7.png\n","/content/export/figures/kan_bases_CLOSE_H1.png\n","/content/export/figures/kan_bases_CLOSE_H3.png\n","/content/export/figures/kan_bases_CLOSE_H7.png\n","/content/export/figures/kan_bases_HIGH_H1.png\n","/content/export/figures/kan_bases_HIGH_H3.png\n","... and 83 more figures\n","\n","Artifacts:\n"," - Model summaries (*.txt) in /content/export/tables\n"," - Parameter counts: /content/export/tables/model_param_counts.csv\n"," - Coverage windows: /content/export/tables/coverage_windows.csv\n"]}]},{"cell_type":"code","source":["!pip -q install torchinfo torchviz > /dev/null\n","\n","import os, json, math, random, warnings\n","from dataclasses import dataclass, asdict\n","from typing import Dict, Tuple, List\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torchinfo import summary as torch_summary\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","try:\n","    from torchviz import make_dot\n","    TORCHVIZ_OK = True\n","except Exception:\n","    TORCHVIZ_OK = False\n","\n","BASE_DIR = \"/content\"\n","TBL_DIR  = f\"{BASE_DIR}/export/tables\"\n","FIG_DIR  = f\"{BASE_DIR}/export/figures\"\n","LOG_DIR  = f\"{BASE_DIR}/export/logs\"\n","for d in [TBL_DIR, FIG_DIR, LOG_DIR]:\n","    os.makedirs(d, exist_ok=True)\n","\n","@dataclass\n","class DRLConfig:\n","    ds_csv: str = f\"{BASE_DIR}/export/tables/dataset_long_1D.csv\"\n","    assets_max: int = 5\n","    horizons: Tuple[int,...] = (1,3,7)\n","    win: int = 64\n","    batch_size: int = 256\n","    teacher_rounds: int = 200\n","    train_steps_per_pull: int = 1\n","    lr: float = 2e-3\n","    gamma: float = 0.99\n","    entropy_beta: float = 1e-3\n","    aux_sup_weight: float = 1e-1\n","    ucb_c: float = 1.2\n","    seed: int = 1337\n","    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    lstm_hidden: int = 64\n","    lstm_layers: int = 1\n","    lstm_dropout: float = 0.0\n","    eval_n_trace: int = 400\n","\n","DRLCFG = DRLConfig()\n","random.seed(DRLCFG.seed); np.random.seed(DRLCFG.seed); torch.manual_seed(DRLCFG.seed)\n","print(\"§4.3 Config\\n\", json.dumps(asdict(DRLCFG), indent=2))\n","\n","df = pd.read_csv(DRLCFG.ds_csv, parse_dates=[\"timestamp\"])\n","assert {\"timestamp\",\"symbol\",\"close\",\"split\"}.issubset(df.columns)\n","df[\"close\"] = pd.to_numeric(df[\"close\"], errors=\"coerce\")\n","df = df.dropna(subset=[\"close\"]).reset_index(drop=True)\n","\n","top_assets = (df.groupby(\"symbol\").size()\n","              .sort_values(ascending=False)\n","              .head(DRLCFG.assets_max).index.tolist())\n","df = df[df[\"symbol\"].isin(top_assets)].copy()\n","\n","scalers: Dict[str, Tuple[float,float]] = {}\n","for a, g in df[df[\"split\"]==\"train\"].groupby(\"symbol\"):\n","    mu = float(g[\"close\"].mean())\n","    sd = float(g[\"close\"].std(ddof=0))\n","    scalers[a] = (mu, sd if sd > 0 else 1.0)\n","\n","def zscore(a, x):\n","    mu, sd = scalers[a]; return (x - mu) / sd\n","def inv_z(a, z):\n","    mu, sd = scalers[a]; return z*sd + mu\n","\n","df = df.sort_values([\"symbol\",\"timestamp\"]).copy()\n","df[\"z\"] = df.apply(lambda r: zscore(r[\"symbol\"], r[\"close\"]), axis=1)\n","\n","def make_xy(series: pd.Series, horizon: int, win: int):\n","    x_list, y_list, last_list = [], [], []\n","    vals = series.values\n","    for t in range(win-1, len(vals)-horizon):\n","        x = vals[t-win+1:t+1]\n","        y = vals[t+horizon]\n","        last = vals[t]\n","        x_list.append(x.astype(np.float32))\n","        y_list.append(np.float32(y))\n","        last_list.append(np.float32(last))\n","    return np.stack(x_list), np.stack(y_list), np.stack(last_list)\n","\n","data_xy = {}\n","coverage = []\n","for a in top_assets:\n","    g = df[df[\"symbol\"]==a].copy()\n","    for split in [\"train\",\"val\",\"test\"]:\n","        gz = g[g[\"split\"]==split][\"z\"].reset_index(drop=True)\n","        for h in DRLCFG.horizons:\n","            if len(gz) >= DRLCFG.win + h + 1:\n","                X, Y, LAST = make_xy(gz, h, DRLCFG.win)\n","                data_xy[(a, split, h)] = (X, Y, LAST)\n","                coverage.append((a, split, h, len(Y)))\n","            else:\n","                coverage.append((a, split, h, 0))\n","\n","pd.DataFrame(coverage, columns=[\"asset\",\"split\",\"horizon\",\"samples\"]).to_csv(\n","    f\"{TBL_DIR}/acg_coverage_windows.csv\", index=False\n",")\n","\n","arms: List[Tuple[str,int]] = []\n","for a in top_assets:\n","    for h in DRLCFG.horizons:\n","        ok = all(((a, sp, h) in data_xy and data_xy[(a,sp,h)][1].shape[0] > 0) for sp in [\"train\",\"val\",\"test\"])\n","        if ok: arms.append((a,h))\n","assert len(arms) > 0\n","n_arms = len(arms)\n","print(f\"Arms (asset,horizon): {arms}\")\n","\n","class PolicyLSTM(nn.Module):\n","    def __init__(self, hidden=64, layers=1, dropout=0.0):\n","        super().__init__()\n","        self.lstm = nn.LSTM(input_size=1, hidden_size=hidden, num_layers=layers,\n","                            dropout=(dropout if layers>1 else 0.0), batch_first=True)\n","        self.mu = nn.Sequential(nn.Linear(hidden, hidden), nn.ReLU(), nn.Linear(hidden, 1))\n","        self.log_sigma = nn.Parameter(torch.tensor(-0.5))\n","    def forward(self, x):\n","        o, _ = self.lstm(x)\n","        last = o[:, -1, :]\n","        mu = self.mu(last)\n","        log_sigma = self.log_sigma.expand_as(mu)\n","        return mu, log_sigma\n","\n","def count_params(m):\n","    return sum(p.numel() for p in m.parameters() if p.requires_grad)\n","\n","device = torch.device(DRLCFG.device)\n","policy = PolicyLSTM(hidden=DRLCFG.lstm_hidden, layers=DRLCFG.lstm_layers, dropout=DRLCFG.lstm_dropout).to(device)\n","opt = torch.optim.AdamW(policy.parameters(), lr=DRLCFG.lr)\n","\n","class UCB1Teacher:\n","    def __init__(self, n_arms, c=1.2):\n","        self.c = c\n","        self.n = np.zeros(n_arms, dtype=np.int64)\n","        self.mean = np.zeros(n_arms, dtype=np.float64)\n","        self.t = 0\n","    def select(self):\n","        self.t += 1\n","        for i in range(n_arms):\n","            if self.n[i] == 0:\n","                return i\n","        ucb = self.mean + self.c * np.sqrt(2.0 * math.log(self.t) / self.n)\n","        return int(np.argmax(ucb))\n","    def update(self, i, reward):\n","        self.n[i] += 1\n","        self.mean[i] += (reward - self.mean[i]) / self.n[i]\n","\n","teacher = UCB1Teacher(n_arms, DRLCFG.ucb_c)\n","\n","def eval_val_mse(policy: nn.Module, arm_idx: int) -> float:\n","    a, h = arms[arm_idx]\n","    X, Y, _ = data_xy[(a,\"val\",h)]\n","    with torch.no_grad():\n","        xb = torch.from_numpy(X).to(device)[:, :, None]\n","        mu, _ = policy(xb)\n","        yhat = mu.squeeze(1).cpu().numpy()\n","    return float(np.mean((Y - yhat)**2))\n","\n","last_val_mse = np.array([eval_val_mse(policy, i) for i in range(n_arms)], dtype=np.float64)\n","\n","def sample_train_minibatch(arm_idx: int, batch_size: int):\n","    a, h = arms[arm_idx]\n","    X, Y, LAST = data_xy[(a,\"train\",h)]\n","    idx = np.random.randint(0, len(Y), size=(min(batch_size, len(Y)),))\n","    xb = torch.from_numpy(X[idx]).float().to(device)[:, :, None]\n","    yb = torch.from_numpy(Y[idx]).float().to(device)[:, None]\n","    lastb = torch.from_numpy(LAST[idx]).float().to(device)[:, None]\n","    return xb, yb, lastb\n","\n","def mae(y, yhat): return float(np.mean(np.abs(y - yhat)))\n","def rmse(y, yhat): return float(np.sqrt(np.mean((y - yhat)**2)))\n","def smape(y, yhat, eps=1e-8):\n","    denom = (np.abs(y) + np.abs(yhat) + eps) / 2.0\n","    return float(np.mean(np.abs(y - yhat) / denom) * 100.0)\n","def dir_acc(y_prev, y_true, yhat):\n","    s_true = np.sign(y_true - y_prev)\n","    s_pred = np.sign(yhat - y_prev)\n","    return float(np.mean((s_true == s_pred).astype(np.float32)))\n","\n","baseline_reward = 0.0\n","baseline_momentum = 0.9\n","\n","def reinforce_step(xb, yb):\n","    policy.train()\n","    opt.zero_grad()\n","    mu, log_sigma = policy(xb)\n","    sigma = torch.exp(log_sigma)\n","    dist = torch.distributions.Normal(mu, sigma)\n","    a = dist.rsample()\n","    r = - (a - yb)**2\n","    global baseline_reward\n","    avg_r = r.mean().detach()\n","    advantage = r - baseline_reward\n","    baseline_reward = baseline_momentum*baseline_reward + (1.0-baseline_momentum)*avg_r\n","    logp = dist.log_prob(a)\n","    loss_policy = - (logp * advantage.detach()).mean()\n","    loss_entropy = - DRLCFG.entropy_beta * dist.entropy().mean()\n","    loss_aux = DRLCFG.aux_sup_weight * nn.MSELoss()(mu, yb)\n","    loss = loss_policy + loss_entropy + loss_aux\n","    loss.backward()\n","    nn.utils.clip_grad_norm_(policy.parameters(), 1.0)\n","    opt.step()\n","\n","for i in range(n_arms):\n","    xb, yb, _ = sample_train_minibatch(i, DRLCFG.batch_size)\n","    reinforce_step(xb, yb)\n","\n","log = {\n","    \"round\": [], \"arm\": [], \"asset\": [], \"horizon\": [],\n","    \"reward_lp\": [], \"val_mse_before\": [], \"val_mse_after\": [],\n","    \"mean_reward_est\": [], \"pulls_arm\": []\n","}\n","\n","for t in range(1, DRLCFG.teacher_rounds + 1):\n","    i = teacher.select()\n","    a, h = arms[i]\n","    val_before = eval_val_mse(policy, i)\n","    for _ in range(DRLCFG.train_steps_per_pull):\n","        xb, yb, _ = sample_train_minibatch(i, DRLCFG.batch_size)\n","        reinforce_step(xb, yb)\n","    val_after = eval_val_mse(policy, i)\n","    reward_lp = float(val_before - val_after)\n","    teacher.update(i, reward_lp)\n","    last_val_mse[i] = val_after\n","    log[\"round\"].append(t)\n","    log[\"arm\"].append(i)\n","    log[\"asset\"].append(a)\n","    log[\"horizon\"].append(h)\n","    log[\"reward_lp\"].append(reward_lp)\n","    log[\"val_mse_before\"].append(val_before)\n","    log[\"val_mse_after\"].append(val_after)\n","    log[\"mean_reward_est\"].append(float(teacher.mean[i]))\n","    log[\"pulls_arm\"].append(int(teacher.n[i]))\n","\n","log_df = pd.DataFrame(log)\n","log_csv = f\"{TBL_DIR}/acg_teacher_log.csv\"\n","log_df.to_csv(log_csv, index=False)\n","\n","means_per_arm = np.zeros(n_arms)\n","counts_per_arm = np.zeros(n_arms, dtype=np.int64)\n","inst_regret = []\n","for r in range(len(log_df)):\n","    rew = log_df.iloc[r][\"reward_lp\"]\n","    means_snapshot = {int(ai): float(am) for ai, am in zip(log_df[\"arm\"][:r+1], log_df[\"mean_reward_est\"][:r+1])}\n","    best_mean = max(means_snapshot.values()) if means_snapshot else 0.0\n","    inst_regret.append(best_mean - rew)\n","regret = np.cumsum(inst_regret)\n","\n","sel_counts = log_df[\"arm\"].value_counts().sort_index()\n","sel_df = pd.DataFrame({\n","    \"arm_idx\": list(range(n_arms)),\n","    \"asset\": [arms[i][0] for i in range(n_arms)],\n","    \"horizon\": [arms[i][1] for i in range(n_arms)],\n","    \"pulls\": [int(sel_counts.get(i, 0)) for i in range(n_arms)],\n","    \"mean_reward\": [float(teacher.mean[i]) for i in range(n_arms)]\n","})\n","sel_df.to_csv(f\"{TBL_DIR}/acg_selection_counts.csv\", index=False)\n","\n","def savefig(fname):\n","    pth = os.path.join(FIG_DIR, fname)\n","    plt.tight_layout(); plt.savefig(pth, dpi=150); plt.close(); return pth\n","\n","plt.figure(figsize=(8,3))\n","plt.plot(log_df[\"arm\"].values, lw=1)\n","plt.yticks(range(n_arms), [f\"{arms[i][0]}-H{arms[i][1]}\" for i in range(n_arms)])\n","plt.xlabel(\"Round\"); plt.ylabel(\"Selected arm\"); plt.title(\"Bandit arm selection over time (UCB1)\")\n","f_sel_over_time = savefig(\"acg_arm_selection_over_time.png\")\n","\n","labels = [f\"{a}-H{h}\" for a,h in arms]\n","plt.figure(figsize=(6,3))\n","plt.bar(range(n_arms), sel_df[\"pulls\"].values)\n","plt.xticks(range(n_arms), labels, rotation=45, ha=\"right\")\n","plt.ylabel(\"Pulls\"); plt.title(\"Selection counts by arm\")\n","f_sel_counts = savefig(\"acg_selection_counts.png\")\n","\n","plt.figure(figsize=(7,3))\n","plt.plot(pd.Series(log_df[\"reward_lp\"]).rolling(10).mean(), label=\"Rolling-10 mean reward\")\n","plt.axhline(0.0, color=\"black\", lw=1)\n","plt.legend(); plt.title(\"Learning progress (Δ val MSE) — rolling mean\")\n","plt.xlabel(\"Round\"); plt.ylabel(\"Reward (↓MSE)\")\n","f_reward_curve = savefig(\"acg_reward_curve.png\")\n","\n","plt.figure(figsize=(7,3))\n","plt.plot(regret)\n","plt.title(\"Cumulative regret (approx.)\")\n","plt.xlabel(\"Round\"); plt.ylabel(\"Regret\")\n","f_regret = savefig(\"acg_cumulative_regret.png\")\n","\n","plt.figure(figsize=(6,3))\n","plt.bar(range(n_arms), sel_df[\"mean_reward\"].values)\n","plt.xticks(range(n_arms), labels, rotation=45, ha=\"right\")\n","plt.ylabel(\"Mean reward\"); plt.title(\"Per-arm mean learning progress\")\n","f_mean_reward = savefig(\"acg_mean_reward_by_arm.png\")\n","\n","val_after_latest = []\n","for i in range(n_arms):\n","    val_mse = eval_val_mse(policy, i)\n","    val_after_latest.append(val_mse)\n","val_snap_df = pd.DataFrame({\n","    \"arm_idx\": list(range(n_arms)),\n","    \"asset\": [arms[i][0] for i in range(n_arms)],\n","    \"horizon\": [arms[i][1] for i in range(n_arms)],\n","    \"val_mse_final\": val_after_latest,\n","    \"pulls\": sel_df[\"pulls\"].values\n","})\n","val_snap_df.to_csv(f\"{TBL_DIR}/acg_val_mse_final.csv\", index=False)\n","\n","plt.figure(figsize=(6,3))\n","plt.bar(range(n_arms), val_snap_df[\"val_mse_final\"].values)\n","plt.xticks(range(n_arms), labels, rotation=45, ha=\"right\")\n","plt.ylabel(\"Val MSE\"); plt.title(\"Final validation MSE by arm\")\n","f_val_mse_final = savefig(\"acg_val_mse_final_bars.png\")\n","\n","model_params = count_params(policy)\n","ms = torch_summary(policy, input_size=(1, DRLCFG.win, 1), verbose=0)\n","with open(os.path.join(TBL_DIR, \"acg_student_model_summary.txt\"), \"w\") as f:\n","    f.write(str(ms))\n","    f.write(f\"\\nTotal trainable parameters: {model_params}\\n\")\n","\n","if TORCHVIZ_OK:\n","    try:\n","        xdummy = torch.randn(1, DRLCFG.win, 1).to(device)\n","        mu, _ = policy(xdummy)\n","        make_dot(mu, params=dict(list(policy.named_parameters()))).render(\n","            os.path.join(FIG_DIR, \"acg_student_graph\"), format=\"png\", cleanup=True\n","        )\n","    except Exception as e:\n","        print(\"torchviz failed:\", e)\n","\n","results = []\n","for i in range(n_arms):\n","    a, h = arms[i]\n","    Xte, Yte, LASTte = data_xy[(a,\"test\",h)]\n","    with torch.no_grad():\n","        xb = torch.from_numpy(Xte).float().to(device)[:, :, None]\n","        mu, _ = policy(xb)\n","        yhat = mu.squeeze(1).cpu().numpy()\n","    y_true = Yte\n","    y_prev = LASTte\n","    results.append(dict(model=\"ACG-DRL\", asset=a, horizon=h,\n","                        MAE=mae(y_true, yhat), RMSE=rmse(y_true, yhat),\n","                        sMAPE=smape(y_true, yhat), DA=dir_acc(y_prev, y_true, yhat)))\n","\n","res_df = pd.DataFrame(results).sort_values([\"asset\",\"horizon\"])\n","res_csv = f\"{TBL_DIR}/acg_results_per_arm.csv\"\n","res_df.to_csv(res_csv, index=False)\n","\n","agg_df = (res_df.groupby(\"horizon\")\n","          .agg(MAE=(\"MAE\",\"mean\"), RMSE=(\"RMSE\",\"mean\"), sMAPE=(\"sMAPE\",\"mean\"), DA=(\"DA\",\"mean\"))\n","          .reset_index().sort_values(\"horizon\"))\n","agg_csv = f\"{TBL_DIR}/acg_results_agg_by_horizon.csv\"\n","agg_df.to_csv(agg_csv, index=False)\n","\n","print(\"\\n§4.3 RESULTS SUMMARY\")\n","print(\"Arms (asset,horizon):\", arms)\n","print(\"Selection counts & mean rewards CSV:\", f\"{TBL_DIR}/acg_selection_counts.csv\")\n","print(sel_df.to_string(index=False))\n","print(\"\\nTeacher rounds:\", DRLCFG.teacher_rounds, \"| UCB1 c =\", DRLCFG.ucb_c)\n","print(\"Teacher log CSV:\", log_csv)\n","print(\"Cumulative regret plotted in:\", f_regret)\n","print(\"\\nACG-DRL per-arm TEST results CSV:\", res_csv)\n","print(res_df.head(len(res_df)).to_string(index=False))\n","print(\"\\nAggregated by horizon CSV:\", agg_csv)\n","print(agg_df.to_string(index=False))\n","\n","figs = sorted([os.path.join(FIG_DIR, f) for f in os.listdir(FIG_DIR) if f.endswith(\".png\")])\n","print(\"\\nFIGURES SAVED (first 40)\")\n","for p in figs[:40]:\n","    print(p)\n","if len(figs) > 40:\n","    print(f\"... and {len(figs)-40} more figures\")\n","\n","print(\"\\nArtifacts:\")\n","print(\" - Student model summary:\", os.path.join(TBL_DIR, \"acg_student_model_summary.txt\"))\n","print(\" - Coverage windows:\", f\"{TBL_DIR}/acg_coverage_windows.csv\")\n","print(\" - Final val MSE by arm:\", f\"{TBL_DIR}/acg_val_mse_final.csv\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bwY-CjABZMdB","executionInfo":{"status":"ok","timestamp":1761564438130,"user_tz":-180,"elapsed":66461,"user":{"displayName":"David Kibaara","userId":"07870126442739887481"}},"outputId":"9fb2e263-963f-449e-979a-38f07c52606d"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["§4.3 Config\n"," {\n","  \"ds_csv\": \"/content/export/tables/dataset_long_1D.csv\",\n","  \"assets_max\": 5,\n","  \"horizons\": [\n","    1,\n","    3,\n","    7\n","  ],\n","  \"win\": 64,\n","  \"batch_size\": 256,\n","  \"teacher_rounds\": 200,\n","  \"train_steps_per_pull\": 1,\n","  \"lr\": 0.002,\n","  \"gamma\": 0.99,\n","  \"entropy_beta\": 0.001,\n","  \"aux_sup_weight\": 0.1,\n","  \"ucb_c\": 1.2,\n","  \"seed\": 1337,\n","  \"device\": \"cpu\",\n","  \"lstm_hidden\": 64,\n","  \"lstm_layers\": 1,\n","  \"lstm_dropout\": 0.0,\n","  \"eval_n_trace\": 400\n","}\n","Arms (asset,horizon): [('CLOSE', 1), ('CLOSE', 3), ('CLOSE', 7), ('HIGH', 1), ('HIGH', 3), ('HIGH', 7), ('LOW', 1), ('LOW', 3), ('LOW', 7), ('OPEN', 1), ('OPEN', 3), ('OPEN', 7)]\n","\n","§4.3 RESULTS SUMMARY\n","Arms (asset,horizon): [('CLOSE', 1), ('CLOSE', 3), ('CLOSE', 7), ('HIGH', 1), ('HIGH', 3), ('HIGH', 7), ('LOW', 1), ('LOW', 3), ('LOW', 7), ('OPEN', 1), ('OPEN', 3), ('OPEN', 7)]\n","Selection counts & mean rewards CSV: /content/export/tables/acg_selection_counts.csv\n"," arm_idx asset  horizon  pulls  mean_reward\n","       0 CLOSE        1     17     0.000515\n","       1 CLOSE        3     17    -0.000053\n","       2 CLOSE        7     17     0.000397\n","       3  HIGH        1     17    -0.000049\n","       4  HIGH        3     17     0.000226\n","       5  HIGH        7     16    -0.000477\n","       6   LOW        1     16    -0.000169\n","       7   LOW        3     17     0.000147\n","       8   LOW        7     16    -0.000337\n","       9  OPEN        1     17     0.001852\n","      10  OPEN        3     16    -0.000599\n","      11  OPEN        7     17     0.003651\n","\n","Teacher rounds: 200 | UCB1 c = 1.2\n","Teacher log CSV: /content/export/tables/acg_teacher_log.csv\n","Cumulative regret plotted in: /content/export/figures/acg_cumulative_regret.png\n","\n","ACG-DRL per-arm TEST results CSV: /content/export/tables/acg_results_per_arm.csv\n","  model asset  horizon      MAE     RMSE      sMAPE       DA\n","ACG-DRL CLOSE        1 0.351924 3.042058 164.252991 0.548387\n","ACG-DRL CLOSE        3 0.352843 3.047830 164.102844 0.544762\n","ACG-DRL CLOSE        7 0.354405 3.059268 163.935196 0.554703\n","ACG-DRL  HIGH        1 0.600466 4.937204 166.134598 0.554080\n","ACG-DRL  HIGH        3 0.602399 4.946638 165.486115 0.550476\n","ACG-DRL  HIGH        7 0.606477 4.965587 166.125977 0.541267\n","ACG-DRL   LOW        1 0.653252 4.648024 165.380722 0.580645\n","ACG-DRL   LOW        3 0.656416 4.656601 166.215210 0.586667\n","ACG-DRL   LOW        7 0.659516 4.674704 164.857498 0.579655\n","ACG-DRL  OPEN        1 1.050703 6.925827 170.122543 0.599621\n","ACG-DRL  OPEN        3 1.054109 6.938030 170.207489 0.567619\n","ACG-DRL  OPEN        7 1.062035 6.965501 170.549576 0.558541\n","\n","Aggregated by horizon CSV: /content/export/tables/acg_results_agg_by_horizon.csv\n"," horizon      MAE     RMSE      sMAPE       DA\n","       1 0.664087 4.888278 166.472713 0.570683\n","       3 0.666442 4.897275 166.502914 0.562381\n","       7 0.670608 4.916265 166.367062 0.558541\n","\n","FIGURES SAVED (first 40)\n","/content/export/figures/acg_arm_selection_over_time.png\n","/content/export/figures/acg_cumulative_regret.png\n","/content/export/figures/acg_mean_reward_by_arm.png\n","/content/export/figures/acg_reward_curve.png\n","/content/export/figures/acg_selection_counts.png\n","/content/export/figures/acg_student_graph.png\n","/content/export/figures/acg_val_mse_final_bars.png\n","/content/export/figures/directional_accuracy_bars.png\n","/content/export/figures/graph_kan_CLOSE_H1.png\n","/content/export/figures/graph_kan_CLOSE_H3.png\n","/content/export/figures/graph_kan_CLOSE_H7.png\n","/content/export/figures/graph_kan_HIGH_H1.png\n","/content/export/figures/graph_kan_HIGH_H3.png\n","/content/export/figures/graph_kan_HIGH_H7.png\n","/content/export/figures/graph_kan_LOW_H1.png\n","/content/export/figures/graph_kan_LOW_H3.png\n","/content/export/figures/graph_kan_LOW_H7.png\n","/content/export/figures/graph_kan_OPEN_H1.png\n","/content/export/figures/graph_kan_OPEN_H3.png\n","/content/export/figures/graph_kan_OPEN_H7.png\n","/content/export/figures/graph_lstm_CLOSE_H1.png\n","/content/export/figures/graph_lstm_CLOSE_H3.png\n","/content/export/figures/graph_lstm_CLOSE_H7.png\n","/content/export/figures/graph_lstm_HIGH_H1.png\n","/content/export/figures/graph_lstm_HIGH_H3.png\n","/content/export/figures/graph_lstm_HIGH_H7.png\n","/content/export/figures/graph_lstm_LOW_H1.png\n","/content/export/figures/graph_lstm_LOW_H3.png\n","/content/export/figures/graph_lstm_LOW_H7.png\n","/content/export/figures/graph_lstm_OPEN_H1.png\n","/content/export/figures/graph_lstm_OPEN_H3.png\n","/content/export/figures/graph_lstm_OPEN_H7.png\n","/content/export/figures/kan_bases_CLOSE_H1.png\n","/content/export/figures/kan_bases_CLOSE_H3.png\n","/content/export/figures/kan_bases_CLOSE_H7.png\n","/content/export/figures/kan_bases_HIGH_H1.png\n","/content/export/figures/kan_bases_HIGH_H3.png\n","/content/export/figures/kan_bases_HIGH_H7.png\n","/content/export/figures/kan_bases_LOW_H1.png\n","/content/export/figures/kan_bases_LOW_H3.png\n","... and 80 more figures\n","\n","Artifacts:\n"," - Student model summary: /content/export/tables/acg_student_model_summary.txt\n"," - Coverage windows: /content/export/tables/acg_coverage_windows.csv\n"," - Final val MSE by arm: /content/export/tables/acg_val_mse_final.csv\n"]}]},{"cell_type":"code","source":["!pip -q install torchinfo torchviz > /dev/null\n","\n","import os, json, math, random, warnings\n","from dataclasses import dataclass, asdict\n","from typing import Dict, Tuple, List\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torchinfo import summary as torch_summary\n","\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","try:\n","    from torchviz import make_dot\n","    TORCHVIZ_OK = True\n","except Exception:\n","    TORCHVIZ_OK = False\n","\n","# Paths\n","BASE_DIR = \"/content\"\n","TBL_DIR  = f\"{BASE_DIR}/export/tables\"\n","FIG_DIR  = f\"{BASE_DIR}/export/figures\"\n","LOG_DIR  = f\"{BASE_DIR}/export/logs\"\n","for d in [TBL_DIR, FIG_DIR, LOG_DIR]:\n","    os.makedirs(d, exist_ok=True)\n","\n","# Config\n","@dataclass\n","class RegimeACGConfig:\n","    ds_csv: str = f\"{BASE_DIR}/export/tables/dataset_long_1D.csv\"\n","    assets_max: int = 5\n","    horizons: Tuple[int,...] = (1,3,7)\n","    win: int = 64\n","    pe_win: int = 64\n","    pe_m: int = 4\n","    pe_tau: int = 1\n","    min_samples_per_arm: int = 128\n","    batch_size: int = 256\n","    teacher_rounds: int = 240\n","    train_steps_per_pull: int = 1\n","    lr: float = 2e-3\n","    entropy_beta: float = 1e-3\n","    aux_sup_weight: float = 1e-1\n","    ucb_c: float = 1.2\n","    seed: int = 1337\n","    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    lstm_hidden: int = 64\n","    lstm_layers: int = 1\n","    lstm_dropout: float = 0.0\n","    eval_n_trace: int = 400\n","\n","CFG = RegimeACGConfig()\n","random.seed(CFG.seed); np.random.seed(CFG.seed); torch.manual_seed(CFG.seed)\n","print(\"§4.4 Config\\n\", json.dumps(asdict(CFG), indent=2))\n","\n","# Load dataset and standardize (train-only stats)\n","df = pd.read_csv(CFG.ds_csv, parse_dates=[\"timestamp\"])\n","assert {\"timestamp\",\"symbol\",\"close\",\"split\"}.issubset(df.columns)\n","df[\"close\"] = pd.to_numeric(df[\"close\"], errors=\"coerce\")\n","df = df.dropna(subset=[\"close\"]).reset_index(drop=True)\n","\n","top_assets = (df.groupby(\"symbol\").size()\n","              .sort_values(ascending=False)\n","              .head(CFG.assets_max).index.tolist())\n","df = df[df[\"symbol\"].isin(top_assets)].copy().sort_values([\"symbol\",\"timestamp\"])\n","\n","scalers: Dict[str, Tuple[float,float]] = {}\n","for a, g in df[df[\"split\"]==\"train\"].groupby(\"symbol\"):\n","    mu = float(g[\"close\"].mean())\n","    sd = float(g[\"close\"].std(ddof=0))\n","    scalers[a] = (mu, sd if sd > 0 else 1.0)\n","\n","def zscore(asset, x):\n","    mu, sd = scalers[asset]; return (x - mu) / sd\n","\n","df[\"z\"] = df.apply(lambda r: zscore(r[\"symbol\"], r[\"close\"]), axis=1)\n","\n","# Permutation entropy\n","from math import factorial\n","\n","def _perm_index(perm: np.ndarray) -> int:\n","    m = len(perm); code = 0\n","    for i in range(m):\n","        c = 0\n","        for j in range(i+1, m):\n","            if perm[j] < perm[i]: c += 1\n","        code = code * (m - i) + c\n","    return code\n","\n","def permutation_entropy_window(x: np.ndarray, m: int = 4) -> float:\n","    n = len(x)\n","    if n < m: return np.nan\n","    counts = np.zeros(factorial(m), dtype=np.int64)\n","    for i in range(n - m + 1):\n","        pat = np.argsort(x[i:i+m], kind=\"mergesort\")\n","        counts[_perm_index(pat)] += 1\n","    total = counts.sum()\n","    if total == 0: return np.nan\n","    p = counts[counts>0].astype(np.float64) / total\n","    H = -np.sum(p * np.log(p))\n","    return float(H / np.log(factorial(m)))\n","\n","def permutation_entropy_series(x: np.ndarray, win: int = 64, m: int = 4) -> np.ndarray:\n","    n = len(x); out = np.full(n, np.nan, dtype=np.float32)\n","    for t in range(win-1, n):\n","        out[t] = permutation_entropy_window(x[t-win+1:t+1], m=m)\n","    return out\n","\n","# Compute PE and regimes (train tertiles)\n","pe_records = []\n","for a, g in df.groupby(\"symbol\"):\n","    z_all = g[\"z\"].values.astype(np.float32)\n","    H = permutation_entropy_series(z_all, win=CFG.pe_win, m=CFG.pe_m)\n","    g = g.copy(); g[\"pe_norm\"] = H\n","    g_train = g[g[\"split\"]==\"train\"].dropna(subset=[\"pe_norm\"])\n","    if len(g_train) < 32:\n","        thr1, thr2 = np.nanpercentile(g[\"pe_norm\"], [33, 67])\n","    else:\n","        thr1, thr2 = np.nanpercentile(g_train[\"pe_norm\"], [33, 67])\n","    def tier(h):\n","        if np.isnan(h): return np.nan\n","        if h <= thr1: return \"low\"\n","        if h <= thr2: return \"mid\"\n","        return \"high\"\n","    g[\"regime\"] = g[\"pe_norm\"].apply(tier)\n","    pe_records.append(g)\n","\n","df_pe = pd.concat(pe_records, ignore_index=True)\n","df_pe.to_csv(f\"{TBL_DIR}/perm_entropy_with_regimes.csv\", index=False)\n","\n","# Example BTC segmentation figure\n","def plot_btc_segmentation():\n","    g = df_pe[df_pe[\"symbol\"]==\"BTC\"].dropna(subset=[\"pe_norm\"]).copy()\n","    if len(g)==0: return None\n","    fig, ax = plt.subplots(2, 1, figsize=(10,5), sharex=True)\n","    ax[0].plot(g[\"timestamp\"], g[\"z\"], lw=0.8); ax[0].set_title(\"BTC standardized close\")\n","    ax[1].plot(g[\"timestamp\"], g[\"pe_norm\"], lw=0.8); ax[1].set_title(f\"BTC permutation entropy (m={CFG.pe_m}, win={CFG.pe_win})\")\n","    for tier_name, color in [(\"low\",\"#d0f0c0\"), (\"mid\",\"#fff3b0\"), (\"high\",\"#f4cccc\")]:\n","        mask = (g[\"regime\"]==tier_name)\n","        if mask.any():\n","            idx = np.where(mask.values)[0]; start=None\n","            for i in range(len(idx)):\n","                if start is None: start = idx[i]\n","                if i==len(idx)-1 or idx[i+1] != idx[i]+1:\n","                    s, e = start, idx[i]\n","                    ax[1].axvspan(g[\"timestamp\"].iloc[s], g[\"timestamp\"].iloc[e], color=color, alpha=0.3)\n","                    start=None\n","    for a in ax: a.grid(True, alpha=0.2)\n","    plt.tight_layout()\n","    p = os.path.join(FIG_DIR, \"regime_segmentation_BTC.png\")\n","    plt.savefig(p, dpi=150); plt.close()\n","    return p\n","\n","seg_fig = plot_btc_segmentation()\n","\n","# Regime-aware datasets\n","def make_xy_with_regime(series: pd.Series, regimes: pd.Series, horizon: int, win: int):\n","    vals, regs = series.values, regimes.values\n","    out = {\"low\":[], \"mid\":[], \"high\":[]}; last_out = {\"low\":[], \"mid\":[], \"high\":[]}; y_out = {\"low\":[], \"mid\":[], \"high\":[]}\n","    for t in range(win-1, len(vals)-horizon):\n","        r = regs[t]\n","        if r not in out: continue\n","        x = vals[t-win+1:t+1].astype(np.float32)\n","        y = np.float32(vals[t+horizon])\n","        out[r].append(x); y_out[r].append(y); last_out[r].append(np.float32(vals[t]))\n","    out2 = {}\n","    for r in [\"low\",\"mid\",\"high\"]:\n","        if len(out[r])>0:\n","            X = np.stack(out[r])[:, :, None].astype(np.float32)\n","            Y = np.stack(y_out[r])[:, None].astype(np.float32)\n","            LAST = np.stack(last_out[r])[:, None].astype(np.float32)\n","            out2[r] = (X, Y, LAST)\n","    return out2\n","\n","data_xy = {}; coverage = []\n","for a, g in df_pe.groupby(\"symbol\"):\n","    g = g.sort_values(\"timestamp\").copy()\n","    for split in [\"train\",\"val\",\"test\"]:\n","        gz = g[g[\"split\"]==split]\n","        z, reg = gz[\"z\"].reset_index(drop=True), gz[\"regime\"].reset_index(drop=True)\n","        for h in CFG.horizons:\n","            buckets = make_xy_with_regime(z, reg, h, CFG.win)\n","            for r, tup in buckets.items():\n","                X, Y, LAST = tup\n","                key = (a, split, h, r)\n","                data_xy[key] = (X, Y, LAST)\n","                coverage.append((a, split, h, r, len(Y)))\n","\n","pd.DataFrame(coverage, columns=[\"asset\",\"split\",\"horizon\",\"regime\",\"samples\"]).to_csv(\n","    f\"{TBL_DIR}/acg_regime_coverage_windows.csv\", index=False\n",")\n","\n","# Arms: (asset,horizon,regime)\n","arms: List[Tuple[str,int,str]] = []\n","for a in top_assets:\n","    for h in CFG.horizons:\n","        for r in [\"low\",\"mid\",\"high\"]:\n","            ok = True\n","            for sp in [\"train\",\"val\",\"test\"]:\n","                key = (a, sp, h, r)\n","                if key not in data_xy: ok=False; break\n","                if data_xy[key][1].shape[0] < CFG.min_samples_per_arm: ok=False; break\n","            if ok: arms.append((a,h,r))\n","assert len(arms)>0, \"No regime-aware arms; reduce CFG.min_samples_per_arm.\"\n","n_arms = len(arms)\n","print(f\"Arms (asset,horizon,regime): {arms}\")\n","\n","# Student model\n","class PolicyLSTM(nn.Module):\n","    def __init__(self, hidden=64, layers=1, dropout=0.0):\n","        super().__init__()\n","        self.lstm = nn.LSTM(input_size=1, hidden_size=hidden, num_layers=layers,\n","                            dropout=(dropout if layers>1 else 0.0), batch_first=True)\n","        self.mu = nn.Sequential(nn.Linear(hidden, hidden), nn.ReLU(), nn.Linear(hidden, 1))\n","        self.log_sigma = nn.Parameter(torch.tensor(-0.5))\n","    def forward(self, x):\n","        o, _ = self.lstm(x)\n","        last = o[:, -1, :]\n","        mu = self.mu(last)\n","        return mu, self.log_sigma.expand_as(mu)\n","\n","def count_params(m): return sum(p.numel() for p in m.parameters() if p.requires_grad)\n","\n","device = torch.device(CFG.device)\n","policy = PolicyLSTM(hidden=CFG.lstm_hidden, layers=CFG.lstm_layers, dropout=CFG.lstm_dropout).to(device)\n","opt = torch.optim.AdamW(policy.parameters(), lr=CFG.lr)\n","\n","# Teacher (UCB1)\n","class UCB1Teacher:\n","    def __init__(self, n_arms, c=1.2):\n","        self.c = c; self.n = np.zeros(n_arms, dtype=np.int64)\n","        self.mean = np.zeros(n_arms, dtype=np.float64); self.t = 0\n","    def select(self):\n","        self.t += 1\n","        for i in range(n_arms):\n","            if self.n[i]==0: return i\n","        ucb = self.mean + self.c * np.sqrt(2.0 * math.log(self.t) / self.n)\n","        return int(np.argmax(ucb))\n","    def update(self, i, reward):\n","        self.n[i] += 1\n","        self.mean[i] += (reward - self.mean[i]) / self.n[i]\n","\n","teacher = UCB1Teacher(n_arms, CFG.ucb_c)\n","\n","# Batching, eval, metrics\n","def sample_train_minibatch(arm_idx: int, batch_size: int):\n","    a,h,r = arms[arm_idx]\n","    X,Y,L = data_xy[(a,\"train\",h,r)]\n","    idx = np.random.randint(0, len(Y), size=(min(batch_size, len(Y)),))\n","    xb = torch.from_numpy(X[idx]).float().to(device)\n","    yb = torch.from_numpy(Y[idx]).float().to(device)\n","    lb = torch.from_numpy(L[idx]).float().to(device)\n","    return xb, yb, lb\n","\n","def eval_val_mse(policy, arm_idx: int) -> float:\n","    a,h,r = arms[arm_idx]\n","    X,Y,_ = data_xy[(a,\"val\",h,r)]\n","    with torch.no_grad():\n","        xb = torch.from_numpy(X).float().to(device)\n","        mu, _ = policy(xb)\n","        yhat = mu.squeeze(1).cpu().numpy()\n","    return float(np.mean((Y.squeeze(1) - yhat)**2))\n","\n","def mae(y, yhat): return float(np.mean(np.abs(y - yhat)))\n","def rmse(y, yhat): return float(np.sqrt(np.mean((y - yhat)**2)))\n","def smape(y, yhat, eps=1e-8):\n","    denom = (np.abs(y) + np.abs(yhat) + eps)/2.0\n","    return float(np.mean(np.abs(y - yhat)/denom) * 100.0)\n","def dir_acc(y_prev, y_true, yhat):\n","    s_true = np.sign(y_true - y_prev); s_pred = np.sign(yhat - y_prev)\n","    return float(np.mean((s_true == s_pred).astype(np.float32)))\n","\n","# REINFORCE step\n","baseline_reward = 0.0\n","baseline_momentum = 0.9\n","def reinforce_step(xb, yb):\n","    policy.train()\n","    opt.zero_grad()\n","    mu, log_sigma = policy(xb)\n","    sigma = torch.exp(log_sigma)\n","    dist = torch.distributions.Normal(mu, sigma)\n","    a = dist.rsample()\n","    r = - (a - yb)**2\n","    global baseline_reward\n","    avg_r = r.mean().detach()\n","    advantage = r - baseline_reward\n","    baseline_reward = baseline_momentum * baseline_reward + (1.0 - baseline_momentum) * avg_r\n","    loss_policy = - (dist.log_prob(a) * advantage.detach()).mean()\n","    loss_entropy = - CFG.entropy_beta * dist.entropy().mean()\n","    loss_aux = CFG.aux_sup_weight * nn.MSELoss()(mu, yb)\n","    loss = loss_policy + loss_entropy + loss_aux\n","    loss.backward()\n","    nn.utils.clip_grad_norm_(policy.parameters(), 1.0)\n","    opt.step()\n","\n","# Warm start\n","for i in range(n_arms):\n","    xb, yb, _ = sample_train_minibatch(i, CFG.batch_size)\n","    reinforce_step(xb, yb)\n","\n","# Bandit loop\n","log = {\"round\":[], \"arm\":[], \"asset\":[], \"horizon\":[], \"regime\":[],\n","       \"reward_lp\":[], \"val_before\":[], \"val_after\":[], \"mean_reward_est\":[], \"pulls_arm\":[]}\n","\n","for t in range(1, CFG.teacher_rounds + 1):\n","    i = teacher.select()\n","    a,h,r = arms[i]\n","    val_before = eval_val_mse(policy, i)\n","    xb, yb, _ = sample_train_minibatch(i, CFG.batch_size)\n","    for _ in range(CFG.train_steps_per_pull): reinforce_step(xb, yb)\n","    val_after = eval_val_mse(policy, i)\n","    reward_lp = float(val_before - val_after)\n","    teacher.update(i, reward_lp)\n","\n","    log[\"round\"].append(t); log[\"arm\"].append(i)\n","    log[\"asset\"].append(a); log[\"horizon\"].append(h); log[\"regime\"].append(r)\n","    log[\"reward_lp\"].append(reward_lp)\n","    log[\"val_before\"].append(val_before); log[\"val_after\"].append(val_after)\n","    log[\"mean_reward_est\"].append(float(teacher.mean[i])); log[\"pulls_arm\"].append(int(teacher.n[i]))\n","\n","log_df = pd.DataFrame(log)\n","log_csv = f\"{TBL_DIR}/acg_regime_teacher_log.csv\"\n","log_df.to_csv(log_csv, index=False)\n","\n","sel_df = (log_df.groupby([\"arm\",\"asset\",\"horizon\",\"regime\"])\n","          .size().reset_index(name=\"pulls\").sort_values(\"arm\"))\n","sel_df[\"mean_reward\"] = [float(teacher.mean[int(i)]) for i in sel_df[\"arm\"].values]\n","sel_df.to_csv(f\"{TBL_DIR}/acg_regime_selection_counts.csv\", index=False)\n","\n","# Regime transitions\n","reg_map = {\"low\":0,\"mid\":1,\"high\":2}\n","reg_list = log_df[\"regime\"].map(reg_map).values\n","trans = np.zeros((3,3), dtype=np.int64)\n","for i in range(1, len(reg_list)):\n","    trans[reg_list[i-1], reg_list[i]] += 1\n","trans_df = pd.DataFrame(trans, index=[\"low\",\"mid\",\"high\"], columns=[\"low\",\"mid\",\"high\"])\n","trans_df.to_csv(f\"{TBL_DIR}/acg_regime_transitions.csv\")\n","\n","plt.figure(figsize=(4.5,4))\n","plt.imshow(trans, cmap=\"Blues\")\n","plt.xticks([0,1,2], [\"low\",\"mid\",\"high\"]); plt.yticks([0,1,2], [\"low\",\"mid\",\"high\"])\n","plt.title(\"Regime transitions (teacher selections)\")\n","for i in range(3):\n","    for j in range(3):\n","        plt.text(j, i, str(trans[i,j]), ha=\"center\", va=\"center\")\n","plt.tight_layout()\n","plt.savefig(os.path.join(FIG_DIR, \"acg_regime_transition_heatmap.png\"), dpi=150); plt.close()\n","\n","# Plots\n","def savefig(fname):\n","    p = os.path.join(FIG_DIR, fname); plt.tight_layout(); plt.savefig(p, dpi=150); plt.close(); return p\n","\n","labels = [f\"{a}-H{h}-{r}\" for (a,h,r) in arms]\n","\n","plt.figure(figsize=(9,3))\n","plt.plot(log_df[\"arm\"].values, lw=0.8)\n","plt.yticks(range(len(labels)), labels)\n","plt.xlabel(\"Round\"); plt.ylabel(\"Selected arm\")\n","plt.title(\"Regime-aware arm selection over time\")\n","f_sel_over_time = savefig(\"acg_regime_arm_selection_over_time.png\")\n","\n","plt.figure(figsize=(9,2.6))\n","plt.scatter(log_df[\"round\"], log_df[\"regime\"].map({\"low\":0,\"mid\":1,\"high\":2}), s=8)\n","plt.yticks([0,1,2], [\"low\",\"mid\",\"high\"])\n","plt.title(\"Selected regime over time\"); plt.xlabel(\"Round\"); plt.ylabel(\"Regime\")\n","f_reg_over_time = savefig(\"acg_selected_regime_over_time.png\")\n","\n","plt.figure(figsize=(8,3))\n","plt.bar(range(len(labels)), sel_df.sort_values(\"arm\")[\"pulls\"].values)\n","plt.xticks(range(len(labels)), labels, rotation=60, ha=\"right\")\n","plt.ylabel(\"Pulls\"); plt.title(\"Selection counts by arm (regime-aware)\")\n","f_sel_counts = savefig(\"acg_regime_selection_counts.png\")\n","\n","plt.figure(figsize=(8,3))\n","plt.bar(range(len(labels)), sel_df.sort_values(\"arm\")[\"mean_reward\"].values)\n","plt.xticks(range(len(labels)), labels, rotation=60, ha=\"right\")\n","plt.ylabel(\"Mean reward (Δ val MSE)\"); plt.title(\"Mean learning progress by arm\")\n","f_mean_reward = savefig(\"acg_regime_mean_reward_by_arm.png\")\n","\n","plt.figure(figsize=(7,3))\n","plt.plot(pd.Series(log_df[\"reward_lp\"]).rolling(10).mean())\n","plt.axhline(0.0, color=\"black\", lw=1)\n","plt.title(\"Learning progress (rolling-10 mean)\"); plt.xlabel(\"Round\"); plt.ylabel(\"Δ val MSE\")\n","f_reward_curve = savefig(\"acg_regime_reward_curve.png\")\n","\n","inst_regret = []\n","for r in range(len(log_df)):\n","    means_snapshot = (log_df.iloc[:r+1].groupby(\"arm\")[\"mean_reward_est\"].last().to_dict())\n","    best_mean = max(means_snapshot.values()) if len(means_snapshot)>0 else 0.0\n","    inst_regret.append(best_mean - log_df.iloc[r][\"reward_lp\"])\n","regret = np.cumsum(inst_regret)\n","\n","plt.figure(figsize=(7,3))\n","plt.plot(regret); plt.title(\"Cumulative regret (approx.)\")\n","plt.xlabel(\"Round\"); plt.ylabel(\"Regret\")\n","f_regret = savefig(\"acg_regime_cumulative_regret.png\")\n","\n","val_after_latest = []\n","for i in range(n_arms):\n","    val_after_latest.append(eval_val_mse(policy, i))\n","val_snap_df = pd.DataFrame({\n","    \"arm_idx\": list(range(n_arms)),\n","    \"asset\": [a for a,_,_ in arms],\n","    \"horizon\": [h for _,h,_ in arms],\n","    \"regime\": [r for *_,r in arms],\n","    \"val_mse_final\": val_after_latest,\n","    \"pulls\": sel_df.sort_values(\"arm\")[\"pulls\"].values\n","})\n","val_snap_df.to_csv(f\"{TBL_DIR}/acg_regime_val_mse_final.csv\", index=False)\n","\n","plt.figure(figsize=(9,3))\n","plt.bar(range(n_arms), val_snap_df[\"val_mse_final\"].values)\n","plt.xticks(range(n_arms), labels, rotation=60, ha=\"right\")\n","plt.ylabel(\"Val MSE\"); plt.title(\"Final validation MSE by arm (regime-aware)\")\n","f_val_mse_final = savefig(\"acg_regime_val_mse_final_bars.png\")\n","\n","reg_reward = log_df.groupby(\"regime\")[\"reward_lp\"].mean().reindex([\"low\",\"mid\",\"high\"])\n","plt.figure(figsize=(5,3))\n","plt.bar(reg_reward.index, reg_reward.values)\n","plt.ylabel(\"Mean reward (Δ val MSE)\"); plt.title(\"Learning progress by regime\")\n","f_reward_by_regime = savefig(\"acg_mean_reward_by_regime.png\")\n","\n","# Test evaluation\n","results = []\n","for i in range(n_arms):\n","    a,h,r = arms[i]\n","    X,Y,L = data_xy[(a,\"test\",h,r)]\n","    with torch.no_grad():\n","        xb = torch.from_numpy(X).float().to(device)\n","        mu, _ = policy(xb)\n","        yhat = mu.squeeze(1).cpu().numpy()\n","    y_true = Y.squeeze(1); y_prev = L.squeeze(1)\n","    results.append(dict(model=\"ACG-DRL\", asset=a, horizon=h, regime=r,\n","                        MAE=mae(y_true, yhat), RMSE=rmse(y_true, yhat),\n","                        sMAPE=smape(y_true, yhat), DA=dir_acc(y_prev, y_true, yhat)))\n","\n","res_df = pd.DataFrame(results).sort_values([\"asset\",\"horizon\",\"regime\"])\n","res_csv = f\"{TBL_DIR}/acg_results_per_arm_regime.csv\"\n","res_df.to_csv(res_csv, index=False)\n","\n","agg_df = (res_df.groupby([\"horizon\",\"regime\"])\n","          .agg(MAE=(\"MAE\",\"mean\"), RMSE=(\"RMSE\",\"mean\"), sMAPE=(\"sMAPE\",\"mean\"), DA=(\"DA\",\"mean\"))\n","          .reset_index().sort_values([\"horizon\",\"regime\"]))\n","agg_csv = f\"{TBL_DIR}/acg_results_agg_by_horizon_regime.csv\"\n","agg_df.to_csv(agg_csv, index=False)\n","\n","def barplot_metric(df, metric, fname, title):\n","    plt.figure(figsize=(7,4))\n","    labels_h = sorted(df[\"horizon\"].unique())\n","    regs = [\"low\",\"mid\",\"high\"]\n","    width = 0.22; idx = np.arange(len(labels_h))\n","    for i, r in enumerate(regs):\n","        sub = df[df[\"regime\"]==r].set_index(\"horizon\").reindex(labels_h)\n","        plt.bar(idx + i*width, sub[metric].values, width=width, label=r)\n","    plt.xticks(idx + width, [f\"H={h}\" for h in labels_h])\n","    plt.ylabel(metric); plt.title(title); plt.legend()\n","    p = os.path.join(FIG_DIR, fname); plt.tight_layout(); plt.savefig(p, dpi=150); plt.close(); return p\n","\n","p_mae = barplot_metric(agg_df, \"MAE\",   \"acg_regime_mae_bars.png\",   \"MAE by horizon and regime\")\n","p_da  = barplot_metric(agg_df, \"DA\",    \"acg_regime_da_bars.png\",    \"Directional Accuracy by horizon and regime\")\n","p_rmse= barplot_metric(agg_df, \"RMSE\",  \"acg_regime_rmse_bars.png\",  \"RMSE by horizon and regime\")\n","p_sm  = barplot_metric(agg_df, \"sMAPE\", \"acg_regime_smape_bars.png\", \"sMAPE by horizon and regime\")\n","\n","# Model report\n","model_params = count_params(policy)\n","ms = torch_summary(policy, input_size=(1, CFG.win, 1), verbose=0)\n","with open(os.path.join(TBL_DIR, \"acg_regime_student_model_summary.txt\"), \"w\") as f:\n","    f.write(str(ms)); f.write(f\"\\nTotal trainable parameters: {model_params}\\n\")\n","\n","if TORCHVIZ_OK:\n","    try:\n","        xdummy = torch.randn(1, CFG.win, 1).to(device)\n","        mu, _ = policy(xdummy)\n","        make_dot(mu, params=dict(list(policy.named_parameters()))).render(\n","            os.path.join(FIG_DIR, \"acg_regime_student_graph\"), format=\"png\", cleanup=True\n","        )\n","    except Exception as e:\n","        print(\"torchviz failed:\", e)\n","\n","# Console summary\n","print(\"\\n§4.4 RESULTS SUMMARY\")\n","print(\"Regime coverage windows CSV:\", f\"{TBL_DIR}/acg_regime_coverage_windows.csv\")\n","print(\"Arms (asset,horizon,regime):\", arms)\n","print(\"\\nSelection counts & mean rewards CSV:\", f\"{TBL_DIR}/acg_regime_selection_counts.csv\")\n","print(sel_df.to_string(index=False))\n","print(\"\\nRegime transition matrix CSV:\", f\"{TBL_DIR}/acg_regime_transitions.csv\")\n","print(trans_df.to_string())\n","print(\"\\nPer-arm test results CSV:\", res_csv)\n","print(res_df.to_string(index=False))\n","print(\"\\nAggregated by horizon × regime CSV:\", agg_csv)\n","print(agg_df.to_string(index=False))\n","print(\"\\nStudent params:\", model_params, \"| Summary:\", os.path.join(TBL_DIR, \"acg_regime_student_model_summary.txt\"))\n","\n","# Figure inventory\n","figs = sorted([os.path.join(FIG_DIR, f) for f in os.listdir(FIG_DIR) if f.endswith(\".png\")])\n","print(\"\\nFIGURES (first 40)\")\n","for p in figs[:40]:\n","    print(p)\n","if len(figs) > 40:\n","    print(f\"... and {len(figs)-40} more\")\n","\n","print(\"\\nKey figures:\")\n","print(\" - Regime segmentation example:\", seg_fig)\n","print(\" - Arm selection over time:\", f_sel_over_time)\n","print(\" - Selected regime over time:\", f_reg_over_time)\n","print(\" - Selection counts:\", f_sel_counts)\n","print(\" - Mean reward by arm:\", f_mean_reward)\n","print(\" - Rolling reward:\", f_reward_curve)\n","print(\" - Cumulative regret:\", f_regret)\n","print(\" - Final val MSE bars:\", f_val_mse_final)\n","print(\" - Reward by regime:\", f_reward_by_regime)\n","print(\" - Regime-stratified bars:\", p_mae, p_da, p_rmse, p_sm)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PXl7rHdYbN74","executionInfo":{"status":"ok","timestamp":1761564939308,"user_tz":-180,"elapsed":72433,"user":{"displayName":"David Kibaara","userId":"07870126442739887481"}},"outputId":"8923b4ca-0fa7-451b-ef50-7dbd39b8d83a"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["§4.4 Config\n"," {\n","  \"ds_csv\": \"/content/export/tables/dataset_long_1D.csv\",\n","  \"assets_max\": 5,\n","  \"horizons\": [\n","    1,\n","    3,\n","    7\n","  ],\n","  \"win\": 64,\n","  \"pe_win\": 64,\n","  \"pe_m\": 4,\n","  \"pe_tau\": 1,\n","  \"min_samples_per_arm\": 128,\n","  \"batch_size\": 256,\n","  \"teacher_rounds\": 240,\n","  \"train_steps_per_pull\": 1,\n","  \"lr\": 0.002,\n","  \"entropy_beta\": 0.001,\n","  \"aux_sup_weight\": 0.1,\n","  \"ucb_c\": 1.2,\n","  \"seed\": 1337,\n","  \"device\": \"cpu\",\n","  \"lstm_hidden\": 64,\n","  \"lstm_layers\": 1,\n","  \"lstm_dropout\": 0.0,\n","  \"eval_n_trace\": 400\n","}\n","Arms (asset,horizon,regime): [('CLOSE', 1, 'high'), ('CLOSE', 3, 'high'), ('CLOSE', 7, 'high'), ('HIGH', 1, 'high'), ('HIGH', 3, 'high'), ('HIGH', 7, 'high'), ('LOW', 1, 'mid'), ('LOW', 1, 'high'), ('LOW', 3, 'mid'), ('LOW', 3, 'high'), ('LOW', 7, 'mid'), ('LOW', 7, 'high'), ('OPEN', 1, 'mid'), ('OPEN', 3, 'mid'), ('OPEN', 7, 'mid')]\n","\n","§4.4 RESULTS SUMMARY\n","Regime coverage windows CSV: /content/export/tables/acg_regime_coverage_windows.csv\n","Arms (asset,horizon,regime): [('CLOSE', 1, 'high'), ('CLOSE', 3, 'high'), ('CLOSE', 7, 'high'), ('HIGH', 1, 'high'), ('HIGH', 3, 'high'), ('HIGH', 7, 'high'), ('LOW', 1, 'mid'), ('LOW', 1, 'high'), ('LOW', 3, 'mid'), ('LOW', 3, 'high'), ('LOW', 7, 'mid'), ('LOW', 7, 'high'), ('OPEN', 1, 'mid'), ('OPEN', 3, 'mid'), ('OPEN', 7, 'mid')]\n","\n","Selection counts & mean rewards CSV: /content/export/tables/acg_regime_selection_counts.csv\n"," arm asset  horizon regime  pulls  mean_reward\n","   0 CLOSE        1   high     16    -0.000906\n","   1 CLOSE        3   high     16     0.003051\n","   2 CLOSE        7   high     16    -0.003159\n","   3  HIGH        1   high     16     0.001633\n","   4  HIGH        3   high     16    -0.000003\n","   5  HIGH        7   high     16     0.000195\n","   6   LOW        1    mid     16    -0.000059\n","   7   LOW        1   high     16     0.000178\n","   8   LOW        3    mid     16    -0.000183\n","   9   LOW        3   high     16    -0.000272\n","  10   LOW        7    mid     16     0.000331\n","  11   LOW        7   high     16     0.000315\n","  12  OPEN        1    mid     16    -0.000882\n","  13  OPEN        3    mid     16    -0.000148\n","  14  OPEN        7    mid     16    -0.000025\n","\n","Regime transition matrix CSV: /content/export/tables/acg_regime_transitions.csv\n","      low  mid  high\n","low     0    0     0\n","mid     0   35    61\n","high    0   61    82\n","\n","Per-arm test results CSV: /content/export/tables/acg_results_per_arm_regime.csv\n","  model asset  horizon regime      MAE     RMSE      sMAPE       DA\n","ACG-DRL CLOSE        1   high 0.342093 3.012454 196.834976 0.541667\n","ACG-DRL CLOSE        3   high 0.162027 0.223129 196.432312 0.548951\n","ACG-DRL CLOSE        7   high 0.350687 3.044747 197.463196 0.542553\n","ACG-DRL  HIGH        1   high 0.841476 6.031252 196.212204 0.548495\n","ACG-DRL  HIGH        3   high 0.850639 6.052089 196.321075 0.569024\n","ACG-DRL  HIGH        7   high 1.009503 6.613739 196.159058 0.511945\n","ACG-DRL   LOW        1   high 0.646843 4.577238 194.635330 0.554217\n","ACG-DRL   LOW        1    mid 0.748133 4.825749 194.748764 0.579545\n","ACG-DRL   LOW        3   high 0.413974 2.854257 194.680267 0.598394\n","ACG-DRL   LOW        3    mid 0.974570 5.969255 193.749268 0.561069\n","ACG-DRL   LOW        7   high 0.412005 2.852790 193.725021 0.590361\n","ACG-DRL   LOW        7    mid 0.983306 6.015008 194.386108 0.550388\n","ACG-DRL  OPEN        1    mid 0.905725 6.208724 197.847672 0.555556\n","ACG-DRL  OPEN        3    mid 1.154533 7.226964 195.385681 0.587963\n","ACG-DRL  OPEN        7    mid 0.975357 6.604174 195.125732 0.555556\n","\n","Aggregated by horizon × regime CSV: /content/export/tables/acg_results_agg_by_horizon_regime.csv\n"," horizon regime      MAE     RMSE      sMAPE       DA\n","       1   high 0.610137 4.540315 195.894170 0.548126\n","       1    mid 0.826929 5.517236 196.298218 0.567551\n","       3   high 0.475547 3.043158 195.811218 0.572123\n","       3    mid 1.064551 6.598109 194.567474 0.574516\n","       7   high 0.590732 4.170425 195.782425 0.548287\n","       7    mid 0.979331 6.309591 194.755920 0.552972\n","\n","Student params: 21378 | Summary: /content/export/tables/acg_regime_student_model_summary.txt\n","\n","FIGURES (first 40)\n","/content/export/figures/acg_arm_selection_over_time.png\n","/content/export/figures/acg_cumulative_regret.png\n","/content/export/figures/acg_mean_reward_by_arm.png\n","/content/export/figures/acg_mean_reward_by_regime.png\n","/content/export/figures/acg_regime_arm_selection_over_time.png\n","/content/export/figures/acg_regime_cumulative_regret.png\n","/content/export/figures/acg_regime_da_bars.png\n","/content/export/figures/acg_regime_mae_bars.png\n","/content/export/figures/acg_regime_mean_reward_by_arm.png\n","/content/export/figures/acg_regime_reward_curve.png\n","/content/export/figures/acg_regime_rmse_bars.png\n","/content/export/figures/acg_regime_selection_counts.png\n","/content/export/figures/acg_regime_smape_bars.png\n","/content/export/figures/acg_regime_student_graph.png\n","/content/export/figures/acg_regime_transition_heatmap.png\n","/content/export/figures/acg_regime_val_mse_final_bars.png\n","/content/export/figures/acg_reward_curve.png\n","/content/export/figures/acg_selected_regime_over_time.png\n","/content/export/figures/acg_selection_counts.png\n","/content/export/figures/acg_student_graph.png\n","/content/export/figures/acg_val_mse_final_bars.png\n","/content/export/figures/directional_accuracy_bars.png\n","/content/export/figures/graph_kan_CLOSE_H1.png\n","/content/export/figures/graph_kan_CLOSE_H3.png\n","/content/export/figures/graph_kan_CLOSE_H7.png\n","/content/export/figures/graph_kan_HIGH_H1.png\n","/content/export/figures/graph_kan_HIGH_H3.png\n","/content/export/figures/graph_kan_HIGH_H7.png\n","/content/export/figures/graph_kan_LOW_H1.png\n","/content/export/figures/graph_kan_LOW_H3.png\n","/content/export/figures/graph_kan_LOW_H7.png\n","/content/export/figures/graph_kan_OPEN_H1.png\n","/content/export/figures/graph_kan_OPEN_H3.png\n","/content/export/figures/graph_kan_OPEN_H7.png\n","/content/export/figures/graph_lstm_CLOSE_H1.png\n","/content/export/figures/graph_lstm_CLOSE_H3.png\n","/content/export/figures/graph_lstm_CLOSE_H7.png\n","/content/export/figures/graph_lstm_HIGH_H1.png\n","/content/export/figures/graph_lstm_HIGH_H3.png\n","/content/export/figures/graph_lstm_HIGH_H7.png\n","... and 94 more\n","\n","Key figures:\n"," - Regime segmentation example: None\n"," - Arm selection over time: /content/export/figures/acg_regime_arm_selection_over_time.png\n"," - Selected regime over time: /content/export/figures/acg_selected_regime_over_time.png\n"," - Selection counts: /content/export/figures/acg_regime_selection_counts.png\n"," - Mean reward by arm: /content/export/figures/acg_regime_mean_reward_by_arm.png\n"," - Rolling reward: /content/export/figures/acg_regime_reward_curve.png\n"," - Cumulative regret: /content/export/figures/acg_regime_cumulative_regret.png\n"," - Final val MSE bars: /content/export/figures/acg_regime_val_mse_final_bars.png\n"," - Reward by regime: /content/export/figures/acg_mean_reward_by_regime.png\n"," - Regime-stratified bars: /content/export/figures/acg_regime_mae_bars.png /content/export/figures/acg_regime_da_bars.png /content/export/figures/acg_regime_rmse_bars.png /content/export/figures/acg_regime_smape_bars.png\n"]}]},{"cell_type":"code","source":["!pip -q install torchinfo torchviz > /dev/null\n","\n","import os, json, math, random, warnings\n","from dataclasses import dataclass, asdict\n","from typing import Dict, Tuple, List\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","from torchinfo import summary as torch_summary\n","\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","try:\n","    from torchviz import make_dot\n","    TORCHVIZ_OK = True\n","except Exception:\n","    TORCHVIZ_OK = False\n","\n","BASE_DIR = \"/content\"\n","TBL_DIR  = f\"{BASE_DIR}/export/tables\"\n","FIG_DIR  = f\"{BASE_DIR}/export/figures\"\n","LOG_DIR  = f\"{BASE_DIR}/export/logs\"\n","for d in [TBL_DIR, FIG_DIR, LOG_DIR]:\n","    os.makedirs(d, exist_ok=True)\n","\n","@dataclass\n","class AblationCfg:\n","    ds_csv: str = f\"{BASE_DIR}/export/tables/dataset_long_1D.csv\"\n","    assets_max: int = 5\n","    horizons: Tuple[int,...] = (1,3,7)\n","    win: int = 64\n","    batch_size: int = 256\n","    epochs_supervised: int = 4\n","    teacher_rounds: int = 150\n","    train_steps_per_pull: int = 1\n","    lr: float = 2e-3\n","    entropy_beta: float = 1e-3\n","    aux_sup_weight: float = 1e-1\n","    ucb_c: float = 1.2\n","    seed: int = 1337\n","    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    lstm_hidden: int = 64\n","    lstm_layers: int = 1\n","    lstm_dropout: float = 0.0\n","    eval_n_trace: int = 400\n","\n","CFG = AblationCfg()\n","random.seed(CFG.seed); np.random.seed(CFG.seed); torch.manual_seed(CFG.seed)\n","print(\"§4.5 Config\\n\", json.dumps(asdict(CFG), indent=2))\n","\n","df = pd.read_csv(CFG.ds_csv, parse_dates=[\"timestamp\"])\n","assert {\"timestamp\",\"symbol\",\"close\",\"split\"}.issubset(df.columns)\n","df[\"close\"] = pd.to_numeric(df[\"close\"], errors=\"coerce\")\n","df = df.dropna(subset=[\"close\"]).reset_index(drop=True)\n","\n","top_assets = (df.groupby(\"symbol\").size()\n","              .sort_values(ascending=False)\n","              .head(CFG.assets_max).index.tolist())\n","df = df[df[\"symbol\"].isin(top_assets)].copy().sort_values([\"symbol\",\"timestamp\"])\n","\n","scalers_close: Dict[str, Tuple[float,float]] = {}\n","for a, g in df[df[\"split\"]==\"train\"].groupby(\"symbol\"):\n","    mu = float(g[\"close\"].mean())\n","    sd = float(g[\"close\"].std(ddof=0))\n","    scalers_close[a] = (mu, sd if sd>0 else 1.0)\n","\n","def z_close(asset, x):\n","    mu, sd = scalers_close[asset]; return (x - mu) / sd\n","\n","df[\"z\"] = df.apply(lambda r: z_close(r[\"symbol\"], r[\"close\"]), axis=1)\n","\n","def rsi(series: pd.Series, period: int = 14) -> pd.Series:\n","    delta = series.diff()\n","    up = delta.clip(lower=0.0)\n","    down = -delta.clip(upper=0.0)\n","    roll_up = up.rolling(period, min_periods=period).mean()\n","    roll_down = down.rolling(period, min_periods=period).mean()\n","    rs = roll_up / (roll_down + 1e-12)\n","    return 100.0 - (100.0 / (1.0 + rs))\n","\n","def _sanitize_cols(df_in: pd.DataFrame, cols: List[str]) -> pd.DataFrame:\n","    out = df_in.copy()\n","    out[cols] = out[cols].replace([np.inf, -np.inf], np.nan)\n","    out[cols] = out[cols].fillna(0.0)\n","    out[cols] = out[cols].clip(-10.0, 10.0)\n","    return out\n","\n","def build_features(g: pd.DataFrame, augmented: bool) -> pd.DataFrame:\n","    g = g.sort_values(\"timestamp\").copy()\n","    if not augmented:\n","        return _sanitize_cols(g[[\"timestamp\",\"symbol\",\"split\",\"z\"]].copy(), [\"z\"])\n","\n","    z = g[\"z\"].astype(float)\n","    ret = z.diff()\n","    vol14 = ret.rolling(14, min_periods=14).std()\n","    rsi14 = rsi(g[\"close\"], period=14)\n","    ma5 = z.rolling(5, min_periods=5).mean()\n","    ma20 = z.rolling(20, min_periods=20).mean()\n","    macd = ma5 - ma20\n","    dist_ma20 = z - ma20\n","\n","    feats = pd.DataFrame({\n","        \"timestamp\": g[\"timestamp\"].values,\n","        \"symbol\": g[\"symbol\"].values,\n","        \"split\": g[\"split\"].values,\n","        \"z\": z.values,\n","        \"dz\": ret.values,\n","        \"vol14\": vol14.values,\n","        \"rsi14\": rsi14.values,\n","        \"macd_z\": macd.values,\n","        \"dist_ma20_z\": dist_ma20.values\n","    })\n","\n","    std_cols = [\"dz\",\"vol14\",\"rsi14\",\"macd_z\",\"dist_ma20_z\"]\n","    out_list = []\n","    for a, ga in feats.groupby(\"symbol\"):\n","        ga = ga.copy()\n","        train_mask = (ga[\"split\"]==\"train\")\n","        stats = {}\n","        for col in std_cols:\n","            mu = ga.loc[train_mask, col].mean()\n","            sd = ga.loc[train_mask, col].std(ddof=0)\n","            if not pd.notna(sd) or sd <= 0: sd = 1.0\n","            stats[col] = (float(mu if pd.notna(mu) else 0.0), float(sd))\n","        for col in std_cols:\n","            mu, sd = stats[col]\n","            ga[col] = (ga[col] - mu) / sd\n","        ga = _sanitize_cols(ga, [\"z\"] + std_cols)\n","        out_list.append(ga)\n","\n","    return pd.concat(out_list, ignore_index=True)\n","\n","df_price = build_features(df, augmented=False)\n","df_aug   = build_features(df, augmented=True)\n","\n","def make_xy_features(g: pd.DataFrame, feature_cols: List[str], horizon: int, win: int):\n","    vals = g[feature_cols].values.astype(np.float32)\n","    z = g[\"z\"].values.astype(np.float32)\n","    x_list, y_list, p_list = [], [], []\n","    dropped = 0\n","    for t in range(win-1, len(vals)-horizon):\n","        x = vals[t-win+1:t+1, :]\n","        y = z[t+horizon]\n","        p = z[t]\n","        if not (np.isfinite(x).all() and np.isfinite(y) and np.isfinite(p)):\n","            dropped += 1\n","            continue\n","        x_list.append(x); y_list.append([y]); p_list.append([p])\n","    if dropped > 0:\n","        print(f\"[make_xy_features] Dropped {dropped} windows due to non-finite values (win={win}, h={horizon}).\")\n","    if not x_list:\n","        return None, None, None\n","    return (np.stack(x_list), np.stack(y_list), np.stack(p_list))\n","\n","def build_arm_data(df_feat: pd.DataFrame, horizons, win) -> Tuple[Dict, List[Tuple[str,int]], pd.DataFrame]:\n","    all_cols = [\"z\",\"dz\",\"vol14\",\"rsi14\",\"macd_z\",\"dist_ma20_z\"]\n","    feature_cols = [c for c in df_feat.columns if c in all_cols]\n","    data_xy, coverage, arms = {}, [], []\n","    for a, g in df_feat.groupby(\"symbol\"):\n","        g = g.sort_values(\"timestamp\")\n","        for split in [\"train\",\"val\",\"test\"]:\n","            gs = g[g[\"split\"]==split].reset_index(drop=True)\n","            for h in horizons:\n","                X,Y,P = make_xy_features(gs, feature_cols, h, win)\n","                if X is not None:\n","                    data_xy[(a,split,h)] = (X,Y,P)\n","                    coverage.append((a,split,h,len(Y)))\n","        for h in horizons:\n","            ok = all(((a,sp,h) in data_xy and data_xy[(a,sp,h)][1].shape[0] > 0) for sp in [\"train\",\"val\",\"test\"])\n","            if ok: arms.append((a,h))\n","    cov_df = pd.DataFrame(coverage, columns=[\"asset\",\"split\",\"horizon\",\"samples\"])\n","    return data_xy, arms, cov_df\n","\n","data_xy_price, arms_price, cov_price = build_arm_data(df_price, CFG.horizons, CFG.win)\n","data_xy_aug,   arms_aug,   cov_aug   = build_arm_data(df_aug,   CFG.horizons, CFG.win)\n","\n","arms_common = sorted(list(set(arms_price).intersection(set(arms_aug))))\n","assert len(arms_common)>0, \"No common feasible arms found.\"\n","print(\"Common feasible arms:\", arms_common)\n","\n","class PolicyLSTM(nn.Module):\n","    def __init__(self, input_dim=1, hidden=64, layers=1, dropout=0.0):\n","        super().__init__()\n","        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden, num_layers=layers,\n","                            dropout=(dropout if layers>1 else 0.0), batch_first=True)\n","        self.mu = nn.Sequential(nn.Linear(hidden, hidden), nn.ReLU(), nn.Linear(hidden, 1))\n","        self.log_sigma = nn.Parameter(torch.tensor(-0.5))\n","    def forward(self, x):\n","        o, _ = self.lstm(x)\n","        last = o[:, -1, :]\n","        mu = self.mu(last)\n","        mu = torch.nan_to_num(mu, nan=0.0, posinf=0.0, neginf=0.0)\n","        return mu, self.log_sigma.expand_as(mu)\n","\n","def count_params(m): return sum(p.numel() for p in m.parameters() if p.requires_grad)\n","\n","class UCB1Teacher:\n","    def __init__(self, n_arms, c=1.2):\n","        self.c=c; self.n=np.zeros(n_arms, dtype=np.int64); self.mean=np.zeros(n_arms, dtype=np.float64); self.t=0\n","    def select(self):\n","        self.t+=1\n","        for i in range(len(self.n)):\n","            if self.n[i]==0: return i\n","        ucb = self.mean + self.c*np.sqrt(2.0*math.log(self.t)/self.n)\n","        return int(np.argmax(ucb))\n","    def update(self, i, reward):\n","        self.n[i]+=1\n","        self.mean[i]+= (reward - self.mean[i])/self.n[i]\n","\n","class UniformTeacher:\n","    def __init__(self, n_arms):\n","        self.n = np.zeros(n_arms, dtype=np.int64)\n","        self.mean = np.zeros(n_arms, dtype=np.float64)\n","        self.t=0\n","    def select(self):\n","        self.t+=1\n","        return int(np.random.randint(0,len(self.n)))\n","    def update(self, i, reward):\n","        self.n[i]+=1\n","        self.mean[i]+= (reward - self.mean[i])/self.n[i]\n","\n","def mae(y, yhat): return float(np.mean(np.abs(y - yhat)))\n","def rmse(y, yhat): return float(np.sqrt(np.mean((y - yhat)**2)))\n","def smape(y, yhat, eps=1e-8):\n","    denom = (np.abs(y) + np.abs(yhat) + eps)/2.0\n","    return float(np.mean(np.abs(y - yhat)/denom) * 100.0)\n","def dir_acc(y_prev, y_true, yhat):\n","    s_true = np.sign(y_true - y_prev); s_pred = np.sign(yhat - y_prev)\n","    return float(np.mean((s_true == s_pred).astype(np.float32)))\n","\n","device = torch.device(CFG.device)\n","\n","def sample_minibatch(data_xy, arms, arm_idx, bs):\n","    a,h = arms[arm_idx]\n","    X,Y,P = data_xy[(a,\"train\",h)]\n","    idx = np.random.randint(0,len(Y), size=(min(bs, len(Y)),))\n","    xb = torch.from_numpy(X[idx]).float().to(device)\n","    yb = torch.from_numpy(Y[idx]).float().to(device)\n","    pb = torch.from_numpy(P[idx]).float().to(device)\n","    return xb, yb, pb\n","\n","def eval_val_mse(model, data_xy, arms, arm_idx):\n","    a,h = arms[arm_idx]\n","    X,Y,_ = data_xy[(a,\"val\",h)]\n","    with torch.no_grad():\n","        xb = torch.from_numpy(X).float().to(device)\n","        mu, _ = model(xb)\n","        yhat = mu.squeeze(1).cpu().numpy()\n","    return float(np.mean((Y.squeeze(1) - yhat)**2))\n","\n","def bandit_run(run_name: str, data_xy, arms, input_dim: int, teacher_type=\"ucb1\", feat_tag=\"price\"):\n","    model = PolicyLSTM(input_dim=input_dim, hidden=CFG.lstm_hidden, layers=CFG.lstm_layers, dropout=CFG.lstm_dropout).to(device)\n","    opt = torch.optim.AdamW(model.parameters(), lr=CFG.lr)\n","    teacher = UCB1Teacher(len(arms), c=CFG.ucb_c) if teacher_type==\"ucb1\" else UniformTeacher(len(arms))\n","\n","    def reinforce_step(xb, yb):\n","        model.train(); opt.zero_grad()\n","        mu, log_sigma = model(xb)\n","        sigma = torch.exp(log_sigma)\n","        dist = torch.distributions.Normal(mu, sigma)\n","        a = dist.rsample()\n","        r = - (a - yb)**2\n","        reinforce_step.baseline = 0.9*reinforce_step.baseline + 0.1*r.mean().detach() if hasattr(reinforce_step,\"baseline\") else r.mean().detach()\n","        adv = r - reinforce_step.baseline\n","        loss = - (dist.log_prob(a) * adv.detach()).mean()\n","        loss += - CFG.entropy_beta * dist.entropy().mean()\n","        loss += CFG.aux_sup_weight * nn.MSELoss()(mu, yb)\n","        loss.backward()\n","        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        opt.step()\n","\n","    for i in range(len(arms)):\n","        xb,yb,_ = sample_minibatch(data_xy, arms, i, CFG.batch_size)\n","        reinforce_step(xb,yb)\n","\n","    log = {\"round\":[], \"arm\":[], \"asset\":[], \"horizon\":[], \"reward_lp\":[], \"val_before\":[], \"val_after\":[], \"mean_reward_est\":[], \"pulls_arm\":[]}\n","    for t in range(1, CFG.teacher_rounds+1):\n","        i = teacher.select()\n","        a,h = arms[i]\n","        vb = eval_val_mse(model, data_xy, arms, i)\n","        for _ in range(CFG.train_steps_per_pull):\n","            xb,yb,_ = sample_minibatch(data_xy, arms, i, CFG.batch_size)\n","            reinforce_step(xb,yb)\n","        va = eval_val_mse(model, data_xy, arms, i)\n","        rw = float(vb - va)\n","        teacher.update(i, rw)\n","\n","        log[\"round\"].append(t); log[\"arm\"].append(i); log[\"asset\"].append(a); log[\"horizon\"].append(h)\n","        log[\"reward_lp\"].append(rw); log[\"val_before\"].append(vb); log[\"val_after\"].append(va)\n","        log[\"mean_reward_est\"].append(float(teacher.mean[i])); log[\"pulls_arm\"].append(int(teacher.n[i]))\n","\n","    log_df = pd.DataFrame(log)\n","    log_csv = os.path.join(TBL_DIR, f\"ablate_{run_name}_{feat_tag}_teacherlog.csv\")\n","    log_df.to_csv(log_csv, index=False)\n","\n","    sel_df = (log_df.groupby([\"arm\",\"asset\",\"horizon\"]).size().reset_index(name=\"pulls\").sort_values(\"arm\"))\n","    sel_df[\"mean_reward\"] = [float(teacher.mean[int(i)]) for i in sel_df[\"arm\"].values]\n","    sel_csv = os.path.join(TBL_DIR, f\"ablate_{run_name}_{feat_tag}_selcounts.csv\")\n","    sel_df.to_csv(sel_csv, index=False)\n","\n","    def savefig(fname):\n","        p = os.path.join(FIG_DIR, fname); plt.tight_layout(); plt.savefig(p, dpi=150); plt.close(); return p\n","\n","    labels = [f\"{a}-H{h}\" for (a,h) in arms]\n","    plt.figure(figsize=(8,3)); plt.plot(log_df[\"arm\"].values, lw=0.8)\n","    plt.yticks(range(len(labels)), labels); plt.xlabel(\"Round\"); plt.ylabel(\"Arm\")\n","    plt.title(f\"{run_name} ({feat_tag}) arm selection over time\")\n","    f_arm_time = savefig(f\"ablate_{run_name}_{feat_tag}_arm_over_time.png\")\n","\n","    plt.figure(figsize=(6,3))\n","    plt.bar(range(len(labels)), sel_df[\"pulls\"].values)\n","    plt.xticks(range(len(labels)), labels, rotation=45, ha=\"right\")\n","    plt.ylabel(\"Pulls\"); plt.title(f\"{run_name} ({feat_tag}) selection counts\")\n","    f_sel_counts = savefig(f\"ablate_{run_name}_{feat_tag}_selcounts.png\")\n","\n","    plt.figure(figsize=(7,3))\n","    plt.plot(pd.Series(log_df[\"reward_lp\"]).rolling(10).mean()); plt.axhline(0.0, color=\"black\", lw=1)\n","    plt.title(f\"{run_name} ({feat_tag}) rolling mean Δ val MSE\"); plt.xlabel(\"Round\"); plt.ylabel(\"Reward\")\n","    f_reward = savefig(f\"ablate_{run_name}_{feat_tag}_reward_curve.png\")\n","\n","    inst_regret = []\n","    for r in range(len(log_df)):\n","        means_snapshot = log_df.iloc[:r+1].groupby(\"arm\")[\"mean_reward_est\"].last().to_dict()\n","        best_mean = max(means_snapshot.values()) if means_snapshot else 0.0\n","        inst_regret.append(best_mean - log_df.iloc[r][\"reward_lp\"])\n","    regret = np.cumsum(inst_regret)\n","    plt.figure(figsize=(7,3)); plt.plot(regret); plt.title(f\"{run_name} ({feat_tag}) cumulative regret\"); plt.xlabel(\"Round\"); plt.ylabel(\"Regret\")\n","    f_regret = savefig(f\"ablate_{run_name}_{feat_tag}_regret.png\")\n","\n","    params = count_params(model)\n","    ms = torch_summary(model, input_size=(1, CFG.win, input_dim), verbose=0)\n","    with open(os.path.join(TBL_DIR, f\"ablate_{run_name}_{feat_tag}_model_summary.txt\"), \"w\") as f:\n","        f.write(str(ms)); f.write(f\"\\nTotal trainable parameters: {params}\\n\")\n","    if TORCHVIZ_OK:\n","        try:\n","            xdummy = torch.randn(1, CFG.win, input_dim).to(device)\n","            mu,_ = model(xdummy)\n","            dot = make_dot(mu, params=dict(list(model.named_parameters())))\n","            dot.render(os.path.join(FIG_DIR, f\"ablate_{run_name}_{feat_tag}_graph\"), format=\"png\", cleanup=True)\n","        except Exception as e:\n","            print(\"torchviz failed:\", e)\n","\n","    results = []\n","    for i,(a,h) in enumerate(arms):\n","        Xte,Yte,Prev = data_xy[(a,\"test\",h)]\n","        with torch.no_grad():\n","            xb = torch.from_numpy(Xte).float().to(device)\n","            mu,_ = model(xb)\n","            yhat = mu.squeeze(1).cpu().numpy()\n","        y_true = Yte.squeeze(1); y_prev = Prev.squeeze(1)\n","\n","        results.append(dict(run=run_name, features=feat_tag, asset=a, horizon=h,\n","                            MAE=mae(y_true, yhat), RMSE=rmse(y_true, yhat),\n","                            sMAPE=smape(y_true, yhat), DA=dir_acc(y_prev, y_true, yhat)))\n","\n","        Nplot = min(CFG.eval_n_trace, len(y_true))\n","        plt.figure(figsize=(8,3.5))\n","        plt.plot(y_true[:Nplot], label=\"true\"); plt.plot(yhat[:Nplot], label=\"pred\")\n","        plt.title(f\"{run_name} ({feat_tag}) — Test trace {a}, H={h}\")\n","        plt.xlabel(\"Index\"); plt.ylabel(\"Standardized close\"); plt.legend()\n","        _ = os.path.join(FIG_DIR, f\"ablate_{run_name}_{feat_tag}_trace_{a}_H{h}.png\"); plt.tight_layout(); plt.savefig(_, dpi=150); plt.close()\n","\n","        plt.figure(figsize=(6,3))\n","        resid = y_true - yhat\n","        plt.hist(resid, bins=40)\n","        plt.title(f\"{run_name} ({feat_tag}) — Residuals {a}, H={h}\")\n","        plt.xlabel(\"Residual\"); plt.ylabel(\"Count\")\n","        _ = os.path.join(FIG_DIR, f\"ablate_{run_name}_{feat_tag}_resid_{a}_H{h}.png\"); plt.tight_layout(); plt.savefig(_, dpi=150); plt.close()\n","\n","    res_df = pd.DataFrame(results).sort_values([\"horizon\",\"asset\"])\n","    res_csv = os.path.join(TBL_DIR, f\"ablate_{run_name}_{feat_tag}_results_per_arm.csv\")\n","    res_df.to_csv(res_csv, index=False)\n","\n","    agg_df = (res_df.groupby(\"horizon\")\n","              .agg(MAE=(\"MAE\",\"mean\"), RMSE=(\"RMSE\",\"mean\"), sMAPE=(\"sMAPE\",\"mean\"), DA=(\"DA\",\"mean\"))\n","              .reset_index().sort_values(\"horizon\"))\n","    agg_csv = os.path.join(TBL_DIR, f\"ablate_{run_name}_{feat_tag}_results_agg_by_horizon.csv\")\n","    agg_df.to_csv(agg_csv, index=False)\n","\n","    def barplot_metric(df, metric, fname, title):\n","        plt.figure(figsize=(6.8,3.8))\n","        plt.bar([f\"H={h}\" for h in df[\"horizon\"]], df[metric].values)\n","        plt.ylabel(metric); plt.title(title);\n","        p = os.path.join(FIG_DIR, fname); plt.tight_layout(); plt.savefig(p, dpi=150); plt.close(); return p\n","\n","    p_mae = barplot_metric(agg_df, \"MAE\",   f\"ablate_{run_name}_{feat_tag}_mae_bars.png\",   f\"{run_name} ({feat_tag}) MAE by horizon\")\n","    p_rmse= barplot_metric(agg_df, \"RMSE\",  f\"ablate_{run_name}_{feat_tag}_rmse_bars.png\",  f\"{run_name} ({feat_tag}) RMSE by horizon\")\n","    p_sm  = barplot_metric(agg_df, \"sMAPE\", f\"ablate_{run_name}_{feat_tag}_smape_bars.png\", f\"{run_name} ({feat_tag}) sMAPE by horizon\")\n","    p_da  = barplot_metric(agg_df, \"DA\",    f\"ablate_{run_name}_{feat_tag}_da_bars.png\",    f\"{run_name} ({feat_tag}) Directional Accuracy by horizon\")\n","\n","    outputs = {\n","        \"log_csv\": log_csv, \"sel_csv\": sel_csv, \"res_csv\": res_csv, \"agg_csv\": agg_csv,\n","        \"figs\": [f_arm_time, f_sel_counts, f_reward, f_regret, p_mae, p_rmse, p_sm, p_da]\n","    }\n","    return outputs, res_df, agg_df, sel_df\n","\n","def supervised_run(run_name: str, data_xy, arms, input_dim: int, feat_tag=\"aug\"):\n","    model = PolicyLSTM(input_dim=input_dim, hidden=CFG.lstm_hidden, layers=CFG.lstm_layers, dropout=CFG.lstm_dropout).to(device)\n","    opt = torch.optim.AdamW(model.parameters(), lr=CFG.lr)\n","    loss_fn = nn.MSELoss()\n","\n","    Xtr = []; Ytr = []\n","    for a,h in arms:\n","        X,Y,_ = data_xy[(a,\"train\",h)]\n","        Xtr.append(X); Ytr.append(Y)\n","    Xtr = torch.from_numpy(np.concatenate(Xtr, axis=0)).float().to(device)\n","    Ytr = torch.from_numpy(np.concatenate(Ytr, axis=0)).float().to(device)\n","\n","    Xva = []; Yva = []\n","    for a,h in arms:\n","        X,Y,_ = data_xy[(a,\"val\",h)]\n","        Xva.append(X); Yva.append(Y)\n","    Xva = torch.from_numpy(np.concatenate(Xva, axis=0)).float().to(device)\n","    Yva = torch.from_numpy(np.concatenate(Yva, axis=0)).float().to(device)\n","\n","    best = (1e9, None)\n","    for ep in range(1, CFG.epochs_supervised+1):\n","        model.train()\n","        idx = torch.randperm(Xtr.shape[0])\n","        for start in range(0, len(idx), CFG.batch_size):\n","            sel = idx[start:start+CFG.batch_size]\n","            xb, yb = Xtr[sel], Ytr[sel]\n","            opt.zero_grad()\n","            mu,_ = model(xb)\n","            loss = loss_fn(mu, yb)\n","            loss.backward()\n","            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","            opt.step()\n","        model.eval()\n","        with torch.no_grad():\n","            mu,_ = model(Xva)\n","            va = loss_fn(mu, Yva).item()\n","        if va < best[0]:\n","            best = (va, {k:v.cpu().clone() for k,v in model.state_dict().items()})\n","    if best[1] is not None:\n","        model.load_state_dict(best[1])\n","\n","    params = count_params(model)\n","    ms = torch_summary(model, input_size=(1, CFG.win, input_dim), verbose=0)\n","    with open(os.path.join(TBL_DIR, f\"ablate_{run_name}_{feat_tag}_model_summary.txt\"), \"w\") as f:\n","        f.write(str(ms)); f.write(f\"\\nTotal trainable parameters: {params}\\n\")\n","    if TORCHVIZ_OK:\n","        try:\n","            xdummy = torch.randn(1, CFG.win, input_dim).to(device)\n","            mu,_ = model(xdummy)\n","            dot = make_dot(mu, params=dict(list(model.named_parameters())))\n","            dot.render(os.path.join(FIG_DIR, f\"ablate_{run_name}_{feat_tag}_graph\"), format=\"png\", cleanup=True)\n","        except Exception as e:\n","            print(\"torchviz failed:\", e)\n","\n","    results = []\n","    for a,h in arms:\n","        Xte,Yte,Prev = data_xy[(a,\"test\",h)]\n","        with torch.no_grad():\n","            xb = torch.from_numpy(Xte).float().to(device)\n","            mu,_ = model(xb)\n","            yhat = mu.squeeze(1).cpu().numpy()\n","        y_true = Yte.squeeze(1); y_prev = Prev.squeeze(1)\n","        results.append(dict(run=run_name, features=feat_tag, asset=a, horizon=h,\n","                            MAE=mae(y_true, yhat), RMSE=rmse(y_true, yhat),\n","                            sMAPE=smape(y_true, yhat), DA=dir_acc(y_prev, y_true, yhat)))\n","    res_df = pd.DataFrame(results).sort_values([\"horizon\",\"asset\"])\n","    res_csv = os.path.join(TBL_DIR, f\"ablate_{run_name}_{feat_tag}_results_per_arm.csv\")\n","    res_df.to_csv(res_csv, index=False)\n","\n","    agg_df = (res_df.groupby(\"horizon\")\n","              .agg(MAE=(\"MAE\",\"mean\"), RMSE=(\"RMSE\",\"mean\"), sMAPE=(\"sMAPE\",\"mean\"), DA=(\"DA\",\"mean\"))\n","              .reset_index().sort_values(\"horizon\"))\n","    agg_csv = os.path.join(TBL_DIR, f\"ablate_{run_name}_{feat_tag}_results_agg_by_horizon.csv\")\n","    agg_df.to_csv(agg_csv, index=False)\n","\n","    def barplot(df, metric, fname, title):\n","        plt.figure(figsize=(6.8,3.8))\n","        plt.bar([f\"H={h}\" for h in df[\"horizon\"]], df[metric].values)\n","        plt.ylabel(metric); plt.title(title)\n","        p = os.path.join(FIG_DIR, fname); plt.tight_layout(); plt.savefig(p, dpi=150); plt.close(); return p\n","    p_mae = barplot(agg_df, \"MAE\",   f\"ablate_{run_name}_{feat_tag}_mae_bars.png\",   f\"{run_name} ({feat_tag}) MAE by horizon\")\n","    p_rmse= barplot(agg_df, \"RMSE\",  f\"ablate_{run_name}_{feat_tag}_rmse_bars.png\",  f\"{run_name} ({feat_tag}) RMSE by horizon\")\n","    p_sm  = barplot(agg_df, \"sMAPE\", f\"ablate_{run_name}_{feat_tag}_smape_bars.png\", f\"{run_name} ({feat_tag}) sMAPE by horizon\")\n","    p_da  = barplot(agg_df, \"DA\",    f\"ablate_{run_name}_{feat_tag}_da_bars.png\",    f\"{run_name} ({feat_tag}) Directional Accuracy by horizon\")\n","\n","    outputs = {\"res_csv\":res_csv, \"agg_csv\":agg_csv,\n","               \"figs\":[p_mae,p_rmse,p_sm,p_da]}\n","    return outputs, res_df, agg_df\n","\n","input_dim_price = 1\n","out_v1, res_v1, agg_v1, sel_v1 = bandit_run(\"ACG_UCB1\", data_xy_price, arms_common, input_dim_price, teacher_type=\"ucb1\", feat_tag=\"price\")\n","\n","input_dim_aug = len([c for c in df_aug.columns if c in [\"z\",\"dz\",\"vol14\",\"rsi14\",\"macd_z\",\"dist_ma20_z\"]])\n","out_v2, res_v2, agg_v2, sel_v2 = bandit_run(\"ACG_UCB1\", data_xy_aug, arms_common, input_dim_aug, teacher_type=\"ucb1\", feat_tag=\"aug\")\n","\n","out_v3, res_v3, agg_v3, sel_v3 = bandit_run(\"UNIFORM\", data_xy_aug, arms_common, input_dim_aug, teacher_type=\"uniform\", feat_tag=\"aug\")\n","\n","out_v4, res_v4, agg_v4 = supervised_run(\"SUPERVISED\", data_xy_aug, arms_common, input_dim_aug, feat_tag=\"aug\")\n","\n","agg_v1[\"run\"]=\"ACG-UCB1(price)\"; agg_v2[\"run\"]=\"ACG-UCB1(aug)\"\n","agg_v3[\"run\"]=\"UNIFORM(aug)\";    agg_v4[\"run\"]=\"SUPERVISED(aug)\"\n","agg_all = pd.concat([agg_v1,agg_v2,agg_v3,agg_v4], ignore_index=True)\n","\n","agg_all_csv = os.path.join(TBL_DIR, \"ablate_agg_horizon_cross_variant.csv\")\n","agg_all.to_csv(agg_all_csv, index=False)\n","\n","def grouped_bars(df, metric, fname, title):\n","    plt.figure(figsize=(8.8,4.2))\n","    horizons = sorted(df[\"horizon\"].unique())\n","    runs = [\"ACG-UCB1(price)\",\"ACG-UCB1(aug)\",\"UNIFORM(aug)\",\"SUPERVISED(aug)\"]\n","    width = 0.18; idx = np.arange(len(horizons))\n","    for i, r in enumerate(runs):\n","        sub = df[df[\"run\"]==r].set_index(\"horizon\").reindex(horizons)\n","        plt.bar(idx + i*width, sub[metric].values, width=width, label=r)\n","    plt.xticks(idx + width*1.5, [f\"H={h}\" for h in horizons])\n","    plt.ylabel(metric); plt.title(title); plt.legend()\n","    p = os.path.join(FIG_DIR, fname); plt.tight_layout(); plt.savefig(p, dpi=150); plt.close(); return p\n","\n","g_mae  = grouped_bars(agg_all, \"MAE\",   \"ablate_cross_variant_mae.png\",   \"MAE by horizon — cross-variant\")\n","g_rmse = grouped_bars(agg_all, \"RMSE\",  \"ablate_cross_variant_rmse.png\",  \"RMSE by horizon — cross-variant\")\n","g_sm   = grouped_bars(agg_all, \"sMAPE\", \"ablate_cross_variant_smape.png\", \"sMAPE by horizon — cross-variant\")\n","g_da   = grouped_bars(agg_all, \"DA\",    \"ablate_cross_variant_da.png\",    \"Directional Accuracy by horizon — cross-variant\")\n","\n","def head_table(df, n=10):\n","    try: return df.head(n).to_string(index=False)\n","    except: return str(df.head(n))\n","\n","print(\"\\n§4.5 RESULTS SUMMARY\")\n","print(\"Common arms:\", arms_common)\n","print(\"\\nAggregates by horizon — ACG-UCB1 (price):\", out_v1[\"agg_csv\"]); print(head_table(agg_v1, 10))\n","print(\"\\nAggregates by horizon — ACG-UCB1 (aug):\", out_v2[\"agg_csv\"]);   print(head_table(agg_v2, 10))\n","print(\"\\nAggregates by horizon — UNIFORM (aug):\", out_v3[\"agg_csv\"]);    print(head_table(agg_v3, 10))\n","print(\"\\nAggregates by horizon — SUPERVISED (aug):\", out_v4[\"agg_csv\"]); print(head_table(agg_v4, 10))\n","print(\"\\nCross-variant aggregate CSV:\", agg_all_csv);                     print(head_table(agg_all, 12))\n","\n","print(\"\\nSelection counts — ACG-UCB1 (price):\", out_v1[\"sel_csv\"]); print(sel_v1.to_string(index=False))\n","print(\"\\nSelection counts — ACG-UCB1 (aug):\",   out_v2[\"sel_csv\"]); print(sel_v2.to_string(index=False))\n","print(\"\\nSelection counts — UNIFORM (aug):\",    out_v3[\"sel_csv\"]); print(sel_v3.to_string(index=False))\n","\n","figs = sorted([os.path.join(FIG_DIR, f) for f in os.listdir(FIG_DIR) if f.endswith(\".png\")])\n","print(\"\\nFIGURES (first 40)\")\n","for p in figs[:40]:\n","    print(p)\n","if len(figs) > 40:\n","    print(f\"... and {len(figs)-40} more figures\")\n","\n","print(\"\\nArtifacts:\")\n","print(\" - V1 logs:\", out_v1[\"log_csv\"])\n","print(\" - V2 logs:\", out_v2[\"log_csv\"])\n","print(\" - V3 logs:\", out_v3[\"log_csv\"])\n","print(\" - Per-arm results:\", out_v1[\"res_csv\"], out_v2[\"res_csv\"], out_v3[\"res_csv\"], out_v4[\"res_csv\"])\n","print(\" - Cross-variant aggregates:\", agg_all_csv)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dSTJLwuQdr4o","executionInfo":{"status":"ok","timestamp":1761565425484,"user_tz":-180,"elapsed":292065,"user":{"displayName":"David Kibaara","userId":"07870126442739887481"}},"outputId":"b69c2c19-6c65-47a8-f7a9-57ef4ce28964"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["§4.5 Config\n"," {\n","  \"ds_csv\": \"/content/export/tables/dataset_long_1D.csv\",\n","  \"assets_max\": 5,\n","  \"horizons\": [\n","    1,\n","    3,\n","    7\n","  ],\n","  \"win\": 64,\n","  \"batch_size\": 256,\n","  \"epochs_supervised\": 4,\n","  \"teacher_rounds\": 150,\n","  \"train_steps_per_pull\": 1,\n","  \"lr\": 0.002,\n","  \"entropy_beta\": 0.001,\n","  \"aux_sup_weight\": 0.1,\n","  \"ucb_c\": 1.2,\n","  \"seed\": 1337,\n","  \"device\": \"cpu\",\n","  \"lstm_hidden\": 64,\n","  \"lstm_layers\": 1,\n","  \"lstm_dropout\": 0.0,\n","  \"eval_n_trace\": 400\n","}\n","Common feasible arms: [('CLOSE', 1), ('CLOSE', 3), ('CLOSE', 7), ('HIGH', 1), ('HIGH', 3), ('HIGH', 7), ('LOW', 1), ('LOW', 3), ('LOW', 7), ('OPEN', 1), ('OPEN', 3), ('OPEN', 7)]\n","\n","§4.5 RESULTS SUMMARY\n","Common arms: [('CLOSE', 1), ('CLOSE', 3), ('CLOSE', 7), ('HIGH', 1), ('HIGH', 3), ('HIGH', 7), ('LOW', 1), ('LOW', 3), ('LOW', 7), ('OPEN', 1), ('OPEN', 3), ('OPEN', 7)]\n","\n","Aggregates by horizon — ACG-UCB1 (price): /content/export/tables/ablate_ACG_UCB1_price_results_agg_by_horizon.csv\n"," horizon      MAE     RMSE      sMAPE       DA             run\n","       1 0.251132 1.086679 123.790483 0.581120 ACG-UCB1(price)\n","       3 0.251700 1.088508 123.827053 0.572857 ACG-UCB1(price)\n","       7 0.252740 1.092246 123.849844 0.567178 ACG-UCB1(price)\n","\n","Aggregates by horizon — ACG-UCB1 (aug): /content/export/tables/ablate_ACG_UCB1_aug_results_agg_by_horizon.csv\n"," horizon      MAE     RMSE      sMAPE       DA           run\n","       1 0.290912 1.084023 186.565105 0.565465 ACG-UCB1(aug)\n","       3 0.291166 1.085766 186.380981 0.559048 ACG-UCB1(aug)\n","       7 0.292158 1.089908 186.057880 0.555182 ACG-UCB1(aug)\n","\n","Aggregates by horizon — UNIFORM (aug): /content/export/tables/ablate_UNIFORM_aug_results_agg_by_horizon.csv\n"," horizon      MAE     RMSE      sMAPE       DA          run\n","       1 0.281976 1.087189 152.706360 0.572581 UNIFORM(aug)\n","       3 0.282148 1.088978 152.591648 0.568095 UNIFORM(aug)\n","       7 0.283086 1.094181 152.545284 0.562860 UNIFORM(aug)\n","\n","Aggregates by horizon — SUPERVISED (aug): /content/export/tables/ablate_SUPERVISED_aug_results_agg_by_horizon.csv\n"," horizon      MAE     RMSE      sMAPE       DA             run\n","       1 0.273192 1.085963 158.672352 0.570683 SUPERVISED(aug)\n","       3 0.273527 1.087005 158.941868 0.564286 SUPERVISED(aug)\n","       7 0.274164 1.090721 158.580116 0.559501 SUPERVISED(aug)\n","\n","Cross-variant aggregate CSV: /content/export/tables/ablate_agg_horizon_cross_variant.csv\n"," horizon      MAE     RMSE      sMAPE       DA             run\n","       1 0.251132 1.086679 123.790483 0.581120 ACG-UCB1(price)\n","       3 0.251700 1.088508 123.827053 0.572857 ACG-UCB1(price)\n","       7 0.252740 1.092246 123.849844 0.567178 ACG-UCB1(price)\n","       1 0.290912 1.084023 186.565105 0.565465   ACG-UCB1(aug)\n","       3 0.291166 1.085766 186.380981 0.559048   ACG-UCB1(aug)\n","       7 0.292158 1.089908 186.057880 0.555182   ACG-UCB1(aug)\n","       1 0.281976 1.087189 152.706360 0.572581    UNIFORM(aug)\n","       3 0.282148 1.088978 152.591648 0.568095    UNIFORM(aug)\n","       7 0.283086 1.094181 152.545284 0.562860    UNIFORM(aug)\n","       1 0.273192 1.085963 158.672352 0.570683 SUPERVISED(aug)\n","       3 0.273527 1.087005 158.941868 0.564286 SUPERVISED(aug)\n","       7 0.274164 1.090721 158.580116 0.559501 SUPERVISED(aug)\n","\n","Selection counts — ACG-UCB1 (price): /content/export/tables/ablate_ACG_UCB1_price_selcounts.csv\n"," arm asset  horizon  pulls  mean_reward\n","   0 CLOSE        1     13     0.000581\n","   1 CLOSE        3     12    -0.000385\n","   2 CLOSE        7     13     0.000024\n","   3  HIGH        1     12    -0.000003\n","   4  HIGH        3     12    -0.000201\n","   5  HIGH        7     12    -0.000038\n","   6   LOW        1     12    -0.000127\n","   7   LOW        3     12    -0.000420\n","   8   LOW        7     13     0.000148\n","   9  OPEN        1     13     0.000080\n","  10  OPEN        3     13    -0.000643\n","  11  OPEN        7     13     0.000611\n","\n","Selection counts — ACG-UCB1 (aug): /content/export/tables/ablate_ACG_UCB1_aug_selcounts.csv\n"," arm asset  horizon  pulls  mean_reward\n","   0 CLOSE        1     12    -0.000130\n","   1 CLOSE        3     13     0.000158\n","   2 CLOSE        7     12    -0.000091\n","   3  HIGH        1     12    -0.000182\n","   4  HIGH        3     13     0.000060\n","   5  HIGH        7     12     0.000017\n","   6   LOW        1     12     0.000051\n","   7   LOW        3     13     0.000038\n","   8   LOW        7     13     0.000061\n","   9  OPEN        1     13     0.000503\n","  10  OPEN        3     12    -0.000330\n","  11  OPEN        7     13     0.000681\n","\n","Selection counts — UNIFORM (aug): /content/export/tables/ablate_UNIFORM_aug_selcounts.csv\n"," arm asset  horizon  pulls  mean_reward\n","   0 CLOSE        1     17    -0.000198\n","   1 CLOSE        3      9     0.000081\n","   2 CLOSE        7     10    -0.000183\n","   3  HIGH        1     13     0.000059\n","   4  HIGH        3      9    -0.000231\n","   5  HIGH        7      6    -0.000061\n","   6   LOW        1     20    -0.000035\n","   7   LOW        3      9     0.000045\n","   8   LOW        7      9    -0.000002\n","   9  OPEN        1     21    -0.000067\n","  10  OPEN        3     14     0.000394\n","  11  OPEN        7     13     0.000355\n","\n","FIGURES (first 40)\n","/content/export/figures/ablate_ACG_UCB1_aug_arm_over_time.png\n","/content/export/figures/ablate_ACG_UCB1_aug_da_bars.png\n","/content/export/figures/ablate_ACG_UCB1_aug_graph.png\n","/content/export/figures/ablate_ACG_UCB1_aug_mae_bars.png\n","/content/export/figures/ablate_ACG_UCB1_aug_regret.png\n","/content/export/figures/ablate_ACG_UCB1_aug_resid_CLOSE_H1.png\n","/content/export/figures/ablate_ACG_UCB1_aug_resid_CLOSE_H3.png\n","/content/export/figures/ablate_ACG_UCB1_aug_resid_CLOSE_H7.png\n","/content/export/figures/ablate_ACG_UCB1_aug_resid_HIGH_H1.png\n","/content/export/figures/ablate_ACG_UCB1_aug_resid_HIGH_H3.png\n","/content/export/figures/ablate_ACG_UCB1_aug_resid_HIGH_H7.png\n","/content/export/figures/ablate_ACG_UCB1_aug_resid_LOW_H1.png\n","/content/export/figures/ablate_ACG_UCB1_aug_resid_LOW_H3.png\n","/content/export/figures/ablate_ACG_UCB1_aug_resid_LOW_H7.png\n","/content/export/figures/ablate_ACG_UCB1_aug_resid_OPEN_H1.png\n","/content/export/figures/ablate_ACG_UCB1_aug_resid_OPEN_H3.png\n","/content/export/figures/ablate_ACG_UCB1_aug_resid_OPEN_H7.png\n","/content/export/figures/ablate_ACG_UCB1_aug_reward_curve.png\n","/content/export/figures/ablate_ACG_UCB1_aug_rmse_bars.png\n","/content/export/figures/ablate_ACG_UCB1_aug_selcounts.png\n","/content/export/figures/ablate_ACG_UCB1_aug_smape_bars.png\n","/content/export/figures/ablate_ACG_UCB1_aug_trace_CLOSE_H1.png\n","/content/export/figures/ablate_ACG_UCB1_aug_trace_CLOSE_H3.png\n","/content/export/figures/ablate_ACG_UCB1_aug_trace_CLOSE_H7.png\n","/content/export/figures/ablate_ACG_UCB1_aug_trace_HIGH_H1.png\n","/content/export/figures/ablate_ACG_UCB1_aug_trace_HIGH_H3.png\n","/content/export/figures/ablate_ACG_UCB1_aug_trace_HIGH_H7.png\n","/content/export/figures/ablate_ACG_UCB1_aug_trace_LOW_H1.png\n","/content/export/figures/ablate_ACG_UCB1_aug_trace_LOW_H3.png\n","/content/export/figures/ablate_ACG_UCB1_aug_trace_LOW_H7.png\n","/content/export/figures/ablate_ACG_UCB1_aug_trace_OPEN_H1.png\n","/content/export/figures/ablate_ACG_UCB1_aug_trace_OPEN_H3.png\n","/content/export/figures/ablate_ACG_UCB1_aug_trace_OPEN_H7.png\n","/content/export/figures/ablate_ACG_UCB1_price_arm_over_time.png\n","/content/export/figures/ablate_ACG_UCB1_price_da_bars.png\n","/content/export/figures/ablate_ACG_UCB1_price_graph.png\n","/content/export/figures/ablate_ACG_UCB1_price_mae_bars.png\n","/content/export/figures/ablate_ACG_UCB1_price_regret.png\n","/content/export/figures/ablate_ACG_UCB1_price_resid_CLOSE_H1.png\n","/content/export/figures/ablate_ACG_UCB1_price_resid_CLOSE_H3.png\n","... and 202 more figures\n","\n","Artifacts:\n"," - V1 logs: /content/export/tables/ablate_ACG_UCB1_price_teacherlog.csv\n"," - V2 logs: /content/export/tables/ablate_ACG_UCB1_aug_teacherlog.csv\n"," - V3 logs: /content/export/tables/ablate_UNIFORM_aug_teacherlog.csv\n"," - Per-arm results: /content/export/tables/ablate_ACG_UCB1_price_results_per_arm.csv /content/export/tables/ablate_ACG_UCB1_aug_results_per_arm.csv /content/export/tables/ablate_UNIFORM_aug_results_per_arm.csv /content/export/tables/ablate_SUPERVISED_aug_results_per_arm.csv\n"," - Cross-variant aggregates: /content/export/tables/ablate_agg_horizon_cross_variant.csv\n"]}]},{"cell_type":"code","source":["# === Chapter 4 §4.6: Comparative Analysis Experiment (CLEAN ONE-CELL) ===\n","# Purpose:\n","# - Compare ACG-DRL vs baselines (LSTM, SMA, Naive)\n","# - Run Wilcoxon (paired) and Friedman+Nemenyi tests per horizon\n","# - Generate boxplots, scatter plots, and CD diagrams\n","# - Denormalize MAE to price scale\n","# Inputs:\n","#   /content/export/tables/baseline_results_per_asset_horizon.csv\n","#   /content/export/tables/acg_results_per_arm.csv\n","#   /content/export/tables/dataset_long_1D.csv\n","# Outputs:\n","#   Tables and Figures saved under /content/export/{tables,figures}/\n","\n","import os, json, math\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Stats library\n","try:\n","    import scipy.stats as stats\n","except ImportError:\n","    !pip -q install scipy > /dev/null\n","    import scipy.stats as stats\n","\n","# --- Paths ---\n","BASE_DIR = \"/content\"\n","TBL_DIR  = f\"{BASE_DIR}/export/tables\"\n","FIG_DIR  = f\"{BASE_DIR}/export/figures\"\n","for d in [TBL_DIR, FIG_DIR]:\n","    os.makedirs(d, exist_ok=True)\n","\n","CSV_BASE = f\"{TBL_DIR}/baseline_results_per_asset_horizon.csv\"\n","CSV_ACG  = f\"{TBL_DIR}/acg_results_per_arm.csv\"\n","CSV_DS   = f\"{BASE_DIR}/export/tables/dataset_long_1D.csv\"\n","\n","assert os.path.exists(CSV_BASE), f\"Missing {CSV_BASE}\"\n","assert os.path.exists(CSV_ACG),  f\"Missing {CSV_ACG}\"\n","assert os.path.exists(CSV_DS),   f\"Missing {CSV_DS}\"\n","\n","# --- Load and clean dataset ---\n","df_all = pd.read_csv(CSV_DS, parse_dates=[\"timestamp\"])\n","\n","# Normalize column names\n","df_all.columns = [c.strip().lower() for c in df_all.columns]\n","rename_map = {}\n","if \"ticker\" in df_all.columns and \"symbol\" not in df_all.columns:\n","    rename_map[\"ticker\"] = \"symbol\"\n","if \"asset\" in df_all.columns and \"symbol\" not in df_all.columns:\n","    rename_map[\"asset\"] = \"symbol\"\n","if \"price\" in df_all.columns and \"close\" not in df_all.columns:\n","    rename_map[\"price\"] = \"close\"\n","if \"close_price\" in df_all.columns and \"close\" not in df_all.columns:\n","    rename_map[\"close_price\"] = \"close\"\n","if rename_map:\n","    df_all = df_all.rename(columns=rename_map)\n","\n","# Ensure numeric close\n","df_all[\"close\"] = pd.to_numeric(df_all[\"close\"], errors=\"coerce\")\n","df_all[\"symbol\"] = df_all[\"symbol\"].astype(str)\n","bad = df_all[\"close\"].isna().sum()\n","if bad:\n","    print(f\"[clean] Coerced {bad} non-numeric 'close' values to NaN (ignored in std).\")\n","\n","# Compute std per asset for train split\n","std_train = (\n","    df_all[df_all[\"split\"] == \"train\"]\n","    .groupby(\"symbol\")[\"close\"]\n","    .std(ddof=0)\n","    .rename(\"std_close_train\")\n",")\n","std_train = std_train.replace({0: 1.0}).fillna(1.0)\n","\n","# --- Load results ---\n","b = pd.read_csv(CSV_BASE)\n","a = pd.read_csv(CSV_ACG)\n","a[\"model\"] = \"ACG-DRL\"\n","\n","keep_models = {\"ACG-DRL\", \"LSTM\", \"SMA\", \"Naive\"}\n","b = b[b[\"model\"].isin(keep_models - {\"ACG-DRL\"})].copy()\n","a = a[a[\"model\"] == \"ACG-DRL\"].copy()\n","\n","# --- Join per horizon ---\n","def join_per_arm(h):\n","    acg_h = a[a[\"horizon\"] == h][[\"asset\", \"horizon\", \"MAE\", \"RMSE\", \"sMAPE\", \"DA\"]].rename(\n","        columns={\"MAE\": \"MAE_ACG\", \"RMSE\": \"RMSE_ACG\", \"sMAPE\": \"sMAPE_ACG\", \"DA\": \"DA_ACG\"}\n","    )\n","    rows = []\n","    for m in [\"LSTM\", \"SMA\", \"Naive\"]:\n","        bm = b[(b[\"horizon\"] == h) & (b[\"model\"] == m)][[\"asset\", \"horizon\", \"MAE\", \"RMSE\", \"sMAPE\", \"DA\"]].rename(\n","            columns={\"MAE\": f\"MAE_{m}\", \"RMSE\": f\"RMSE_{m}\", \"sMAPE\": f\"sMAPE_{m}\", \"DA\": f\"DA_{m}\"}\n","        )\n","        rows.append(bm)\n","    base_wide = rows[0]\n","    for r in rows[1:]:\n","        base_wide = base_wide.merge(r, on=[\"asset\", \"horizon\"], how=\"inner\")\n","    joined = base_wide.merge(acg_h, on=[\"asset\", \"horizon\"], how=\"inner\")\n","    joined = joined.merge(std_train, left_on=\"asset\", right_index=True, how=\"left\")\n","    joined[\"std_close_train\"] = joined[\"std_close_train\"].fillna(1.0)\n","    return joined\n","\n","J = {h: join_per_arm(h) for h in [1, 3, 7]}\n","all_joined = pd.concat([J[1], J[3], J[7]], ignore_index=True)\n","out_joined_csv = f\"{TBL_DIR}/compare_per_arm_joined.csv\"\n","all_joined.to_csv(out_joined_csv, index=False)\n","print(\"Joined per-arm rows:\", {h: len(J[h]) for h in J})\n","print(\"Saved:\", out_joined_csv)\n","\n","# --- Paired Wilcoxon ---\n","def paired_wilcoxon(x, y, alternative=\"two-sided\"):\n","    diff = y - x\n","    diff = diff[np.isfinite(diff)]\n","    n = diff.size\n","    if n == 0:\n","        return np.nan, np.nan, 0, np.nan, np.nan\n","    try:\n","        stat, p = stats.wilcoxon(x, y, zero_method=\"pratt\", alternative=alternative)\n","    except Exception:\n","        stat, p = np.nan, np.nan\n","    md = float(np.median(diff))\n","    d = float(np.mean(diff) / (np.std(diff, ddof=1) + 1e-12))\n","    return stat, p, n, md, d\n","\n","rows = []\n","for h in [1, 3, 7]:\n","    dfh = J[h].copy()\n","    for m in [\"LSTM\", \"SMA\", \"Naive\"]:\n","        stat, p, n, md, d = paired_wilcoxon(dfh[\"MAE_ACG\"], dfh[f\"MAE_{m}\"], alternative=\"less\")\n","        rows.append(dict(horizon=h, metric=\"MAE\", baseline=m, n=n, median_diff=(dfh[f\"MAE_{m}\"]-dfh[\"MAE_ACG\"]).median(), p_value=p))\n","        stat, p, n, md, d = paired_wilcoxon(dfh[\"RMSE_ACG\"], dfh[f\"RMSE_{m}\"], alternative=\"less\")\n","        rows.append(dict(horizon=h, metric=\"RMSE\", baseline=m, n=n, median_diff=(dfh[f\"RMSE_{m}\"]-dfh[\"RMSE_ACG\"]).median(), p_value=p))\n","        stat, p, n, md, d = paired_wilcoxon(dfh[\"DA_ACG\"], dfh[f\"DA_{m}\"], alternative=\"greater\")\n","        rows.append(dict(horizon=h, metric=\"DA\", baseline=m, n=n, median_diff=(dfh[\"DA_ACG\"]-dfh[f\"DA_{m}\"]).median(), p_value=p))\n","\n","wilcoxon_df = pd.DataFrame(rows)\n","wilcoxon_csv = f\"{TBL_DIR}/wilcoxon_acg_vs_baselines_by_horizon.csv\"\n","wilcoxon_df.to_csv(wilcoxon_csv, index=False)\n","print(\"Saved Wilcoxon summary:\", wilcoxon_csv)\n","print(wilcoxon_df.head(12).to_string(index=False))\n","\n","# --- Friedman + Nemenyi ---\n","def avg_ranks(values_2d, model_names):\n","    ranks = []\n","    for row in values_2d:\n","        r = stats.rankdata(row, method=\"average\")\n","        ranks.append(r)\n","    ranks = np.array(ranks)\n","    return dict(zip(model_names, ranks.mean(axis=0))), ranks\n","\n","def nemenyi_cd(k, N, alpha=0.05):\n","    q_table = {2:1.960, 3:2.343, 4:2.569, 5:2.728, 6:2.850, 7:2.948}\n","    q = q_table.get(k, 2.569)\n","    return q * math.sqrt(k*(k+1)/(6.0*N))\n","\n","def plot_cd(avg_ranks_dict, cd, title, savepath):\n","    models = list(avg_ranks_dict.keys())\n","    ranks = np.array([avg_ranks_dict[m] for m in models])\n","    order = np.argsort(ranks)\n","    models = [models[i] for i in order]\n","    ranks = ranks[order]\n","    min_r, max_r = min(ranks)-0.5, max(ranks)+0.5\n","\n","    plt.figure(figsize=(8, 1.8))\n","    y = 0.5\n","    plt.hlines(y, min_r, max_r, color=\"black\")\n","    for r in np.arange(math.floor(min_r), math.ceil(max_r)+1):\n","        plt.vlines(r, y-0.05, y+0.05, color=\"black\")\n","        plt.text(r, y+0.12, f\"{r:.0f}\", ha=\"center\", va=\"bottom\", fontsize=9)\n","    for m, r in zip(models, ranks):\n","        plt.vlines(r, y-0.05, y+0.05, color=\"black\")\n","        plt.text(r, y-0.20, m, ha=\"center\", va=\"top\", fontsize=10)\n","    cd_left = max_r - cd\n","    plt.hlines(y+0.30, cd_left, max_r, color=\"black\", linewidth=2)\n","    plt.vlines(cd_left, y+0.25, y+0.35, color=\"black\")\n","    plt.vlines(max_r, y+0.25, y+0.35, color=\"black\")\n","    plt.text((cd_left+max_r)/2, y+0.38, f\"CD = {cd:.2f}\", ha=\"center\", va=\"bottom\", fontsize=9)\n","    plt.title(title, fontsize=11)\n","    plt.yticks([]); plt.ylim(0, 1.0); plt.xlim(min_r, max_r)\n","    plt.tight_layout(); plt.savefig(savepath, dpi=150); plt.close()\n","\n","friedman_rows = []\n","for h in [1, 3, 7]:\n","    dfh = J[h].copy()\n","    models = [\"ACG-DRL\",\"LSTM\",\"SMA\",\"Naive\"]\n","    mat = np.vstack([\n","        dfh[\"MAE_ACG\"], dfh[\"MAE_LSTM\"], dfh[\"MAE_SMA\"], dfh[\"MAE_Naive\"]\n","    ]).T\n","    mat = mat[np.isfinite(mat).all(axis=1)]\n","    N = mat.shape[0]\n","    if N < 2:\n","        continue\n","    F_stat, F_p = stats.friedmanchisquare(*[mat[:,i] for i in range(mat.shape[1])])\n","    avg_r, _ = avg_ranks(mat, models)\n","    cd = nemenyi_cd(k=len(models), N=N)\n","    friedman_rows.append(dict(horizon=h, N=N, friedman_stat=F_stat, friedman_p=F_p, cd=cd, **{f\"rank_{m}\":avg_r[m] for m in models}))\n","    plot_cd(avg_r, cd, f\"Critical Difference (MAE) — H={h} (N={N})\", f\"{FIG_DIR}/comp_cd_diagram_H{h}.png\")\n","\n","friedman_df = pd.DataFrame(friedman_rows)\n","friedman_csv = f\"{TBL_DIR}/friedman_nemenyi_by_horizon.csv\"\n","friedman_df.to_csv(friedman_csv, index=False)\n","print(\"Saved Friedman+Nemenyi summary:\", friedman_csv)\n","print(friedman_df.to_string(index=False))\n","\n","# --- Boxplots and Scatter ---\n","def savefig(p):\n","    plt.tight_layout(); plt.savefig(p, dpi=150); plt.close(); return p\n","\n","for h in [1, 3, 7]:\n","    dfh = J[h]\n","    if len(dfh) == 0: continue\n","    data = [dfh[\"MAE_ACG\"], dfh[\"MAE_LSTM\"], dfh[\"MAE_SMA\"], dfh[\"MAE_Naive\"]]\n","    labels = [\"ACG-DRL\",\"LSTM\",\"SMA\",\"Naive\"]\n","    plt.figure(figsize=(6.5,3.6))\n","    plt.boxplot(data, labels=labels, showmeans=True)\n","    plt.ylabel(\"MAE (standardized)\"); plt.title(f\"MAE by model — H={h}\")\n","    savefig(f\"{FIG_DIR}/comp_boxplot_mae_H{h}.png\")\n","\n","    data = [dfh[\"RMSE_ACG\"], dfh[\"RMSE_LSTM\"], dfh[\"RMSE_SMA\"], dfh[\"RMSE_Naive\"]]\n","    plt.figure(figsize=(6.5,3.6))\n","    plt.boxplot(data, labels=labels, showmeans=True)\n","    plt.ylabel(\"RMSE (standardized)\"); plt.title(f\"RMSE by model — H={h}\")\n","    savefig(f\"{FIG_DIR}/comp_boxplot_rmse_H{h}.png\")\n","\n","    x = dfh[\"MAE_LSTM\"] - dfh[\"MAE_ACG\"]\n","    y = dfh[\"DA_ACG\"] - dfh[\"DA_LSTM\"]\n","    plt.figure(figsize=(5.5,3.6))\n","    plt.axvline(0, color=\"k\", lw=1); plt.axhline(0, color=\"k\", lw=1)\n","    plt.scatter(x, y, s=18)\n","    plt.xlabel(\"ΔMAE (LSTM − ACG)  [>0 → ACG better]\")\n","    plt.ylabel(\"ΔDA  (ACG − LSTM)  [>0 → ACG better]\")\n","    plt.title(f\"Trade-off: Direction vs. Magnitude — H={h}\")\n","    savefig(f\"{FIG_DIR}/comp_scatter_da_vs_mae_diff_H{h}.png\")\n","\n","# --- Denormalized MAE ---\n","denorm_rows = []\n","for h in [1, 3, 7]:\n","    dfh = J[h]\n","    for m, col in [(\"ACG-DRL\",\"MAE_ACG\"),(\"LSTM\",\"MAE_LSTM\"),(\"SMA\",\"MAE_SMA\"),(\"Naive\",\"MAE_Naive\")]:\n","        tmp = dfh[[\"asset\", col, \"std_close_train\"]].copy()\n","        tmp[\"MAE_price\"] = tmp[col] * tmp[\"std_close_train\"]\n","        denorm_rows.append(tmp.assign(horizon=h, model=m)[[\"horizon\",\"model\",\"asset\",\"MAE_price\"]])\n","\n","denorm_df = pd.concat(denorm_rows, ignore_index=True)\n","denorm_csv = f\"{TBL_DIR}/compare_denorm_mae_price_per_arm.csv\"\n","denorm_df.to_csv(denorm_csv, index=False)\n","print(\"Denormalized MAE saved:\", denorm_csv)\n","\n","print(\"\\n=== §4.6 COMPLETED SUCCESSFULLY ===\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n07ca40Sev5_","executionInfo":{"status":"ok","timestamp":1761566107602,"user_tz":-180,"elapsed":10317,"user":{"displayName":"David Kibaara","userId":"07870126442739887481"}},"outputId":"e8ba87e6-3b92-472b-bd7c-4451e8b6adaf"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["[clean] Coerced 5507 non-numeric 'close' values to NaN (ignored in std).\n","Joined per-arm rows: {1: 4, 3: 4, 7: 4}\n","Saved: /content/export/tables/compare_per_arm_joined.csv\n","Saved Wilcoxon summary: /content/export/tables/wilcoxon_acg_vs_baselines_by_horizon.csv\n"," horizon metric baseline  n  median_diff  p_value\n","       1    MAE     LSTM  4     0.024718   0.0625\n","       1   RMSE     LSTM  4    -0.001189   1.0000\n","       1     DA     LSTM  4     0.001898   0.2500\n","       1    MAE      SMA  4     0.453427   0.0625\n","       1   RMSE      SMA  4     0.446517   0.0625\n","       1     DA      SMA  4    -0.084440   1.0000\n","       1    MAE    Naive  4     0.464792   0.0625\n","       1   RMSE    Naive  4     1.996613   0.0625\n","       1     DA    Naive  4     0.567362   0.0625\n","       3    MAE     LSTM  4     0.013663   0.0625\n","       3   RMSE     LSTM  4    -0.001404   0.8750\n","       3     DA     LSTM  4     0.000952   0.2500\n","Saved Friedman+Nemenyi summary: /content/export/tables/friedman_nemenyi_by_horizon.csv\n"," horizon  N  friedman_stat  friedman_p       cd  rank_ACG-DRL  rank_LSTM  rank_SMA  rank_Naive\n","       1  4           11.1    0.011197 2.345165          1.00       2.00      3.25        3.75\n","       3  4           12.0    0.007383 2.345165          1.00       2.00      3.00        4.00\n","       7  4           11.1    0.011197 2.345165          1.25       1.75      3.00        4.00\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2040097956.py:214: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n","  plt.boxplot(data, labels=labels, showmeans=True)\n","/tmp/ipython-input-2040097956.py:220: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n","  plt.boxplot(data, labels=labels, showmeans=True)\n","/tmp/ipython-input-2040097956.py:214: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n","  plt.boxplot(data, labels=labels, showmeans=True)\n","/tmp/ipython-input-2040097956.py:220: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n","  plt.boxplot(data, labels=labels, showmeans=True)\n","/tmp/ipython-input-2040097956.py:214: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n","  plt.boxplot(data, labels=labels, showmeans=True)\n","/tmp/ipython-input-2040097956.py:220: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n","  plt.boxplot(data, labels=labels, showmeans=True)\n"]},{"output_type":"stream","name":"stdout","text":["Denormalized MAE saved: /content/export/tables/compare_denorm_mae_price_per_arm.csv\n","\n","=== §4.6 COMPLETED SUCCESSFULLY ===\n"]}]},{"cell_type":"code","source":["# === Chapter 4 §4.7: Sensitivity & Hyperparameter Studies  ===\n","# Covers:\n","#  4.7.1 Seeds & early stopping (protocol)\n","#  4.7.2 Teacher sensitivity: UCB1 c, rounds, train-steps-per-pull\n","#  4.7.3 Student sensitivity: hidden, layers, dropout, learning rate\n","#  4.7.4 Window length w and horizon H dependence\n","#  4.7.5 Feature set sensitivity: price-only vs. augmented\n","#  4.7.6 Data coverage sensitivity: assets_max, sparse/downsampled train windows\n","\n","import os, json, math, random, warnings\n","from dataclasses import dataclass, asdict\n","from typing import Dict, Tuple, List\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","\n","# -----------------------------\n","# Paths & folders\n","# -----------------------------\n","BASE_DIR = \"/content\"\n","DS_CSV   = f\"{BASE_DIR}/export/tables/dataset_long_1D.csv\"\n","TBL_DIR  = f\"{BASE_DIR}/export/tables\"\n","FIG_DIR  = f\"{BASE_DIR}/export/figures\"\n","for d in [TBL_DIR, FIG_DIR]: os.makedirs(d, exist_ok=True)\n","\n","assert os.path.exists(DS_CSV), f\"Dataset not found: {DS_CSV}\"\n","\n","# -----------------------------\n","# Config (baseline + sweep grids)\n","# -----------------------------\n","@dataclass\n","class SensCfg:\n","    ds_csv: str = DS_CSV\n","    assets_max_base: int = 5\n","    horizons_base: Tuple[int,...] = (1,3,7)\n","    win_base: int = 64\n","    batch_size: int = 256\n","    teacher_rounds_base: int = 80\n","    train_steps_per_pull_base: int = 1\n","    ucb_c_base: float = 1.2\n","    seed_base: int = 1337\n","    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    lstm_hidden_base: int = 64\n","    lstm_layers_base: int = 1\n","    lstm_dropout_base: float = 0.0\n","    lr_base: float = 2e-3\n","    eval_n_trace: int = 200\n","    patience_rounds: int = 15     # early stopping on global val MSE\n","    eval_every: int = 10\n","    # sweep values\n","    seeds: Tuple[int,...] = (42, 1337, 2024)\n","    ucb_c_vals: Tuple[float,...] = (0.6, 1.2, 2.0)\n","    rounds_vals: Tuple[int,...] = (60, 120)\n","    steps_vals: Tuple[int,...] = (1, 2)\n","    hidden_vals: Tuple[int,...] = (32, 64)\n","    layers_vals: Tuple[int,...] = (1, 2)\n","    dropout_vals: Tuple[float,...] = (0.0, 0.2)\n","    lr_vals: Tuple[float,...] = (1e-3, 2e-3)\n","    win_vals: Tuple[int,...] = (32, 64, 128)\n","    feature_sets: Tuple[str,...] = (\"price\",\"aug\")\n","    assets_max_vals: Tuple[int,...] = (3, 5)\n","    downsample_fracs: Tuple[float,...] = (1.0, 0.5, 0.25)\n","\n","CFG = SensCfg()\n","print(\"§4.7 Config\\n\", json.dumps(asdict(CFG), indent=2))\n","\n","# -----------------------------\n","# Utilities\n","# -----------------------------\n","def set_seed(s):\n","    random.seed(s); np.random.seed(s); torch.manual_seed(s);\n","    if torch.cuda.is_available(): torch.cuda.manual_seed_all(s)\n","\n","device = torch.device(CFG.device)\n","\n","def savefig(path):\n","    plt.tight_layout(); plt.savefig(path, dpi=150); plt.close(); return path\n","\n","# -----------------------------\n","# Load dataset and build features (robust coercion; train-only std)\n","# -----------------------------\n","df_all = pd.read_csv(CFG.ds_csv, parse_dates=[\"timestamp\"])\n","\n","# Normalize column names and ensure required fields\n","df_all.columns = [c.strip().lower() for c in df_all.columns]\n","if \"ticker\" in df_all.columns and \"symbol\" not in df_all.columns:\n","    df_all = df_all.rename(columns={\"ticker\":\"symbol\"})\n","if \"asset\" in df_all.columns and \"symbol\" not in df_all.columns:\n","    df_all = df_all.rename(columns={\"asset\":\"symbol\"})\n","if \"price\" in df_all.columns and \"close\" not in df_all.columns:\n","    df_all = df_all.rename(columns={\"price\":\"close\"})\n","if \"close_price\" in df_all.columns and \"close\" not in df_all.columns:\n","    df_all = df_all.rename(columns={\"close_price\":\"close\"})\n","\n","assert {\"timestamp\",\"symbol\",\"close\",\"split\"}.issubset(df_all.columns), \"Dataset missing required columns.\"\n","\n","# Coerce close to numeric (this prevents TypeError during std/mean)\n","df_all[\"close\"] = pd.to_numeric(df_all[\"close\"], errors=\"coerce\")\n","close_nans = int(df_all[\"close\"].isna().sum())\n","if close_nans:\n","    print(f\"[clean] 'close': coerced {close_nans} non-numeric values to NaN; handled downstream.\")\n","\n","# Select top assets by coverage (max cap applied per sweep)\n","def top_assets(df, k):\n","    return (df.groupby(\"symbol\").size().sort_values(ascending=False).head(k).index.tolist())\n","\n","# Train-split close z-score per asset\n","def fit_close_scalers(df, assets):\n","    scalers = {}\n","    for a, g in df[(df[\"split\"]==\"train\") & (df[\"symbol\"].isin(assets))].groupby(\"symbol\"):\n","        mu = pd.to_numeric(g[\"close\"], errors=\"coerce\").mean()\n","        sd = pd.to_numeric(g[\"close\"], errors=\"coerce\").std(ddof=0)\n","        if not np.isfinite(sd) or sd <= 0: sd = 1.0\n","        mu = 0.0 if not np.isfinite(mu) else float(mu)\n","        scalers[a] = (float(mu), float(sd))\n","    return scalers\n","\n","def build_features(df, assets, augmented: bool):\n","    df = df[df[\"symbol\"].isin(assets)].sort_values([\"symbol\",\"timestamp\"]).copy()\n","    # ensure close numeric here too\n","    df[\"close\"] = pd.to_numeric(df[\"close\"], errors=\"coerce\")\n","    scalers = fit_close_scalers(df, assets)\n","    df[\"z\"] = (df[\"close\"] - df[\"symbol\"].map({k:v[0] for k,v in scalers.items()})) / df[\"symbol\"].map({k:v[1] for k,v in scalers.items()})\n","    df[\"z\"] = pd.to_numeric(df[\"z\"], errors=\"coerce\")\n","\n","    if not augmented:\n","        out = df[[\"timestamp\",\"symbol\",\"split\",\"z\"]].copy()\n","        out[\"z\"] = out[\"z\"].replace([np.inf,-np.inf], np.nan).fillna(0.0).clip(-10,10)\n","        return out\n","\n","    # augmented (past-only)\n","    def rsi(series: pd.Series, period: int = 14) -> pd.Series:\n","        series = pd.to_numeric(series, errors=\"coerce\")\n","        delta = series.diff()\n","        up = delta.clip(lower=0.0)\n","        down = -delta.clip(upper=0.0)\n","        roll_up = up.rolling(period, min_periods=period).mean()\n","        roll_down = down.rolling(period, min_periods=period).mean()\n","        rs = roll_up / (roll_down + 1e-12)\n","        return 100.0 - (100.0 / (1.0 + rs))\n","\n","    feats = []\n","    for a, g in df.groupby(\"symbol\"):\n","        g = g.sort_values(\"timestamp\").copy()\n","        z = pd.to_numeric(g[\"z\"], errors=\"coerce\")\n","        dz = z.diff()\n","        vol14 = dz.rolling(14, min_periods=14).std()\n","        rsi14 = rsi(g[\"close\"], 14)\n","        ma5  = z.rolling(5,  min_periods=5).mean()\n","        ma20 = z.rolling(20, min_periods=20).mean()\n","        macd_z = ma5 - ma20\n","        dist_ma20_z = z - ma20\n","\n","        tmp = pd.DataFrame({\n","            \"timestamp\": g[\"timestamp\"].values,\n","            \"symbol\": a,\n","            \"split\": g[\"split\"].values,\n","            \"z\": z.values,\n","            \"dz\": dz.values,\n","            \"vol14\": vol14.values,\n","            \"rsi14\": rsi14.values,\n","            \"macd_z\": macd_z.values,\n","            \"dist_ma20_z\": dist_ma20_z.values\n","        })\n","\n","        # Coerce engineered columns to numeric BEFORE standardization\n","        std_cols = [\"dz\",\"vol14\",\"rsi14\",\"macd_z\",\"dist_ma20_z\"]\n","        for c in [\"z\"] + std_cols:\n","            tmp[c] = pd.to_numeric(tmp[c], errors=\"coerce\")\n","\n","        # standardize engineered feats using TRAIN stats only (per asset)\n","        train_mask = (tmp[\"split\"]==\"train\")\n","        for c in std_cols:\n","            mu = pd.to_numeric(tmp.loc[train_mask, c], errors=\"coerce\").mean()\n","            sd = pd.to_numeric(tmp.loc[train_mask, c], errors=\"coerce\").std(ddof=0)\n","            if not np.isfinite(sd) or sd<=0: sd = 1.0\n","            mu = 0.0 if not np.isfinite(mu) else float(mu)\n","            tmp[c] = (tmp[c] - mu) / sd\n","\n","        # sanitize engineered + z\n","        tmp[[\"z\"]+std_cols] = (\n","            tmp[[\"z\"]+std_cols]\n","            .replace([np.inf,-np.inf], np.nan)\n","            .fillna(0.0)\n","            .clip(-10,10)\n","        )\n","        feats.append(tmp)\n","\n","    feats_std = pd.concat(feats, ignore_index=True)\n","    # final dtype check (defensive)\n","    for c in [\"z\",\"dz\",\"vol14\",\"rsi14\",\"macd_z\",\"dist_ma20_z\"]:\n","        feats_std[c] = pd.to_numeric(feats_std[c], errors=\"coerce\").fillna(0.0)\n","    return feats_std\n","\n","# Sliding-window dataset builder (skip any non-finite window)\n","def make_xy_features(g: pd.DataFrame, feature_cols: List[str], horizon: int, win: int):\n","    vals = g[feature_cols].values.astype(np.float32)\n","    z = g[\"z\"].values.astype(np.float32)\n","    X,Y,Prev = [], [], []\n","    dropped = 0\n","    for t in range(win-1, len(vals)-horizon):\n","        x = vals[t-win+1:t+1, :]\n","        y = z[t+horizon]\n","        p = z[t]\n","        if not (np.isfinite(x).all() and np.isfinite(y) and np.isfinite(p)):\n","            dropped += 1; continue\n","        X.append(x); Y.append([y]); Prev.append([p])\n","    if dropped>0: print(f\"[make_xy_features] Dropped {dropped} windows (win={win}, h={horizon}).\")\n","    if not X: return None, None, None\n","    return np.stack(X), np.stack(Y), np.stack(Prev)\n","\n","def build_arm_data(df_feat: pd.DataFrame, horizons, win) -> Tuple[Dict, List[Tuple[str,int]], pd.DataFrame]:\n","    all_cols = [\"z\",\"dz\",\"vol14\",\"rsi14\",\"macd_z\",\"dist_ma20_z\"]\n","    feature_cols = [c for c in df_feat.columns if c in all_cols]\n","    data_xy, arms, coverage = {}, [], []\n","    for a, g in df_feat.groupby(\"symbol\"):\n","        g = g.sort_values(\"timestamp\")\n","        for split in [\"train\",\"val\",\"test\"]:\n","            gs = g[g[\"split\"]==split].reset_index(drop=True)\n","            for h in horizons:\n","                X,Y,P = make_xy_features(gs, feature_cols, h, win)\n","                if X is not None:\n","                    data_xy[(a,split,h)] = (X,Y,P)\n","                    coverage.append((a,split,h,len(Y)))\n","        for h in horizons:\n","            ok = all(((a,sp,h) in data_xy and data_xy[(a,sp,h)][1].shape[0]>0) for sp in [\"train\",\"val\",\"test\"])\n","            if ok: arms.append((a,h))\n","    cov_df = pd.DataFrame(coverage, columns=[\"asset\",\"split\",\"horizon\",\"samples\"])\n","    return data_xy, arms, cov_df\n","\n","def downsample_train(data_xy: Dict, arms: List[Tuple[str,int]], frac: float):\n","    if frac>=0.999: return data_xy\n","    out = {}\n","    rng = np.random.default_rng(123)\n","    for k in list(data_xy.keys()):\n","        a, split, h = k\n","        X,Y,P = data_xy[k]\n","        if split==\"train\":\n","            n = len(Y); m = max(1, int(round(n*frac)))\n","            idx = rng.choice(n, size=m, replace=False)\n","            out[k] = (X[idx], Y[idx], P[idx])\n","        else:\n","            out[k] = (X,Y,P)\n","    return out\n","\n","# -----------------------------\n","# Models & training\n","# -----------------------------\n","class PolicyLSTM(nn.Module):\n","    def __init__(self, input_dim=1, hidden=64, layers=1, dropout=0.0):\n","        super().__init__()\n","        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden, num_layers=layers,\n","                            dropout=(dropout if layers>1 else 0.0), batch_first=True)\n","        self.mu = nn.Sequential(nn.Linear(hidden, hidden), nn.ReLU(), nn.Linear(hidden, 1))\n","        self.log_sigma = nn.Parameter(torch.tensor(-0.5))\n","    def forward(self, x):\n","        o,_ = self.lstm(x)\n","        last = o[:, -1, :]\n","        mu = self.mu(last)\n","        log_sigma = self.log_sigma.expand_as(mu)\n","        mu = torch.nan_to_num(mu, nan=0.0, posinf=0.0, neginf=0.0)\n","        return mu, log_sigma\n","\n","def count_params(m): return sum(p.numel() for p in m.parameters() if p.requires_grad)\n","\n","class UCB1Teacher:\n","    def __init__(self, n_arms, c=1.2):\n","        self.c=c; self.n=np.zeros(n_arms, dtype=np.int64); self.mean=np.zeros(n_arms, dtype=np.float64); self.t=0\n","    def select(self):\n","        self.t+=1\n","        for i in range(len(self.n)):\n","            if self.n[i]==0: return i\n","        ucb = self.mean + self.c*np.sqrt(2.0*math.log(self.t)/self.n)\n","        return int(np.argmax(ucb))\n","    def update(self, i, reward):\n","        self.n[i]+=1\n","        self.mean[i]+= (reward - self.mean[i])/self.n[i]\n","\n","class UniformTeacher:\n","    def __init__(self, n_arms):\n","        self.n = np.zeros(n_arms, dtype=np.int64); self.mean = np.zeros(n_arms, dtype=np.float64); self.t=0\n","    def select(self):\n","        self.t+=1\n","        return int(np.random.randint(0,len(self.n)))\n","    def update(self, i, reward):\n","        self.n[i]+=1\n","        self.mean[i]+= (reward - self.mean[i])/self.n[i]\n","\n","# metrics\n","def mae(y, yhat): return float(np.mean(np.abs(y - yhat)))\n","def rmse(y, yhat): return float(np.sqrt(np.mean((y - yhat)**2)))\n","def smape(y, yhat, eps=1e-8):\n","    denom = (np.abs(y) + np.abs(yhat) + eps)/2.0\n","    return float(np.mean(np.abs(y - yhat)/denom) * 100.0)\n","def dir_acc(y_prev, y_true, yhat):\n","    s_true = np.sign(y_true - y_prev); s_pred = np.sign(yhat - y_prev)\n","    return float(np.mean((s_true == s_pred).astype(np.float32)))\n","\n","def sample_minibatch(data_xy, arms, arm_idx, bs):\n","    a,h = arms[arm_idx]\n","    X,Y,P = data_xy[(a,\"train\",h)]\n","    if len(Y)==0:\n","        idx = np.array([0])\n","    else:\n","        idx = np.random.randint(0,len(Y), size=(min(bs, len(Y)),))\n","    xb = torch.from_numpy(X[idx]).float().to(device)\n","    yb = torch.from_numpy(Y[idx]).float().to(device)\n","    pb = torch.from_numpy(P[idx]).float().to(device)\n","    return xb, yb, pb\n","\n","def eval_val_mse(model, data_xy, arms, arm_idx):\n","    a,h = arms[arm_idx]\n","    X,Y,_ = data_xy[(a,\"val\",h)]\n","    with torch.no_grad():\n","        xb = torch.from_numpy(X).float().to(device)\n","        mu, _ = model(xb)\n","        yhat = mu.squeeze(1).cpu().numpy()\n","    return float(np.mean((Y.squeeze(1) - yhat)**2))\n","\n","def eval_overall_val_mse(model, data_xy, arms):\n","    vals = []\n","    for i,_ in enumerate(arms):\n","        vals.append(eval_val_mse(model, data_xy, arms, i))\n","    return float(np.mean(vals)) if vals else np.nan\n","\n","def evaluate_test(model, data_xy, arms):\n","    rows = []\n","    for (a,h) in arms:\n","        Xte,Yte,Prev = data_xy[(a,\"test\",h)]\n","        with torch.no_grad():\n","            xb = torch.from_numpy(Xte).float().to(device)\n","            mu,_ = model(xb)\n","            yhat = mu.squeeze(1).cpu().numpy()\n","        y = Yte.squeeze(1); p = Prev.squeeze(1)\n","        rows.append(dict(asset=a, horizon=h, MAE=mae(y,yhat), RMSE=rmse(y,yhat),\n","                         sMAPE=smape(y,yhat), DA=dir_acc(p,y,yhat)))\n","    per_arm = pd.DataFrame(rows).sort_values([\"horizon\",\"asset\"])\n","    agg = (per_arm.groupby(\"horizon\").agg(MAE=(\"MAE\",\"mean\"), RMSE=(\"RMSE\",\"mean\"),\n","                                          sMAPE=(\"sMAPE\",\"mean\"), DA=(\"DA\",\"mean\"))\n","           .reset_index().sort_values(\"horizon\"))\n","    return per_arm, agg\n","\n","def bandit_run_once(seed, df_feat, arms, data_xy, *,\n","                    hidden, layers, dropout, lr,\n","                    ucb_c, rounds, steps_per_pull,\n","                    batch_size=CFG.batch_size, patience=CFG.patience_rounds, eval_every=CFG.eval_every):\n","    set_seed(seed)\n","    input_dim = len([c for c in df_feat.columns if c in [\"z\",\"dz\",\"vol14\",\"rsi14\",\"macd_z\",\"dist_ma20_z\"]])\n","    model = PolicyLSTM(input_dim=input_dim, hidden=hidden, layers=layers, dropout=dropout).to(device)\n","    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n","    teacher = UCB1Teacher(len(arms), c=ucb_c)\n","\n","    def reinforce_step(xb, yb):\n","        model.train(); opt.zero_grad()\n","        mu, log_sigma = model(xb)\n","        sigma = torch.exp(log_sigma)\n","        dist = torch.distributions.Normal(mu, sigma)\n","        a = dist.rsample()\n","        r = - (a - yb)**2\n","        reinforce_step.baseline = 0.9*reinforce_step.baseline + 0.1*r.mean().detach() if hasattr(reinforce_step,\"baseline\") else r.mean().detach()\n","        adv = r - reinforce_step.baseline\n","        loss = - (dist.log_prob(a) * adv.detach()).mean()\n","        loss += - 1e-3 * dist.entropy().mean()\n","        loss += 0.1 * nn.MSELoss()(mu, yb)\n","        loss.backward()\n","        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        opt.step()\n","\n","    # warm\n","    for i in range(min(len(arms), 3)):\n","        xb,yb,_ = sample_minibatch(data_xy, arms, i, batch_size)\n","        reinforce_step(xb,yb)\n","\n","    best = (np.inf, None, 0)  # (global_val, state_dict, rounds_used)\n","    no_improve = 0\n","    for t in range(1, rounds+1):\n","        i = teacher.select()\n","        vb = eval_val_mse(model, data_xy, arms, i)\n","        for _ in range(steps_per_pull):\n","            xb,yb,_ = sample_minibatch(data_xy, arms, i, batch_size)\n","            reinforce_step(xb,yb)\n","        va = eval_val_mse(model, data_xy, arms, i)\n","        teacher.update(i, float(vb - va))\n","\n","        if t % eval_every == 0:\n","            gval = eval_overall_val_mse(model, data_xy, arms)\n","            if gval < best[0]:\n","                best = (gval, {k:v.cpu().clone() for k,v in model.state_dict().items()}, t)\n","                no_improve = 0\n","            else:\n","                no_improve += 1\n","            if no_improve >= patience:\n","                break\n","\n","    rounds_used = best[2] if best[1] is not None else rounds\n","    if best[1] is not None: model.load_state_dict(best[1])\n","\n","    per_arm, agg = evaluate_test(model, data_xy, arms)\n","    per_arm[\"rounds_used\"] = rounds_used\n","    agg[\"rounds_used\"] = rounds_used\n","    agg[\"params\"] = count_params(model)\n","    return per_arm, agg\n","\n","# -----------------------------\n","# Helper to prepare data per feature set / assets_max / win\n","# -----------------------------\n","def prepare_data(feature_set=\"price\", assets_max=CFG.assets_max_base, win=CFG.win_base, horizons=CFG.horizons_base):\n","    assets = top_assets(df_all, assets_max)\n","    df_feat = build_features(df_all, assets, augmented=(feature_set==\"aug\"))\n","    data_xy, arms, cov = build_arm_data(df_feat, horizons, win)\n","    return df_feat, data_xy, arms, cov\n","\n","# Baseline data for most sweeps (price-only) + augmented for §4.7.5\n","df_price, data_xy_price, arms_price, cov_price = prepare_data(\"price\", CFG.assets_max_base, CFG.win_base, CFG.horizons_base)\n","assert len(arms_price)>0, \"No feasible arms for baseline (price).\"\n","df_aug, data_xy_aug, arms_aug, cov_aug = prepare_data(\"aug\", CFG.assets_max_base, CFG.win_base, CFG.horizons_base)\n","arms_common = sorted(list(set(arms_price).intersection(set(arms_aug))))\n","if len(arms_common)==0:\n","    arms_common = arms_price  # fallback\n","\n","print(\"Baseline feasible arms (price):\", arms_price)\n","print(\"Feasible arms (aug)          :\", arms_aug)\n","print(\"Arms used for shared comparisons:\", arms_common)\n","\n","# -----------------------------\n","# 4.7.1 Seeds & early stopping\n","# -----------------------------\n","def sweep_seeds():\n","    rows=[]\n","    for s in CFG.seeds:\n","        per_arm, agg = bandit_run_once(s, df_price, arms_price, data_xy_price,\n","                                       hidden=CFG.lstm_hidden_base, layers=CFG.lstm_layers_base,\n","                                       dropout=CFG.lstm_dropout_base, lr=CFG.lr_base,\n","                                       ucb_c=CFG.ucb_c_base, rounds=CFG.teacher_rounds_base,\n","                                       steps_per_pull=CFG.train_steps_per_pull_base)\n","        agg[\"seed\"]=s; rows.append(agg)\n","    out = pd.concat(rows, ignore_index=True)\n","    out.to_csv(f\"{TBL_DIR}/sens_471_seeds.csv\", index=False)\n","    # plot variance across seeds (MAE by horizon)\n","    plt.figure(figsize=(7.2,4))\n","    for h in CFG.horizons_base:\n","        tmp = out[out[\"horizon\"]==h]\n","        plt.plot(tmp[\"seed\"].astype(str), tmp[\"MAE\"].values, marker=\"o\", label=f\"H={h}\")\n","    plt.xlabel(\"Seed\"); plt.ylabel(\"MAE\"); plt.title(\"Seed sensitivity (MAE)\"); plt.legend()\n","    savefig(f\"{FIG_DIR}/sens_471_seeds_mae.png\")\n","    return out\n","\n","# -----------------------------\n","# 4.7.2 Teacher sensitivity (c, rounds, steps)\n","# -----------------------------\n","def sweep_teacher():\n","    rows=[]\n","    # c\n","    for c in CFG.ucb_c_vals:\n","        _, agg = bandit_run_once(CFG.seed_base, df_price, arms_price, data_xy_price,\n","                                 hidden=CFG.lstm_hidden_base, layers=CFG.lstm_layers_base,\n","                                 dropout=CFG.lstm_dropout_base, lr=CFG.lr_base,\n","                                 ucb_c=c, rounds=CFG.teacher_rounds_base,\n","                                 steps_per_pull=CFG.train_steps_per_pull_base)\n","        agg[\"sweep\"]=\"ucb_c\"; agg[\"value\"]=c; rows.append(agg)\n","    # rounds\n","    for r in CFG.rounds_vals:\n","        _, agg = bandit_run_once(CFG.seed_base, df_price, arms_price, data_xy_price,\n","                                 hidden=CFG.lstm_hidden_base, layers=CFG.lstm_layers_base,\n","                                 dropout=CFG.lstm_dropout_base, lr=CFG.lr_base,\n","                                 ucb_c=CFG.ucb_c_base, rounds=r,\n","                                 steps_per_pull=CFG.train_steps_per_pull_base)\n","        agg[\"sweep\"]=\"rounds\"; agg[\"value\"]=r; rows.append(agg)\n","    # steps\n","    for sp in CFG.steps_vals:\n","        _, agg = bandit_run_once(CFG.seed_base, df_price, arms_price, data_xy_price,\n","                                 hidden=CFG.lstm_hidden_base, layers=CFG.lstm_layers_base,\n","                                 dropout=CFG.lstm_dropout_base, lr=CFG.lr_base,\n","                                 ucb_c=CFG.ucb_c_base, rounds=CFG.teacher_rounds_base,\n","                                 steps_per_pull=sp)\n","        agg[\"sweep\"]=\"steps\"; agg[\"value\"]=sp; rows.append(agg)\n","    out = pd.concat(rows, ignore_index=True)\n","    out.to_csv(f\"{TBL_DIR}/sens_472_teacher.csv\", index=False)\n","    # figures\n","    def lineplot(sub, title, fname):\n","        plt.figure(figsize=(6.8,3.8))\n","        for h in CFG.horizons_base:\n","            tmp = sub[sub[\"horizon\"]==h].sort_values(\"value\")\n","            plt.plot(tmp[\"value\"].astype(float), tmp[\"MAE\"].values, marker=\"o\", label=f\"H={h}\")\n","        plt.xlabel(sub[\"sweep\"].iloc[0]); plt.ylabel(\"MAE\"); plt.title(title); plt.legend()\n","        savefig(f\"{FIG_DIR}/{fname}\")\n","    lineplot(out[out[\"sweep\"]==\"ucb_c\"], \"Teacher UCB c vs. MAE\", \"sens_472_ucb_c_vs_mae.png\")\n","    lineplot(out[out[\"sweep\"]==\"rounds\"], \"Teacher rounds vs. MAE\", \"sens_472_rounds_vs_mae.png\")\n","    lineplot(out[out[\"sweep\"]==\"steps\"],  \"Train-steps-per-pull vs. MAE\", \"sens_472_steps_vs_mae.png\")\n","    return out\n","\n","# -----------------------------\n","# 4.7.3 Student sensitivity (one-factor sweeps)\n","# -----------------------------\n","def sweep_student():\n","    outs=[]\n","    # hidden\n","    rows=[]\n","    for hdim in CFG.hidden_vals:\n","        _, agg = bandit_run_once(CFG.seed_base, df_price, arms_price, data_xy_price,\n","                                 hidden=hdim, layers=CFG.lstm_layers_base,\n","                                 dropout=CFG.lstm_dropout_base, lr=CFG.lr_base,\n","                                 ucb_c=CFG.ucb_c_base, rounds=CFG.teacher_rounds_base,\n","                                 steps_per_pull=CFG.train_steps_per_pull_base)\n","        agg[\"sweep\"]=\"hidden\"; agg[\"value\"]=hdim; rows.append(agg)\n","    out_h = pd.concat(rows, ignore_index=True); out_h.to_csv(f\"{TBL_DIR}/sens_473_student_hidden.csv\", index=False); outs.append(out_h)\n","\n","    # layers\n","    rows=[]\n","    for L in CFG.layers_vals:\n","        _, agg = bandit_run_once(CFG.seed_base, df_price, arms_price, data_xy_price,\n","                                 hidden=CFG.lstm_hidden_base, layers=L,\n","                                 dropout=CFG.lstm_dropout_base, lr=CFG.lr_base,\n","                                 ucb_c=CFG.ucb_c_base, rounds=CFG.teacher_rounds_base,\n","                                 steps_per_pull=CFG.train_steps_per_pull_base)\n","        agg[\"sweep\"]=\"layers\"; agg[\"value\"]=L; rows.append(agg)\n","    out_L = pd.concat(rows, ignore_index=True); out_L.to_csv(f\"{TBL_DIR}/sens_473_student_layers.csv\", index=False); outs.append(out_L)\n","\n","    # dropout\n","    rows=[]\n","    for dp in CFG.dropout_vals:\n","        _, agg = bandit_run_once(CFG.seed_base, df_price, arms_price, data_xy_price,\n","                                 hidden=CFG.lstm_hidden_base, layers=CFG.lstm_layers_base,\n","                                 dropout=dp, lr=CFG.lr_base,\n","                                 ucb_c=CFG.ucb_c_base, rounds=CFG.teacher_rounds_base,\n","                                 steps_per_pull=CFG.train_steps_per_pull_base)\n","        agg[\"sweep\"]=\"dropout\"; agg[\"value\"]=dp; rows.append(agg)\n","    out_D = pd.concat(rows, ignore_index=True); out_D.to_csv(f\"{TBL_DIR}/sens_473_student_dropout.csv\", index=False); outs.append(out_D)\n","\n","    # lr\n","    rows=[]\n","    for lr in CFG.lr_vals:\n","        _, agg = bandit_run_once(CFG.seed_base, df_price, arms_price, data_xy_price,\n","                                 hidden=CFG.lstm_hidden_base, layers=CFG.lstm_layers_base,\n","                                 dropout=CFG.lstm_dropout_base, lr=lr,\n","                                 ucb_c=CFG.ucb_c_base, rounds=CFG.teacher_rounds_base,\n","                                 steps_per_pull=CFG.train_steps_per_pull_base)\n","        agg[\"sweep\"]=\"lr\"; agg[\"value\"]=lr; rows.append(agg)\n","    out_R = pd.concat(rows, ignore_index=True); out_R.to_csv(f\"{TBL_DIR}/sens_473_student_lr.csv\", index=False); outs.append(out_R)\n","\n","    # plots\n","    def plot_one(sub, label, fname):\n","        plt.figure(figsize=(6.8,3.8))\n","        for h in CFG.horizons_base:\n","            tmp = sub[sub[\"horizon\"]==h].sort_values(\"value\")\n","            plt.plot(tmp[\"value\"].astype(float), tmp[\"MAE\"].values, marker=\"o\", label=f\"H={h}\")\n","        plt.xlabel(label); plt.ylabel(\"MAE\"); plt.title(f\"Student {label} vs. MAE\"); plt.legend()\n","        savefig(f\"{FIG_DIR}/{fname}\")\n","    plot_one(out_h, \"hidden\", \"sens_473_hidden_vs_mae.png\")\n","    plot_one(out_L, \"layers\", \"sens_473_layers_vs_mae.png\")\n","    plot_one(out_D, \"dropout\", \"sens_473_dropout_vs_mae.png\")\n","    plot_one(out_R, \"lr\",      \"sens_473_lr_vs_mae.png\")\n","\n","    return outs\n","\n","# -----------------------------\n","# 4.7.4 Window length w & horizon H\n","# -----------------------------\n","def sweep_window():\n","    rows=[]\n","    for w in CFG.win_vals:\n","        dfp, dxp, armsp, _ = prepare_data(\"price\", CFG.assets_max_base, w, CFG.horizons_base)\n","        _, agg = bandit_run_once(CFG.seed_base, dfp, armsp, dxp,\n","                                 hidden=CFG.lstm_hidden_base, layers=CFG.lstm_layers_base,\n","                                 dropout=CFG.lstm_dropout_base, lr=CFG.lr_base,\n","                                 ucb_c=CFG.ucb_c_base, rounds=CFG.teacher_rounds_base,\n","                                 steps_per_pull=CFG.train_steps_per_pull_base)\n","        agg[\"w\"]=w; rows.append(agg)\n","    out = pd.concat(rows, ignore_index=True); out.to_csv(f\"{TBL_DIR}/sens_474_win.csv\", index=False)\n","    # plot MAE vs w for each horizon\n","    plt.figure(figsize=(7.2,4))\n","    for h in CFG.horizons_base:\n","        tmp = out[out[\"horizon\"]==h].sort_values(\"w\")\n","        plt.plot(tmp[\"w\"], tmp[\"MAE\"], marker=\"o\", label=f\"H={h}\")\n","    plt.xlabel(\"Window length (w)\"); plt.ylabel(\"MAE\"); plt.title(\"Window length vs. MAE by horizon\"); plt.legend()\n","    savefig(f\"{FIG_DIR}/sens_474_w_by_horizon_mae.png\")\n","    return out\n","\n","# -----------------------------\n","# 4.7.5 Feature set sensitivity\n","# -----------------------------\n","def sweep_features():\n","    rows=[]\n","    # price\n","    _, agg_p = bandit_run_once(CFG.seed_base, df_price, arms_common, data_xy_price,\n","                               hidden=CFG.lstm_hidden_base, layers=CFG.lstm_layers_base,\n","                               dropout=CFG.lstm_dropout_base, lr=CFG.lr_base,\n","                               ucb_c=CFG.ucb_c_base, rounds=CFG.teacher_rounds_base,\n","                               steps_per_pull=CFG.train_steps_per_pull_base)\n","    agg_p[\"feature_set\"]=\"price\"; rows.append(agg_p)\n","    # aug\n","    _, agg_a = bandit_run_once(CFG.seed_base, df_aug, arms_common, data_xy_aug,\n","                               hidden=CFG.lstm_hidden_base, layers=CFG.lstm_layers_base,\n","                               dropout=CFG.lstm_dropout_base, lr=CFG.lr_base,\n","                               ucb_c=CFG.ucb_c_base, rounds=CFG.teacher_rounds_base,\n","                               steps_per_pull=CFG.train_steps_per_pull_base)\n","    agg_a[\"feature_set\"]=\"aug\"; rows.append(agg_a)\n","    out = pd.concat(rows, ignore_index=True); out.to_csv(f\"{TBL_DIR}/sens_475_features.csv\", index=False)\n","    # bar plot\n","    plt.figure(figsize=(6.8,3.8))\n","    for h in CFG.horizons_base:\n","        tmp = out[out[\"horizon\"]==h]\n","        x = np.arange(2)\n","        vals = [tmp[tmp[\"feature_set\"]==\"price\"][\"MAE\"].values[0],\n","                tmp[tmp[\"feature_set\"]==\"aug\"][\"MAE\"].values[0]]\n","        plt.bar(x + (h-1)*0.25, vals, width=0.22, label=f\"H={h}\")\n","    plt.xticks(x + 0.25, [\"price\",\"aug\"])\n","    plt.ylabel(\"MAE\"); plt.title(\"Feature set sensitivity (MAE)\"); plt.legend()\n","    savefig(f\"{FIG_DIR}/sens_475_feature_set_bars.png\")\n","    return out\n","\n","# -----------------------------\n","# 4.7.6 Data coverage sensitivity\n","# -----------------------------\n","def sweep_coverage():\n","    # (i) assets_max\n","    rows=[]\n","    for k in CFG.assets_max_vals:\n","        dfp, dxp, armsp, _ = prepare_data(\"price\", k, CFG.win_base, CFG.horizons_base)\n","        _, agg = bandit_run_once(CFG.seed_base, dfp, armsp, dxp,\n","                                 hidden=CFG.lstm_hidden_base, layers=CFG.lstm_layers_base,\n","                                 dropout=CFG.lstm_dropout_base, lr=CFG.lr_base,\n","                                 ucb_c=CFG.ucb_c_base, rounds=CFG.teacher_rounds_base,\n","                                 steps_per_pull=CFG.train_steps_per_pull_base)\n","        agg[\"assets_max\"]=k; rows.append(agg)\n","    out_assets = pd.concat(rows, ignore_index=True); out_assets.to_csv(f\"{TBL_DIR}/sens_476_coverage_assetsmax.csv\", index=False)\n","    plt.figure(figsize=(6.8,3.8))\n","    for h in CFG.horizons_base:\n","        tmp = out_assets[out_assets[\"horizon\"]==h].sort_values(\"assets_max\")\n","        plt.plot(tmp[\"assets_max\"], tmp[\"MAE\"], marker=\"o\", label=f\"H={h}\")\n","    plt.xlabel(\"assets_max\"); plt.ylabel(\"MAE\"); plt.title(\"assets_max vs. MAE\"); plt.legend()\n","    savefig(f\"{FIG_DIR}/sens_476_assetsmax_bars.png\")\n","\n","    # (ii) downsample training\n","    rows=[]\n","    for frac in CFG.downsample_fracs:\n","        dx = downsample_train(data_xy_price, arms_price, frac)\n","        _, agg = bandit_run_once(CFG.seed_base, df_price, arms_price, dx,\n","                                 hidden=CFG.lstm_hidden_base, layers=CFG.lstm_layers_base,\n","                                 dropout=CFG.lstm_dropout_base, lr=CFG.lr_base,\n","                                 ucb_c=CFG.ucb_c_base, rounds=CFG.teacher_rounds_base,\n","                                 steps_per_pull=CFG.train_steps_per_pull_base)\n","        agg[\"train_frac\"]=frac; rows.append(agg)\n","    out_down = pd.concat(rows, ignore_index=True); out_down.to_csv(f\"{TBL_DIR}/sens_476_coverage_downsample.csv\", index=False)\n","    # figure\n","    plt.figure(figsize=(6.8,3.8))\n","    for h in CFG.horizons_base:\n","        tmp = out_down[out_down[\"horizon\"]==h].sort_values(\"train_frac\")\n","        plt.plot(tmp[\"train_frac\"], tmp[\"MAE\"], marker=\"o\", label=f\"H={h}\")\n","    plt.xlabel(\"Train fraction kept\"); plt.ylabel(\"MAE\"); plt.title(\"Training coverage vs. MAE\"); plt.legend()\n","    savefig(f\"{FIG_DIR}/sens_476_downsample_line.png\")\n","\n","    return out_assets, out_down\n","\n","# -----------------------------\n","# RUN ALL SWEEPS\n","# -----------------------------\n","print(\"\\nRunning §4.7 sweeps... (this is a compact, CPU-friendly run)\")\n","res_471 = sweep_seeds()\n","res_472 = sweep_teacher()\n","res_473_hidden, res_473_layers, res_473_dropout, res_473_lr = sweep_student()\n","res_474 = sweep_window()\n","res_475 = sweep_features()\n","res_476_assets, res_476_down = sweep_coverage()\n","\n","# -----------------------------\n","# PRINT SUMMARY PATHS\n","# -----------------------------\n","print(\"\\n=== §4.7 SENSITIVITY — ARTIFACT SUMMARY ===\")\n","tbls = [f for f in sorted(os.listdir(TBL_DIR)) if f.startswith(\"sens_\")]\n","figs = [f for f in sorted(os.listdir(FIG_DIR)) if f.startswith(\"sens_\") and f.endswith(\".png\")]\n","print(\"Tables:\")\n","for t in tbls: print(\" -\", os.path.join(TBL_DIR, t))\n","print(\"Figures:\")\n","for f in figs[:24]: print(\" -\", os.path.join(FIG_DIR, f))\n","if len(figs)>24: print(f\" ... and {len(figs)-24} more figures\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qpup9rCcf_Bh","executionInfo":{"status":"ok","timestamp":1761567406654,"user_tz":-180,"elapsed":281956,"user":{"displayName":"David Kibaara","userId":"07870126442739887481"}},"outputId":"b3d83a82-92fd-42d8-dada-29968e3bf888"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["§4.7 Config\n"," {\n","  \"ds_csv\": \"/content/export/tables/dataset_long_1D.csv\",\n","  \"assets_max_base\": 5,\n","  \"horizons_base\": [\n","    1,\n","    3,\n","    7\n","  ],\n","  \"win_base\": 64,\n","  \"batch_size\": 256,\n","  \"teacher_rounds_base\": 80,\n","  \"train_steps_per_pull_base\": 1,\n","  \"ucb_c_base\": 1.2,\n","  \"seed_base\": 1337,\n","  \"device\": \"cpu\",\n","  \"lstm_hidden_base\": 64,\n","  \"lstm_layers_base\": 1,\n","  \"lstm_dropout_base\": 0.0,\n","  \"lr_base\": 0.002,\n","  \"eval_n_trace\": 200,\n","  \"patience_rounds\": 15,\n","  \"eval_every\": 10,\n","  \"seeds\": [\n","    42,\n","    1337,\n","    2024\n","  ],\n","  \"ucb_c_vals\": [\n","    0.6,\n","    1.2,\n","    2.0\n","  ],\n","  \"rounds_vals\": [\n","    60,\n","    120\n","  ],\n","  \"steps_vals\": [\n","    1,\n","    2\n","  ],\n","  \"hidden_vals\": [\n","    32,\n","    64\n","  ],\n","  \"layers_vals\": [\n","    1,\n","    2\n","  ],\n","  \"dropout_vals\": [\n","    0.0,\n","    0.2\n","  ],\n","  \"lr_vals\": [\n","    0.001,\n","    0.002\n","  ],\n","  \"win_vals\": [\n","    32,\n","    64,\n","    128\n","  ],\n","  \"feature_sets\": [\n","    \"price\",\n","    \"aug\"\n","  ],\n","  \"assets_max_vals\": [\n","    3,\n","    5\n","  ],\n","  \"downsample_fracs\": [\n","    1.0,\n","    0.5,\n","    0.25\n","  ]\n","}\n","[clean] 'close': coerced 5507 non-numeric values to NaN; handled downstream.\n","Baseline feasible arms (price): [('CLOSE', 1), ('CLOSE', 3), ('CLOSE', 7), ('HIGH', 1), ('HIGH', 3), ('HIGH', 7), ('LOW', 1), ('LOW', 3), ('LOW', 7), ('OPEN', 1), ('OPEN', 3), ('OPEN', 7), ('TICKER', 1), ('TICKER', 3), ('TICKER', 7)]\n","Feasible arms (aug)          : [('CLOSE', 1), ('CLOSE', 3), ('CLOSE', 7), ('HIGH', 1), ('HIGH', 3), ('HIGH', 7), ('LOW', 1), ('LOW', 3), ('LOW', 7), ('OPEN', 1), ('OPEN', 3), ('OPEN', 7), ('TICKER', 1), ('TICKER', 3), ('TICKER', 7)]\n","Arms used for shared comparisons: [('CLOSE', 1), ('CLOSE', 3), ('CLOSE', 7), ('HIGH', 1), ('HIGH', 3), ('HIGH', 7), ('LOW', 1), ('LOW', 3), ('LOW', 7), ('OPEN', 1), ('OPEN', 3), ('OPEN', 7), ('TICKER', 1), ('TICKER', 3), ('TICKER', 7)]\n","\n","Running §4.7 sweeps... (this is a compact, CPU-friendly run)\n","\n","=== §4.7 SENSITIVITY — ARTIFACT SUMMARY ===\n","Tables:\n"," - /content/export/tables/sens_471_seeds.csv\n"," - /content/export/tables/sens_472_teacher.csv\n"," - /content/export/tables/sens_473_student_dropout.csv\n"," - /content/export/tables/sens_473_student_hidden.csv\n"," - /content/export/tables/sens_473_student_layers.csv\n"," - /content/export/tables/sens_473_student_lr.csv\n"," - /content/export/tables/sens_474_win.csv\n"," - /content/export/tables/sens_475_features.csv\n"," - /content/export/tables/sens_476_coverage_assetsmax.csv\n"," - /content/export/tables/sens_476_coverage_downsample.csv\n","Figures:\n"," - /content/export/figures/sens_471_seeds_mae.png\n"," - /content/export/figures/sens_472_rounds_vs_mae.png\n"," - /content/export/figures/sens_472_steps_vs_mae.png\n"," - /content/export/figures/sens_472_ucb_c_vs_mae.png\n"," - /content/export/figures/sens_473_dropout_vs_mae.png\n"," - /content/export/figures/sens_473_hidden_vs_mae.png\n"," - /content/export/figures/sens_473_layers_vs_mae.png\n"," - /content/export/figures/sens_473_lr_vs_mae.png\n"," - /content/export/figures/sens_474_w_by_horizon_mae.png\n"," - /content/export/figures/sens_475_feature_set_bars.png\n"," - /content/export/figures/sens_476_assetsmax_bars.png\n"," - /content/export/figures/sens_476_downsample_line.png\n"]}]}]}