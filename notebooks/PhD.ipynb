{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":52954,"status":"ok","timestamp":1761543219782,"user":{"displayName":"David Kibaara","userId":"07870126442739887481"},"user_tz":-180},"id":"rCN_FuzrT6Ki","outputId":"d6ce6976-e162-4f69-8dd1-81e451c3e7e0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":2845,"status":"error","timestamp":1761543222631,"user":{"displayName":"David Kibaara","userId":"07870126442739887481"},"user_tz":-180},"id":"qWqncHhZP2Xc","outputId":"5296d1bc-eff6-40b1-cec0-fff36a2bf819"},"outputs":[{"output_type":"stream","name":"stdout","text":["Experiment Config:\n"," {\n","  \"assets\": [\n","    \"BTC\",\n","    \"ETH\",\n","    \"BNB\",\n","    \"XRP\",\n","    \"ADA\",\n","    \"SOL\",\n","    \"DOGE\",\n","    \"TRX\",\n","    \"DOT\",\n","    \"LTC\"\n","  ],\n","  \"freq\": \"1D\",\n","  \"tz\": \"UTC\",\n","  \"t_train_end\": \"2022-12-31\",\n","  \"t_val_end\": \"2023-12-31\",\n","  \"t_test_end\": \"2025-09-30\",\n","  \"min_rows_per_asset\": 500,\n","  \"expect_cols\": [\n","    \"timestamp\",\n","    \"symbol\",\n","    \"open\",\n","    \"high\",\n","    \"low\",\n","    \"close\",\n","    \"volume\"\n","  ],\n","  \"data_paths\": [\n","    \"/content/drive/My Drive/PHD/Sep 2025/Dataset/archive (9)/*.csv\"\n","  ]\n","}\n"]},{"output_type":"error","ename":"ValueError","evalue":"value_name (close) cannot match an element in the DataFrame columns.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2234021919.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;31m# 3) Load & filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;31m# -----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m \u001b[0mraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;31m# Keep configured assets if present; otherwise keep top-N by coverage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0masset_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'symbol'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-2234021919.py\u001b[0m in \u001b[0;36mload_any\u001b[0;34m(paths)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mid_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mvalue_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mid_vars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0mdf0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmelt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'symbol'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'close'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0mdf0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_standardize_cols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mmelt\u001b[0;34m(self, id_vars, value_vars, var_name, value_name, col_level, ignore_index)\u001b[0m\n\u001b[1;32m   9940\u001b[0m         \u001b[0mignore_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9941\u001b[0m     ) -> DataFrame:\n\u001b[0;32m-> 9942\u001b[0;31m         return melt(\n\u001b[0m\u001b[1;32m   9943\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9944\u001b[0m             \u001b[0mid_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid_vars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/reshape/melt.py\u001b[0m in \u001b[0;36mmelt\u001b[0;34m(frame, id_vars, value_vars, var_name, value_name, col_level, ignore_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m ) -> DataFrame:\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalue_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0;34mf\"value_name ({value_name}) cannot match an element in \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;34m\"the DataFrame columns.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: value_name (close) cannot match an element in the DataFrame columns."]}],"source":["# === Chapter 4 Experiment Scaffold: Dataset audit & splits ===\n","# Colab-ready. No internet required. Place your CSV(s) in Google Drive or upload via the Colab sidebar.\n","# Recommended schema (long): timestamp,symbol,open,high,low,close,volume\n","# If you have wide format (one file per asset), the loader will try to infer and melt.\n","\n","import os, re, json, textwrap, glob\n","from dataclasses import dataclass, asdict\n","from datetime import datetime\n","import numpy as np\n","import pandas as pd\n","\n","# -----------------------------\n","# 0) Reproducibility & folders\n","# -----------------------------\n","np.random.seed(1337)\n","\n","BASE_DIR = \"/content\"\n","EXPORT_DIRS = [f\"{BASE_DIR}/export/tables\", f\"{BASE_DIR}/export/figures\", f\"{BASE_DIR}/export/logs\"]\n","for d in EXPORT_DIRS:\n","    os.makedirs(d, exist_ok=True)\n","\n","# -----------------------------\n","# 1) Minimal experiment config\n","# -----------------------------\n","@dataclass\n","class ExpConfig:\n","    assets: list = tuple([\"BTC\", \"ETH\", \"BNB\", \"XRP\", \"ADA\", \"SOL\", \"DOGE\", \"TRX\", \"DOT\", \"LTC\"])\n","    freq: str = \"1D\"                     # \"1D\" or \"1H\"\n","    tz: str = \"UTC\"\n","    t_train_end: str = \"2022-12-31\"\n","    t_val_end: str = \"2023-12-31\"\n","    t_test_end: str = \"2025-09-30\"\n","    min_rows_per_asset: int = 500        # skip extremely sparse series\n","    expect_cols: tuple = tuple([\"timestamp\",\"symbol\",\"open\",\"high\",\"low\",\"close\",\"volume\"])\n","    # Update paths to where you put your CSV(s)\n","    data_paths: list = tuple([\n","        # Use a forward slash and the '/content/drive/' prefix\n","        \"/content/drive/My Drive/PHD/Sep 2025/Dataset/archive (9)/*.csv\"\n","    ])\n","\n","CFG = ExpConfig()\n","print(\"Experiment Config:\\n\", json.dumps(asdict(CFG), indent=2))\n","\n","# -----------------------------\n","# 2) Load helper\n","# -----------------------------\n","def _standardize_cols(df: pd.DataFrame) -> pd.DataFrame:\n","    colmap = {c.lower().strip(): c for c in df.columns}\n","    # build lower-case copy and align to expected\n","    df = df.rename(columns={c: c.lower().strip() for c in df.columns})\n","    # coerce common variants\n","    rename = {\n","        'date':'timestamp', 'time':'timestamp', 'datetime':'timestamp',\n","        'asset':'symbol', 'ticker':'symbol', 'coin':'symbol'\n","    }\n","    df = df.rename(columns={k:v for k,v in rename.items() if k in df.columns})\n","    # handle close price variants\n","    for k in ['close','adj_close','close_price','price']:\n","        if k in df.columns:\n","            df = df.rename(columns={k:'close'})\n","            break\n","    # ensure required cols exist if possible\n","    for need in ['timestamp','symbol','open','high','low','close','volume']:\n","        if need not in df.columns:\n","            if need in ['open','high','low','volume']:\n","                df[need] = np.nan  # tolerate missing OHLCV extras\n","            else:\n","                raise ValueError(f\"Missing required column: {need}\")\n","    return df\n","\n","def _parse_timestamp(s):\n","    # robust parser for common formats\n","    try:\n","        return pd.to_datetime(s, utc=True, infer_datetime_format=True)\n","    except Exception:\n","        return pd.NaT\n","\n","def load_any(paths) -> pd.DataFrame:\n","    if not paths:\n","        raise SystemExit(\"Please set CFG.data_paths to your CSV path(s). You can use wildcards like '/content/drive/.../*.csv'\")\n","    files = []\n","    for p in paths:\n","        files.extend(glob.glob(p))\n","    if not files:\n","        raise SystemExit(f\"No files matched: {paths}\")\n","    frames = []\n","    for f in files:\n","        df0 = pd.read_csv(f)\n","        # if wide format, try to melt (e.g., timestamp + columns per asset)\n","        if 'symbol' not in [c.lower() for c in df0.columns]:\n","            # Heuristic: one of the cols is timestamp/date, all others are assets' close prices\n","            # Keep OHLCV if present; otherwise treat values as 'close'\n","            lc = [c.lower() for c in df0.columns]\n","            ts_candidates = [c for c in df0.columns if c.lower() in ['timestamp','date','datetime','time']]\n","            if not ts_candidates:\n","                raise ValueError(f\"Cannot infer timestamp column in wide file: {f}\")\n","            ts_col = ts_candidates[0]\n","            df0 = df0.rename(columns={ts_col:'timestamp'})\n","            id_vars = ['timestamp']\n","            value_vars = [c for c in df0.columns if c not in id_vars]\n","            df0 = df0.melt(id_vars='timestamp', var_name='symbol', value_name='close')\n","        df0 = _standardize_cols(df0)\n","        frames.append(df0)\n","    df = pd.concat(frames, ignore_index=True)\n","    # basic cleaning\n","    df['timestamp'] = df['timestamp'].apply(_parse_timestamp)\n","    df = df.dropna(subset=['timestamp','symbol','close'])\n","    df['symbol'] = df['symbol'].str.upper().str.replace(r'[^A-Z0-9]', '', regex=True)\n","    df = df.sort_values('timestamp')\n","    return df\n","\n","# -----------------------------\n","# 3) Load & filter\n","# -----------------------------\n","raw = load_any(CFG.data_paths)\n","# Keep configured assets if present; otherwise keep top-N by coverage\n","asset_counts = raw['symbol'].value_counts()\n","wanted = [a for a in CFG.assets if a in asset_counts.index]\n","if len(wanted) < len(CFG.assets):\n","    # augment with most covered assets\n","    for a in asset_counts.index:\n","        if a not in wanted and len(wanted) < len(CFG.assets):\n","            wanted.append(a)\n","data = raw[raw['symbol'].isin(wanted)].copy()\n","\n","# resample to desired frequency per asset if high-frequency present\n","def resample_ohlcv(df, freq):\n","    df = df.set_index('timestamp').sort_index()\n","    agg = {\n","        'open':'first','high':'max','low':'min','close':'last','volume':'sum'\n","    }\n","    out = df.groupby('symbol').apply(\n","        lambda g: g.resample(freq).agg(agg).dropna(subset=['close'])\n","    ).reset_index()\n","    return out\n","\n","data = resample_ohlcv(data, CFG.freq)\n","\n","# drop sparse assets\n","keep_assets = []\n","for a, g in data.groupby('symbol'):\n","    if len(g) >= CFG.min_rows_per_asset:\n","        keep_assets.append(a)\n","data = data[data['symbol'].isin(keep_assets)]\n","\n","# -----------------------------\n","# 4) Temporal splits\n","# -----------------------------\n","t_train_end = pd.Timestamp(CFG.t_train_end, tz='UTC')\n","t_val_end   = pd.Timestamp(CFG.t_val_end, tz='UTC')\n","t_test_end  = pd.Timestamp(CFG.t_test_end, tz='UTC')\n","\n","def label_split(ts):\n","    if ts <= t_train_end: return \"train\"\n","    if ts <= t_val_end:   return \"val\"\n","    if ts <= t_test_end:  return \"test\"\n","    return \"holdout_future\"\n","\n","data['split'] = data['timestamp'].apply(label_split)\n","data = data[data['split'] != \"holdout_future\"]  # enforce closed test window\n","\n","# -----------------------------\n","# 5) Summary tables\n","# -----------------------------\n","summary = {\n","    \"date_range\": {\n","        \"min\": str(data['timestamp'].min()),\n","        \"max\": str(data['timestamp'].max()),\n","        \"freq\": CFG.freq\n","    },\n","    \"assets\": sorted(data['symbol'].unique().tolist()),\n","    \"rows_total\": int(len(data)),\n","    \"rows_by_split\": data['split'].value_counts().to_dict(),\n","    \"rows_by_asset\": data.groupby('symbol').size().sort_values(ascending=False).to_dict(),\n","    \"missing_by_col\": {c:int(data[c].isna().sum()) for c in ['open','high','low','close','volume']},\n","}\n","print(\"\\n=== DATASET SUMMARY (for §4.1 paste) ===\")\n","print(json.dumps(summary, indent=2))\n","\n","# Save CSV/JSON for the thesis appendix/repo\n","data.to_csv(f\"{BASE_DIR}/export/tables/dataset_long_{CFG.freq}.csv\", index=False)\n","with open(f\"{BASE_DIR}/export/tables/dataset_summary.json\",\"w\") as f:\n","    json.dump(summary, f, indent=2)\n","\n","print(f\"\\nSaved: {BASE_DIR}/export/tables/dataset_long_{CFG.freq}.csv\")\n","print(f\"Saved: {BASE_DIR}/export/tables/dataset_summary.json\")\n","\n","# Small check: per-split coverage by asset × split\n","pivot = (data\n","         .groupby(['symbol','split'])\n","         .size()\n","         .unstack(fill_value=0)\n","         .reset_index()\n","         .sort_values(by=['test','val','train'], ascending=False))\n","pivot.to_csv(f\"{BASE_DIR}/export/tables/coverage_by_asset_split.csv\", index=False)\n","print(f\"Saved: {BASE_DIR}/export/tables/coverage_by_asset_split.csv\")\n","pivot.head(20)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":56181,"status":"aborted","timestamp":1761543222625,"user":{"displayName":"David Kibaara","userId":"07870126442739887481"},"user_tz":-180},"id":"DEGj2cMrjeXR"},"outputs":[],"source":["# === Recovery Cell: fix loader for ticker/date schema and resume ===\n","import glob, json, numpy as np, pandas as pd\n","from datetime import datetime\n","\n","def _standardize_cols(df: pd.DataFrame) -> pd.DataFrame:\n","    # lower + strip\n","    df = df.rename(columns={c: c.lower().strip() for c in df.columns})\n","    # normalize common variants\n","    rename = {\n","        'date':'timestamp', 'datetime':'timestamp', 'time':'timestamp',\n","        'asset':'symbol', 'ticker':'symbol', 'coin':'symbol'\n","    }\n","    df = df.rename(columns={k:v for k,v in rename.items() if k in df.columns})\n","    # close variants\n","    for k in ['close','adj_close','close_price','price']:\n","        if k in df.columns:\n","            df = df.rename(columns={k:'close'})\n","            break\n","    # ensure required cols (volume may be missing and is okay)\n","    for need in ['timestamp','symbol','open','high','low','close','volume']:\n","        if need not in df.columns:\n","            if need in ['open','high','low','volume']:\n","                df[need] = np.nan\n","            else:\n","                raise ValueError(f\"Missing required column: {need}\")\n","    return df\n","\n","def _parse_timestamp(s):\n","    try:\n","        return pd.to_datetime(s, utc=True, infer_datetime_format=True)\n","    except Exception:\n","        return pd.NaT\n","\n","def load_any(paths) -> pd.DataFrame:\n","    if not paths:\n","        raise SystemExit(\"Please set CFG.data_paths to your CSV path(s).\")\n","    files = []\n","    for p in paths:\n","        files.extend(glob.glob(p))\n","    if not files:\n","        raise SystemExit(f\"No files matched: {paths}\")\n","\n","    frames = []\n","    for f in files:\n","        df0 = pd.read_csv(f)\n","        # Standardize FIRST so 'ticker' -> 'symbol' and 'date' -> 'timestamp'\n","        df0 = _standardize_cols(df0)\n","\n","        # Decide format AFTER standardization\n","        is_long = ('symbol' in df0.columns) and ('timestamp' in df0.columns)\n","        if not is_long:\n","            # Rare case: truly wide file (timestamp + asset columns)\n","            ts_candidates = [c for c in df0.columns if c in ['timestamp','date','datetime','time']]\n","            if not ts_candidates:\n","                raise ValueError(f\"Cannot infer timestamp column in wide file: {f}\")\n","            ts_col = ts_candidates[0]\n","            id_vars = [ts_col]\n","            value_vars = [c for c in df0.columns if c not in id_vars]\n","            df0 = df0.rename(columns={ts_col:'timestamp'})\n","            melted = df0.melt(id_vars='timestamp', var_name='symbol', value_name='close_melt')\n","            if 'close' not in melted.columns:\n","                melted = melted.rename(columns={'close_melt':'close'})\n","            df0 = _standardize_cols(melted)\n","\n","        # Clean and sort\n","        df0['timestamp'] = df0['timestamp'].apply(_parse_timestamp)\n","        df0 = df0.dropna(subset=['timestamp','symbol','close'])\n","        df0['symbol'] = df0['symbol'].str.upper().str.replace(r'[^A-Z0-9]', '', regex=True)\n","        frames.append(df0)\n","\n","    df = pd.concat(frames, ignore_index=True).sort_values('timestamp')\n","    return df\n","\n","# ---- Re-run Steps 3–5 with the patched loader ----\n","raw = load_any(CFG.data_paths)\n","\n","asset_counts = raw['symbol'].value_counts()\n","wanted = [a for a in CFG.assets if a in asset_counts.index]\n","if len(wanted) < len(CFG.assets):\n","    for a in asset_counts.index:\n","        if a not in wanted and len(wanted) < len(CFG.assets):\n","            wanted.append(a)\n","\n","data = raw[raw['symbol'].isin(wanted)].copy()\n","\n","def resample_ohlcv(df, freq):\n","    df = df.set_index('timestamp').sort_index()\n","    agg = {'open':'first','high':'max','low':'min','close':'last','volume':'sum'}\n","    out = df.groupby('symbol').apply(\n","        lambda g: g.resample(freq).agg(agg).dropna(subset=['close'])\n","    ).reset_index()\n","    return out\n","\n","data = resample_ohlcv(data, CFG.freq)\n","\n","keep_assets = []\n","for a, g in data.groupby('symbol'):\n","    if len(g) >= CFG.min_rows_per_asset:\n","        keep_assets.append(a)\n","data = data[data['symbol'].isin(keep_assets)]\n","\n","t_train_end = pd.Timestamp(CFG.t_train_end, tz='UTC')\n","t_val_end   = pd.Timestamp(CFG.t_val_end, tz='UTC')\n","t_test_end  = pd.Timestamp(CFG.t_test_end, tz='UTC')\n","\n","def label_split(ts):\n","    if ts <= t_train_end: return \"train\"\n","    if ts <= t_val_end:   return \"val\"\n","    if ts <= t_test_end:  return \"test\"\n","    return \"holdout_future\"\n","\n","data['split'] = data['timestamp'].apply(label_split)\n","data = data[data['split'] != \"holdout_future\"]\n","\n","summary = {\n","    \"date_range\": {\n","        \"min\": str(data['timestamp'].min()),\n","        \"max\": str(data['timestamp'].max()),\n","        \"freq\": CFG.freq\n","    },\n","    \"assets\": sorted(data['symbol'].unique().tolist()),\n","    \"rows_total\": int(len(data)),\n","    \"rows_by_split\": data['split'].value_counts().to_dict(),\n","    \"rows_by_asset\": data.groupby('symbol').size().sort_values(ascending=False).to_dict(),\n","    \"missing_by_col\": {c:int(data[c].isna().sum()) for c in ['open','high','low','close','volume']},\n","}\n","print(\"\\n=== DATASET SUMMARY (for §4.1 paste) ===\")\n","print(json.dumps(summary, indent=2))\n","\n","data.to_csv(f\"{BASE_DIR}/export/tables/dataset_long_{CFG.freq}.csv\", index=False)\n","with open(f\"{BASE_DIR}/export/tables/dataset_summary.json\",\"w\") as f:\n","    json.dump(summary, f, indent=2)\n","\n","pivot = (data\n","         .groupby(['symbol','split'])\n","         .size()\n","         .unstack(fill_value=0)\n","         .reset_index()\n","         .sort_values(by=['test','val','train'], ascending=False))\n","pivot.to_csv(f\"{BASE_DIR}/export/tables/coverage_by_asset_split.csv\", index=False)\n","\n","print(f\"\\nSaved: {BASE_DIR}/export/tables/dataset_long_{CFG.freq}.csv\")\n","print(f\"Saved: {BASE_DIR}/export/tables/dataset_summary.json\")\n","print(f\"Saved: {BASE_DIR}/export/tables/coverage_by_asset_split.csv\")\n","pivot.head(20)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":56177,"status":"aborted","timestamp":1761543222628,"user":{"displayName":"David Kibaara","userId":"07870126442739887481"},"user_tz":-180},"id":"5J9mcABUwS5q"},"outputs":[],"source":["# === Chapter 4 §4.2 Baselines: Naïve, SMA, LSTM, KAN-inspired (single cell) ===\n","# Colab-ready. Produces metrics, model summaries, parameter counts, and many plots.\n","# Files written to: /content/export/tables/ and /content/export/figures/\n","\n","# (1) Optional installs for model summaries/graphs\n","!pip -q install torchinfo torchviz > /dev/null\n","\n","# (2) Imports\n","import os, json, math, itertools, warnings\n","from dataclasses import dataclass, asdict\n","from typing import Dict, List, Tuple\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torchinfo import summary as torch_summary\n","\n","try:\n","    from torchviz import make_dot\n","    TORCHVIZ_OK = True\n","except Exception:\n","    TORCHVIZ_OK = False\n","\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","\n","# (3) Paths & folders\n","BASE_DIR = \"/content\"\n","TBL_DIR  = f\"{BASE_DIR}/export/tables\"\n","FIG_DIR  = f\"{BASE_DIR}/export/figures\"\n","os.makedirs(TBL_DIR, exist_ok=True)\n","os.makedirs(FIG_DIR, exist_ok=True)\n","\n","# (4) Config\n","@dataclass\n","class ExpConfig:\n","    ds_csv: str = f\"{BASE_DIR}/export/tables/dataset_long_1D.csv\"\n","    assets_max: int = 5                 # limit compute by training on top-N assets by coverage\n","    horizons: List[int] = (1, 3, 7)\n","    win: int = 64                       # input window length\n","    sma_window: int = 5                 # SMA baseline window\n","    batch_size: int = 128\n","    epochs: int = 10\n","    patience: int = 3                   # early stopping patience\n","    lr: float = 1e-3\n","    lstm_hidden: int = 64\n","    lstm_layers: int = 2\n","    lstm_dropout: float = 0.1\n","    spline_K: int = 16                  # number of triangular basis per scalar input\n","    seed: int = 1337\n","    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","CFG = ExpConfig()\n","np.random.seed(CFG.seed)\n","torch.manual_seed(CFG.seed)\n","\n","print(\"Baseline Config\\n\", json.dumps(asdict(CFG), indent=2))\n","\n","# (5) Load dataset produced in §4.1 cell\n","df = pd.read_csv(CFG.ds_csv, parse_dates=[\"timestamp\"])\n","assert {\"timestamp\",\"symbol\",\"close\",\"split\"}.issubset(df.columns), \"Dataset missing required cols.\"\n","\n","# Select top-N assets by coverage to control runtime\n","top_assets = (df.groupby(\"symbol\").size()\n","              .sort_values(ascending=False)\n","              .head(CFG.assets_max).index.tolist())\n","df = df[df[\"symbol\"].isin(top_assets)].copy()\n","\n","# (6) Per-asset standardization (fit on train only)\n","scalers: Dict[str, Tuple[float,float]] = {}\n","for a, g in df[df[\"split\"]==\"train\"].groupby(\"symbol\"):\n","    mu, sd = g[\"close\"].mean(), g[\"close\"].std(ddof=0)\n","    sd = sd if sd > 0 else 1.0\n","    scalers[a] = (mu, sd)\n","\n","def zscore(a, x):\n","    mu, sd = scalers[a]\n","    return (x - mu) / sd\n","\n","def inv_zscore(a, z):\n","    mu, sd = scalers[a]\n","    return z * sd + mu\n","\n","# (7) Build sliding-window samples per (asset, horizon) and split\n","def make_xy(series: pd.Series, horizon: int, win: int) -> Tuple[np.ndarray,np.ndarray]:\n","    \"\"\"\n","    series: close (standardized) as 1D numpy array\n","    Predict future close at t+horizon using window [t-win+1 ... t]\n","    \"\"\"\n","    x_list, y_list = [], []\n","    vals = series.values\n","    for t in range(win-1, len(vals)-horizon):\n","        x = vals[t-win+1:t+1]        # length win\n","        y = vals[t+horizon]          # scalar target\n","        x_list.append(x.astype(np.float32))\n","        y_list.append(np.float32(y))\n","    return np.stack(x_list), np.stack(y_list)\n","\n","class SeqDataset(Dataset):\n","    def __init__(self, X, Y):\n","        self.X = torch.from_numpy(X)[:, :, None]   # (B, W, 1)\n","        self.Y = torch.from_numpy(Y)[:, None]      # (B, 1)\n","    def __len__(self): return self.X.shape[0]\n","    def __getitem__(self, i): return self.X[i], self.Y[i]\n","\n","# prepare dict: data[(asset, split, horizon)] -> (X, Y)\n","data_xy = {}\n","coverage = []\n","for a in top_assets:\n","    g = df[df[\"symbol\"]==a].sort_values(\"timestamp\")\n","    # standardize by train statistics\n","    z = g[\"close\"].copy()\n","    z = z.apply(lambda v: zscore(a, v))\n","    g = g.assign(z=z.values)\n","\n","    for split in [\"train\",\"val\",\"test\"]:\n","        gz = g[g[\"split\"]==split][\"z\"]\n","        for h in CFG.horizons:\n","            if len(gz) >= CFG.win + h + 1:\n","                X, Y = make_xy(gz.reset_index(drop=True), h, CFG.win)\n","                data_xy[(a, split, h)] = (X, Y)\n","                coverage.append((a, split, h, len(Y)))\n","            else:\n","                coverage.append((a, split, h, 0))\n","\n","cov_df = pd.DataFrame(coverage, columns=[\"asset\",\"split\",\"horizon\",\"samples\"])\n","cov_df.to_csv(f\"{TBL_DIR}/coverage_windows.csv\", index=False)\n","print(\"Saved:\", f\"{TBL_DIR}/coverage_windows.csv\")\n","\n","# (8) Baseline predictors (no learning): Naïve & SMA\n","def predict_naive(x_batch):\n","    # last element of the window (standardized domain)\n","    return x_batch[:, -1, 0:1]\n","\n","def predict_sma(x_batch, k=5):\n","    # simple moving average over last k points\n","    xs = x_batch[:, -k:, 0]\n","    m = xs.mean(dim=1, keepdim=True)\n","    return m\n","\n","# (9) Models: LSTM & KAN-inspired spline MLP\n","class LSTMForecast(nn.Module):\n","    def __init__(self, hidden=64, layers=2, dropout=0.1):\n","        super().__init__()\n","        self.lstm = nn.LSTM(input_size=1, hidden_size=hidden,\n","                            num_layers=layers, dropout=dropout, batch_first=True)\n","        self.head = nn.Sequential(\n","            nn.Linear(hidden, hidden),\n","            nn.ReLU(),\n","            nn.Linear(hidden, 1)\n","        )\n","    def forward(self, x):\n","        # x: (B, W, 1)\n","        out, _ = self.lstm(x)\n","        last = out[:, -1, :]        # (B, hidden)\n","        yhat = self.head(last)      # (B, 1)\n","        return yhat\n","\n","class TriangularSpline(nn.Module):\n","    \"\"\"\n","    Triangular (hat) basis over standardized input in [-4,4].\n","    Acts like a simple linear B-spline basis (KAN-inspired).\n","    \"\"\"\n","    def __init__(self, K=16, xmin=-4.0, xmax=4.0):\n","        super().__init__()\n","        self.K = K\n","        centers = torch.linspace(xmin, xmax, K)\n","        self.register_buffer(\"centers\", centers)\n","        self.delta = (xmax - xmin) / (K - 1 + 1e-6)\n","\n","    def forward(self, x):\n","        # x: (B, W, 1)\n","        # return: (B, W, K) triangular activations\n","        # basis_i(x) = relu(1 - |(x - c_i)/delta|)\n","        B, W, _ = x.shape\n","        xexp = x.expand(-1, -1, self.K)     # (B, W, K)\n","        cexp = self.centers.view(1,1,-1).expand(B, W, -1)\n","        act = torch.relu(1.0 - torch.abs((xexp - cexp) / self.delta))\n","        return act\n","\n","class KANForecast(nn.Module):\n","    \"\"\"\n","    Minimal KAN-inspired forecaster:\n","    - Triangular spline basis per timestep value -> (B, W, K)\n","    - Mean pool over time -> (B, K)\n","    - Linear head to 1\n","    \"\"\"\n","    def __init__(self, K=16):\n","        super().__init__()\n","        self.spline = TriangularSpline(K=K)\n","        self.head = nn.Sequential(\n","            nn.Linear(K, K),\n","            nn.ReLU(),\n","            nn.Linear(K, 1)\n","        )\n","    def forward(self, x):\n","        # x: (B, W, 1)\n","        phi = self.spline(x)        # (B, W, K)\n","        pooled = phi.mean(dim=1)    # (B, K) (interpretable: average basis activation)\n","        return self.head(pooled)    # (B, 1)\n","\n","def count_params(m: nn.Module) -> int:\n","    return sum(p.numel() for p in m.parameters() if p.requires_grad)\n","\n","# (10) Training utilities\n","def train_model(model, train_loader, val_loader, epochs, lr, patience, device):\n","    model = model.to(device)\n","    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n","    loss_fn = nn.MSELoss()\n","    best_state, best_val = None, float(\"inf\")\n","    hist = {\"train\": [], \"val\": []}\n","    patience_left = patience\n","\n","    for ep in range(1, epochs+1):\n","        model.train()\n","        tr_loss = 0.0\n","        for xb, yb in train_loader:\n","            xb, yb = xb.to(device), yb.to(device)\n","            opt.zero_grad()\n","            yhat = model(xb)\n","            loss = loss_fn(yhat, yb)\n","            loss.backward()\n","            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","            opt.step()\n","            tr_loss += loss.item() * xb.size(0)\n","        tr_loss /= len(train_loader.dataset)\n","\n","        model.eval()\n","        va_loss = 0.0\n","        with torch.no_grad():\n","            for xb, yb in val_loader:\n","                xb, yb = xb.to(device), yb.to(device)\n","                yhat = model(xb)\n","                loss = loss_fn(yhat, yb)\n","                va_loss += loss.item() * xb.size(0)\n","        va_loss /= len(val_loader.dataset)\n","\n","        hist[\"train\"].append(tr_loss)\n","        hist[\"val\"].append(va_loss)\n","\n","        if va_loss + 1e-9 < best_val:\n","            best_val = va_loss\n","            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n","            patience_left = patience\n","        else:\n","            patience_left -= 1\n","            if patience_left <= 0:\n","                break\n","\n","    if best_state is not None:\n","        model.load_state_dict(best_state)\n","    return model.cpu(), hist\n","\n","# (11) Metrics\n","def mae(y, yhat): return float(np.mean(np.abs(y - yhat)))\n","def rmse(y, yhat): return float(np.sqrt(np.mean((y - yhat)**2)))\n","def smape(y, yhat, eps=1e-8):\n","    denom = (np.abs(y) + np.abs(yhat) + eps) / 2.0\n","    return float(np.mean(np.abs(y - yhat) / denom) * 100.0)\n","def dir_acc(y_prev, y_true, yhat):\n","    # sign of movement from last obs: compare predicted vs actual\n","    # y_prev is last standardized close in window; convert all to raw scale for sign\n","    s_true = np.sign(y_true - y_prev)\n","    s_pred = np.sign(yhat - y_prev)\n","    return float(np.mean((s_true == s_pred).astype(np.float32)))\n","\n","# (12) Fit & evaluate per (asset, horizon)\n","results = []\n","model_summaries = []\n","\n","# Pre-build LSTM/KAN once per horizon per asset (keeps it simple & interpretable)\n","for a in top_assets:\n","    for h in CFG.horizons:\n","        key_tr = (a, \"train\", h)\n","        key_va = (a, \"val\", h)\n","        key_te = (a, \"test\", h)\n","        if key_tr not in data_xy or key_va not in data_xy or key_te not in data_xy:\n","            continue\n","\n","        Xtr, Ytr = data_xy[key_tr]\n","        Xva, Yva = data_xy[key_va]\n","        Xte, Yte = data_xy[key_te]\n","\n","        ds_tr, ds_va, ds_te = SeqDataset(Xtr, Ytr), SeqDataset(Xva, Yva), SeqDataset(Xte, Yte)\n","        dl_tr = DataLoader(ds_tr, batch_size=CFG.batch_size, shuffle=True, drop_last=False)\n","        dl_va = DataLoader(ds_va, batch_size=CFG.batch_size, shuffle=False, drop_last=False)\n","        dl_te = DataLoader(ds_te, batch_size=CFG.batch_size, shuffle=False, drop_last=False)\n","\n","        # ---- Baselines (no train) ----\n","        with torch.no_grad():\n","            xvb = ds_te.X                  # (N, W, 1)\n","            y_prev = xvb[:, -1, 0].numpy() # last obs in window (standardized)\n","            y_true = ds_te.Y[:, 0].numpy()\n","            y_naive = predict_naive(ds_te.X).numpy()[:,0]\n","            y_sma   = predict_sma(ds_te.X, k=CFG.sma_window).numpy()[:,0]\n","\n","        # Convert standardized → raw for metrics if preferred; here we keep standardized for MAE/RMSE\n","        # For directional accuracy, we use standardized but based on movement vs y_prev (consistent).\n","\n","        # Record baselines\n","        res_naive = dict(model=\"Naive\", asset=a, horizon=h,\n","                         MAE=mae(y_true, y_naive),\n","                         RMSE=rmse(y_true, y_naive),\n","                         sMAPE=smape(y_true, y_naive),\n","                         DA=dir_acc(y_prev, y_true, y_naive))\n","        res_sma   = dict(model=\"SMA\", asset=a, horizon=h,\n","                         MAE=mae(y_true, y_sma),\n","                         RMSE=rmse(y_true, y_sma),\n","                         sMAPE=smape(y_true, y_sma),\n","                         DA=dir_acc(y_prev, y_true, y_sma))\n","        results.extend([res_naive, res_sma])\n","\n","        # ---- LSTM ----\n","        lstm = LSTMForecast(hidden=CFG.lstm_hidden, layers=CFG.lstm_layers, dropout=CFG.lstm_dropout)\n","        lstm, hist_lstm = train_model(lstm, dl_tr, dl_va, CFG.epochs, CFG.lr, CFG.patience, CFG.device)\n","        with torch.no_grad():\n","            yhat_lstm = []\n","            for xb, _ in dl_te:\n","                yhat_lstm.append(lstm(xb).numpy())\n","            yhat_lstm = np.concatenate(yhat_lstm, axis=0)[:,0]\n","        res_lstm = dict(model=\"LSTM\", asset=a, horizon=h,\n","                        MAE=mae(y_true, yhat_lstm),\n","                        RMSE=rmse(y_true, yhat_lstm),\n","                        sMAPE=smape(y_true, yhat_lstm),\n","                        DA=dir_acc(y_prev, y_true, yhat_lstm))\n","        results.append(res_lstm)\n","\n","        # ---- KAN-inspired ----\n","        kan = KANForecast(K=CFG.spline_K)\n","        kan, hist_kan = train_model(kan, dl_tr, dl_va, CFG.epochs, CFG.lr, CFG.patience, CFG.device)\n","        with torch.no_grad():\n","            yhat_kan = []\n","            for xb, _ in dl_te:\n","                yhat_kan.append(kan(xb).numpy())\n","            yhat_kan = np.concatenate(yhat_kan, axis=0)[:,0]\n","        res_kan = dict(model=\"KAN-spline\", asset=a, horizon=h,\n","                       MAE=mae(y_true, yhat_kan),\n","                       RMSE=rmse(y_true, yhat_kan),\n","                       sMAPE=smape(y_true, yhat_kan),\n","                       DA=dir_acc(y_prev, y_true, yhat_kan))\n","        results.append(res_kan)\n","\n","        # ---- Save loss curves (per model/horizon/asset)\n","        def plot_loss(hist, title, fname):\n","            plt.figure(figsize=(6,4))\n","            plt.plot(hist[\"train\"], label=\"train\")\n","            plt.plot(hist[\"val\"], label=\"val\")\n","            plt.xlabel(\"Epoch\"); plt.ylabel(\"MSE loss\"); plt.title(title); plt.legend()\n","            plt.tight_layout()\n","            pth = os.path.join(FIG_DIR, fname)\n","            plt.savefig(pth, dpi=150); plt.close()\n","            return pth\n","\n","        p1 = plot_loss(hist_lstm, f\"LSTM Loss — {a}, H={h}\", f\"loss_lstm_{a}_H{h}.png\")\n","        p2 = plot_loss(hist_kan,  f\"KAN-spline Loss — {a}, H={h}\", f\"loss_kan_{a}_H{h}.png\")\n","\n","        # ---- Save prediction trace for test (first N=400 points)\n","        Nplot = min(400, len(y_true))\n","        def plot_trace(y_true, yhat, title, fname):\n","            plt.figure(figsize=(8,4))\n","            plt.plot(y_true[:Nplot], label=\"true\")\n","            plt.plot(yhat[:Nplot],  label=\"pred\")\n","            plt.xlabel(\"Test sample index\"); plt.ylabel(\"Standardized close\")\n","            plt.title(title); plt.legend(); plt.tight_layout()\n","            pth = os.path.join(FIG_DIR, fname)\n","            plt.savefig(pth, dpi=150); plt.close()\n","            return pth\n","\n","        t_lstm = plot_trace(y_true, yhat_lstm, f\"LSTM Test Trace — {a}, H={h}\", f\"trace_lstm_{a}_H{h}.png\")\n","        t_kan  = plot_trace(y_true, yhat_kan,  f\"KAN-spline Test Trace — {a}, H={h}\", f\"trace_kan_{a}_H{h}.png\")\n","\n","        # ---- Residual histograms\n","        def plot_residuals(y_true, yhat, title, fname):\n","            plt.figure(figsize=(6,4))\n","            resid = y_true - yhat\n","            plt.hist(resid, bins=40)\n","            plt.xlabel(\"Residual\"); plt.ylabel(\"Count\"); plt.title(title); plt.tight_layout()\n","            pth = os.path.join(FIG_DIR, fname)\n","            plt.savefig(pth, dpi=150); plt.close()\n","            return pth\n","\n","        r_lstm = plot_residuals(y_true, yhat_lstm, f\"Residuals LSTM — {a}, H={h}\", f\"resid_lstm_{a}_H{h}.png\")\n","        r_kan  = plot_residuals(y_true, yhat_kan,  f\"Residuals KAN — {a}, H={h}\", f\"resid_kan_{a}_H{h}.png\")\n","\n","        # ---- Model summaries & parameter counts (save once per (asset,h))\n","        # LSTM\n","        lstm_params = count_params(lstm)\n","        ms_lstm = torch_summary(lstm, input_size=(1, CFG.win, 1), verbose=0)\n","        with open(os.path.join(TBL_DIR, f\"model_summary_lstm_{a}_H{h}.txt\"), \"w\") as f:\n","            f.write(str(ms_lstm))\n","            f.write(f\"\\nTotal trainable parameters: {lstm_params}\\n\")\n","        # KAN\n","        kan_params = count_params(kan)\n","        ms_kan = torch_summary(kan, input_size=(1, CFG.win, 1), verbose=0)\n","        with open(os.path.join(TBL_DIR, f\"model_summary_kan_{a}_H{h}.txt\"), \"w\") as f:\n","            f.write(str(ms_kan))\n","            f.write(f\"\\nTotal trainable parameters: {kan_params}\\n\")\n","\n","        model_summaries.append({\"asset\":a,\"horizon\":h,\"model\":\"LSTM\",\"params\":lstm_params})\n","        model_summaries.append({\"asset\":a,\"horizon\":h,\"model\":\"KAN-spline\",\"params\":kan_params})\n","\n","        # ---- Architecture graphs (best-effort)\n","        if TORCHVIZ_OK:\n","            try:\n","                xdummy = torch.randn(1, CFG.win, 1)\n","                dot_l = make_dot(lstm(xdummy), params=dict(list(lstm.named_parameters())))\n","                dot_l.render(os.path.join(FIG_DIR, f\"graph_lstm_{a}_H{h}\"), format=\"png\", cleanup=True)\n","                dot_k = make_dot(kan(xdummy), params=dict(list(kan.named_parameters())))\n","                dot_k.render(os.path.join(FIG_DIR, f\"graph_kan_{a}_H{h}\"), format=\"png\", cleanup=True)\n","            except Exception as e:\n","                print(\"torchviz failed for\", a, h, \":\", e)\n","\n","        # ---- Visualize KAN triangular bases (once per run is enough, but we save per (a,h) for convenience)\n","        xx = np.linspace(-4, 4, 400, dtype=np.float32)\n","        with torch.no_grad():\n","            x_t = torch.from_numpy(xx).view(1, -1, 1)\n","            phi = kan.spline(x_t).squeeze(0).numpy()  # (W=400, K)\n","        plt.figure(figsize=(7,4))\n","        for k in range(phi.shape[1]):\n","            plt.plot(xx, phi[:,k])\n","        plt.title(f\"KAN Triangular Bases (K={CFG.spline_K}) — {a}, H={h}\")\n","        plt.xlabel(\"Standardized input\"); plt.ylabel(\"Activation\")\n","        plt.tight_layout()\n","        plt.savefig(os.path.join(FIG_DIR, f\"kan_bases_{a}_H{h}.png\"), dpi=150)\n","        plt.close()\n","\n","# (13) Collate & save results\n","res_df = pd.DataFrame(results).sort_values([\"asset\",\"horizon\",\"model\"])\n","res_csv = os.path.join(TBL_DIR, \"baseline_results_per_asset_horizon.csv\")\n","res_df.to_csv(res_csv, index=False)\n","\n","# Aggregate by horizon across assets (mean metrics)\n","agg_df = (res_df.groupby([\"model\",\"horizon\"])\n","          .agg(MAE=(\"MAE\",\"mean\"), RMSE=(\"RMSE\",\"mean\"), sMAPE=(\"sMAPE\",\"mean\"), DA=(\"DA\",\"mean\"))\n","          .reset_index()\n","          .sort_values([\"horizon\",\"model\"]))\n","agg_csv = os.path.join(TBL_DIR, \"baseline_results_agg_by_horizon.csv\")\n","agg_df.to_csv(agg_csv, index=False)\n","\n","# Param counts table\n","param_df = pd.DataFrame(model_summaries)\n","param_csv = os.path.join(TBL_DIR, \"model_param_counts.csv\")\n","param_df.to_csv(param_csv, index=False)\n","\n","# (14) Plots: parameter counts (bar), DA by model/horizon, metrics bars\n","def barplot_param_counts(df, fname):\n","    piv = df.pivot_table(index=[\"model\"], values=\"params\", aggfunc=\"mean\").reset_index()\n","    plt.figure(figsize=(6,4))\n","    plt.bar(piv[\"model\"], piv[\"params\"])\n","    plt.ylabel(\"Parameters (mean across assets & horizons)\")\n","    plt.title(\"Model Parameter Counts\")\n","    plt.tight_layout()\n","    pth = os.path.join(FIG_DIR, fname); plt.savefig(pth, dpi=150); plt.close(); return pth\n","\n","def barplot_metric(df, metric, fname, title):\n","    plt.figure(figsize=(7,4))\n","    # group by horizon for grouped bars\n","    labels = sorted(df[\"horizon\"].unique())\n","    models = df[\"model\"].unique().tolist()\n","    width = 0.15\n","    idx = np.arange(len(labels))\n","    for i, m in enumerate(models):\n","        sub = df[df[\"model\"]==m].set_index(\"horizon\").reindex(labels)\n","        plt.bar(idx + i*width, sub[metric].values, width=width, label=m)\n","    plt.xticks(idx + width*(len(models)-1)/2, [f\"H={h}\" for h in labels])\n","    plt.ylabel(metric); plt.title(title); plt.legend()\n","    plt.tight_layout()\n","    pth = os.path.join(FIG_DIR, fname); plt.savefig(pth, dpi=150); plt.close(); return pth\n","\n","p_par = barplot_param_counts(param_df, \"param_counts_bar.png\")\n","p_da  = barplot_metric(agg_df, \"DA\",    \"directional_accuracy_bars.png\", \"Directional Accuracy by Model & Horizon\")\n","p_mae = barplot_metric(agg_df, \"MAE\",   \"mae_bars.png\", \"MAE by Model & Horizon\")\n","p_rmse= barplot_metric(agg_df, \"RMSE\",  \"rmse_bars.png\",\"RMSE by Model & Horizon\")\n","p_sm  = barplot_metric(agg_df, \"sMAPE\", \"smape_bars.png\",\"sMAPE by Model & Horizon\")\n","\n","# (15) Print short summary for pasting\n","print(\"\\n=== RESULTS SUMMARY (paste into chat) ===\")\n","print(\"Per-asset × horizon results CSV:\", res_csv)\n","print(res_df.head(12).to_string(index=False))\n","print(\"\\nAggregated by horizon (means across assets):\", agg_csv)\n","print(agg_df.to_string(index=False))\n","\n","# (16) List of key figures produced\n","figs = sorted([os.path.join(FIG_DIR, f) for f in os.listdir(FIG_DIR) if f.endswith(\".png\")])\n","print(\"\\n=== FIGURES SAVED ===\")\n","for p in figs[:30]:\n","    print(p)\n","if len(figs) > 30:\n","    print(f\"... and {len(figs)-30} more figures\")\n","\n","print(\"\\nArtifacts:\")\n","print(\" - Model summaries (*.txt) in\", TBL_DIR)\n","print(\" - Parameter counts:\", param_csv)\n","print(\" - Coverage windows:\", f\"{TBL_DIR}/coverage_windows.csv\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":56173,"status":"aborted","timestamp":1761543222629,"user":{"displayName":"David Kibaara","userId":"07870126442739887481"},"user_tz":-180},"id":"L4uRf-_gAK3M"},"outputs":[],"source":["# === Chapter 4 §4.3: Bandit-Teacher + DRL Student (Automatic Curriculum) ===\n","# Colab single-cell scaffold. Schedules (asset, horizon) tasks via UCB1 using learning progress as reward.\n","# Student is a lightweight REINFORCE forecaster (Gaussian policy) over standardized close with an LSTM encoder.\n","# Outputs: logs, regret curves, pull distributions, model graphs, test metrics, and many plots for the thesis.\n","\n","# Optional diagramming\n","!pip -q install torchinfo torchviz > /dev/null\n","\n","import os, json, math, random, warnings, glob\n","from dataclasses import dataclass, asdict\n","from typing import Dict, Tuple, List\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torchinfo import summary as torch_summary\n","try:\n","    from torchviz import make_dot\n","    TORCHVIZ_OK = True\n","except Exception:\n","    TORCHVIZ_OK = False\n","\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","\n","# -----------------------------\n","# Paths & folders\n","# -----------------------------\n","BASE_DIR = \"/content\"\n","TBL_DIR  = f\"{BASE_DIR}/export/tables\"\n","FIG_DIR  = f\"{BASE_DIR}/export/figures\"\n","LOG_DIR  = f\"{BASE_DIR}/export/logs\"\n","for d in [TBL_DIR, FIG_DIR, LOG_DIR]:\n","    os.makedirs(d, exist_ok=True)\n","\n","# -----------------------------\n","# Experiment Config\n","# -----------------------------\n","@dataclass\n","class DRLConfig:\n","    ds_csv: str = f\"{BASE_DIR}/export/tables/dataset_long_1D.csv\"\n","    assets_max: int = 5               # top-N assets by coverage\n","    horizons: Tuple[int,...] = (1,3,7)\n","    win: int = 64\n","    batch_size: int = 256\n","    teacher_rounds: int = 200         # number of bandit scheduling rounds\n","    train_steps_per_pull: int = 1     # policy updates per scheduled task\n","    lr: float = 2e-3\n","    gamma: float = 0.99               # (unused for one-step reward, kept for extensibility)\n","    entropy_beta: float = 1e-3        # entropy bonus weight\n","    aux_sup_weight: float = 1e-1      # supervised MSE auxiliary weight on policy mean\n","    ucb_c: float = 1.2                # UCB1 exploration constant\n","    seed: int = 1337\n","    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    # model\n","    lstm_hidden: int = 64\n","    lstm_layers: int = 1\n","    lstm_dropout: float = 0.0\n","    # evaluation\n","    eval_n_trace: int = 400           # points for test traces\n","DRLCFG = DRLConfig()\n","random.seed(DRLCFG.seed); np.random.seed(DRLCFG.seed); torch.manual_seed(DRLCFG.seed)\n","\n","print(\"§4.3 Config\\n\", json.dumps(asdict(DRLCFG), indent=2))\n","\n","# -----------------------------\n","# Load dataset produced in §4.1\n","# -----------------------------\n","df = pd.read_csv(DRLCFG.ds_csv, parse_dates=[\"timestamp\"])\n","assert {\"timestamp\",\"symbol\",\"close\",\"split\"}.issubset(df.columns), \"Dataset missing required columns.\"\n","\n","# Select top-N assets by coverage\n","top_assets = (df.groupby(\"symbol\").size()\n","              .sort_values(ascending=False)\n","              .head(DRLCFG.assets_max).index.tolist())\n","df = df[df[\"symbol\"].isin(top_assets)].copy()\n","\n","# Standardize per asset using TRAIN split only\n","scalers: Dict[str, Tuple[float,float]] = {}\n","for a, g in df[df[\"split\"]==\"train\"].groupby(\"symbol\"):\n","    mu, sd = g[\"close\"].mean(), g[\"close\"].std(ddof=0)\n","    scalers[a] = (float(mu), float(sd if sd>0 else 1.0))\n","\n","def zscore(a, x):\n","    mu, sd = scalers[a]; return (x - mu) / sd\n","def inv_z(a, z):\n","    mu, sd = scalers[a]; return z*sd + mu\n","\n","# Build standardized z\n","df = df.sort_values([\"symbol\",\"timestamp\"])\n","df[\"z\"] = df.apply(lambda r: zscore(r[\"symbol\"], r[\"close\"]), axis=1)\n","\n","# -----------------------------\n","# Sliding-window datasets per (asset, split, horizon)\n","# -----------------------------\n","def make_xy(series: pd.Series, horizon: int, win: int):\n","    x_list, y_list, last_list = [], [], []\n","    vals = series.values\n","    for t in range(win-1, len(vals)-horizon):\n","        x = vals[t-win+1:t+1]               # window\n","        y = vals[t+horizon]                 # target\n","        last = vals[t]                      # last obs in window (for DA)\n","        x_list.append(x.astype(np.float32))\n","        y_list.append(np.float32(y))\n","        last_list.append(np.float32(last))\n","    return np.stack(x_list), np.stack(y_list), np.stack(last_list)\n","\n","data_xy = {}\n","coverage = []\n","for a in top_assets:\n","    g = df[df[\"symbol\"]==a].copy()\n","    for split in [\"train\",\"val\",\"test\"]:\n","        gz = g[g[\"split\"]==split][\"z\"].reset_index(drop=True)\n","        for h in DRLCFG.horizons:\n","            if len(gz) >= DRLCFG.win + h + 1:\n","                X, Y, LAST = make_xy(gz, h, DRLCFG.win)\n","                data_xy[(a, split, h)] = (X, Y, LAST)\n","                coverage.append((a, split, h, len(Y)))\n","            else:\n","                coverage.append((a, split, h, 0))\n","cov_df = pd.DataFrame(coverage, columns=[\"asset\",\"split\",\"horizon\",\"samples\"])\n","cov_df.to_csv(f\"{TBL_DIR}/acg_coverage_windows.csv\", index=False)\n","\n","# Define ARMS = all (asset,horizon) with sufficient train & val & test\n","arms: List[Tuple[str,int]] = []\n","for a in top_assets:\n","    for h in DRLCFG.horizons:\n","        ok = all(((a, sp, h) in data_xy and data_xy[(a,sp,h)][1].shape[0] > 0) for sp in [\"train\",\"val\",\"test\"])\n","        if ok: arms.append((a,h))\n","assert len(arms) > 0, \"No valid (asset,horizon) arms found.\"\n","n_arms = len(arms)\n","print(f\"Arms (asset,horizon): {arms}\")\n","\n","# -----------------------------\n","# Student: REINFORCE forecaster (Gaussian policy)\n","# -----------------------------\n","class PolicyLSTM(nn.Module):\n","    def __init__(self, hidden=64, layers=1, dropout=0.0):\n","        super().__init__()\n","        self.lstm = nn.LSTM(input_size=1, hidden_size=hidden, num_layers=layers,\n","                            dropout=(dropout if layers>1 else 0.0), batch_first=True)\n","        self.mu = nn.Sequential(nn.Linear(hidden, hidden), nn.ReLU(), nn.Linear(hidden, 1))\n","        self.log_sigma = nn.Parameter(torch.tensor(-0.5))  # global log-std\n","    def forward(self, x):\n","        # x: (B, W, 1)\n","        o, _ = self.lstm(x)\n","        last = o[:, -1, :]\n","        mu = self.mu(last)            # (B,1)\n","        log_sigma = self.log_sigma.expand_as(mu)\n","        return mu, log_sigma\n","\n","def count_params(m):\n","    return sum(p.numel() for p in m.parameters() if p.requires_grad)\n","\n","device = torch.device(DRLCFG.device)\n","policy = PolicyLSTM(hidden=DRLCFG.lstm_hidden, layers=DRLCFG.lstm_layers, dropout=DRLCFG.lstm_dropout).to(device)\n","opt = torch.optim.AdamW(policy.parameters(), lr=DRLCFG.lr)\n","\n","# -----------------------------\n","# Teacher: UCB1 using Learning Progress (Δ val MSE) as reward\n","# -----------------------------\n","class UCB1Teacher:\n","    def __init__(self, n_arms, c=1.2):\n","        self.c = c\n","        self.n = np.zeros(n_arms, dtype=np.int64)\n","        self.mean = np.zeros(n_arms, dtype=np.float64)  # average reward per arm\n","        self.t = 0\n","    def select(self):\n","        self.t += 1\n","        # Pull each arm at least once\n","        for i in range(n_arms):\n","            if self.n[i] == 0:\n","                return i\n","        ucb = self.mean + self.c * np.sqrt(2.0 * math.log(self.t) / self.n)\n","        return int(np.argmax(ucb))\n","    def update(self, i, reward):\n","        self.n[i] += 1\n","        # incremental mean\n","        self.mean[i] += (reward - self.mean[i]) / self.n[i]\n","\n","teacher = UCB1Teacher(n_arms, DRLCFG.ucb_c)\n","\n","# Track last validation MSE per arm to compute learning progress\n","def eval_val_mse(policy: nn.Module, arm_idx: int) -> float:\n","    a, h = arms[arm_idx]\n","    X, Y, _ = data_xy[(a,\"val\",h)]\n","    with torch.no_grad():\n","        xb = torch.from_numpy(X).to(device)[:, :, None]\n","        mu, log_sigma = policy(xb)\n","        yhat = mu.squeeze(1).cpu().numpy()\n","    return float(np.mean((Y - yhat)**2))\n","\n","last_val_mse = np.array([eval_val_mse(policy, i) for i in range(n_arms)], dtype=np.float64)\n","\n","# -----------------------------\n","# Batching utility per arm\n","# -----------------------------\n","def sample_train_minibatch(arm_idx: int, batch_size: int):\n","    a, h = arms[arm_idx]\n","    X, Y, LAST = data_xy[(a,\"train\",h)]\n","    idx = np.random.randint(0, len(Y), size=(min(batch_size, len(Y)),))\n","    xb = torch.from_numpy(X[idx]).float().to(device)[:, :, None]  # (B,W,1)\n","    yb = torch.from_numpy(Y[idx]).float().to(device)[:, None]     # (B,1)\n","    lastb = torch.from_numpy(LAST[idx]).float().to(device)[:, None]\n","    return xb, yb, lastb\n","\n","# -----------------------------\n","# Metrics\n","# -----------------------------\n","def mae(y, yhat): return float(np.mean(np.abs(y - yhat)))\n","def rmse(y, yhat): return float(np.sqrt(np.mean((y - yhat)**2)))\n","def smape(y, yhat, eps=1e-8):\n","    denom = (np.abs(y) + np.abs(yhat) + eps) / 2.0\n","    return float(np.mean(np.abs(y - yhat) / denom) * 100.0)\n","def dir_acc(y_prev, y_true, yhat):\n","    s_true = np.sign(y_true - y_prev)\n","    s_pred = np.sign(yhat - y_prev)\n","    return float(np.mean((s_true == s_pred).astype(np.float32)))\n","\n","# -----------------------------\n","# Training loop with bandit scheduling\n","# -----------------------------\n","log = {\n","    \"round\": [], \"arm\": [], \"asset\": [], \"horizon\": [],\n","    \"reward_lp\": [], \"val_mse_before\": [], \"val_mse_after\": [],\n","    \"mean_reward_est\": [], \"pulls_arm\": []\n","}\n","baseline_reward = 0.0  # simple moving baseline for REINFORCE\n","baseline_momentum = 0.9\n","\n","def reinforce_step(xb, yb):\n","    policy.train()\n","    opt.zero_grad()\n","    mu, log_sigma = policy(xb)             # (B,1)\n","    sigma = torch.exp(log_sigma)\n","    dist = torch.distributions.Normal(mu, sigma)\n","    a = dist.rsample()                     # (B,1)\n","    # Reward: negative squared error in standardized space\n","    r = - (a - yb)**2                      # (B,1)\n","    # Baseline (scalar) to reduce variance\n","    global baseline_reward\n","    avg_r = r.mean().detach()\n","    advantage = r - baseline_reward\n","    baseline_reward = baseline_momentum*baseline_reward + (1.0-baseline_momentum)*avg_r\n","\n","    # Policy loss (maximize expected reward)\n","    logp = dist.log_prob(a)                # (B,1)\n","    loss_policy = - (logp * advantage.detach()).mean()\n","    # Entropy bonus\n","    loss_entropy = - DRLCFG.entropy_beta * dist.entropy().mean()\n","    # Small supervised auxiliary on mu to stabilize\n","    loss_aux = DRLCFG.aux_sup_weight * nn.MSELoss()(mu, yb)\n","    loss = loss_policy + loss_entropy + loss_aux\n","    loss.backward()\n","    nn.utils.clip_grad_norm_(policy.parameters(), 1.0)\n","    opt.step()\n","\n","# Warm start: one pass over all arms (ensures last_val_mse initialized sensibly)\n","for i in range(n_arms):\n","    xb, yb, _ = sample_train_minibatch(i, DRLCFG.batch_size)\n","    reinforce_step(xb, yb)\n","\n","# Bandit rounds\n","for t in range(1, DRLCFG.teacher_rounds + 1):\n","    i = teacher.select()\n","    a, h = arms[i]\n","\n","    # Evaluate before\n","    val_before = eval_val_mse(policy, i)\n","\n","    # Train on selected arm\n","    for _ in range(DRLCFG.train_steps_per_pull):\n","        xb, yb, _ = sample_train_minibatch(i, DRLCFG.batch_size)\n","        reinforce_step(xb, yb)\n","\n","    # Evaluate after\n","    val_after = eval_val_mse(policy, i)\n","\n","    # Learning progress reward (positive if MSE decreased)\n","    reward_lp = float(val_before - val_after)\n","    teacher.update(i, reward_lp)\n","\n","    last_val_mse[i] = val_after\n","\n","    log[\"round\"].append(t)\n","    log[\"arm\"].append(i)\n","    log[\"asset\"].append(a)\n","    log[\"horizon\"].append(h)\n","    log[\"reward_lp\"].append(reward_lp)\n","    log[\"val_mse_before\"].append(val_before)\n","    log[\"val_mse_after\"].append(val_after)\n","    log[\"mean_reward_est\"].append(float(teacher.mean[i]))\n","    log[\"pulls_arm\"].append(int(teacher.n[i]))\n","\n","# Save teacher log\n","log_df = pd.DataFrame(log)\n","log_csv = f\"{TBL_DIR}/acg_teacher_log.csv\"\n","log_df.to_csv(log_csv, index=False)\n","\n","# -----------------------------\n","# Regret & selection summaries\n","# -----------------------------\n","# Per-round instantaneous regret: best current mean - chosen reward\n","means_per_arm = np.zeros(n_arms)\n","counts_per_arm = np.zeros(n_arms, dtype=np.int64)\n","inst_regret = []\n","for r in range(len(log_df)):\n","    # simulate the means tracked by log, but we can approximate regret with:\n","    rew = log_df.iloc[r][\"reward_lp\"]\n","    chosen = log_df.iloc[r][\"arm\"]\n","    # best-so-far average reward among arms (approx using teacher.mean up to round r)\n","    # we'll reconstruct from log by taking latest mean per arm up to current round\n","    means_snapshot = {int(ai): float(am) for ai, am in zip(log_df[\"arm\"][:r+1], log_df[\"mean_reward_est\"][:r+1])}\n","    best_mean = max(means_snapshot.values()) if means_snapshot else 0.0\n","    inst_regret.append(best_mean - rew)\n","regret = np.cumsum(inst_regret)\n","\n","sel_counts = log_df[\"arm\"].value_counts().sort_index()\n","sel_df = pd.DataFrame({\n","    \"arm_idx\": list(range(n_arms)),\n","    \"asset\": [arms[i][0] for i in range(n_arms)],\n","    \"horizon\": [arms[i][1] for i in range(n_arms)],\n","    \"pulls\": [int(sel_counts.get(i, 0)) for i in range(n_arms)],\n","    \"mean_reward\": [float(teacher.mean[i]) for i in range(n_arms)]\n","})\n","sel_df.to_csv(f\"{TBL_DIR}/acg_selection_counts.csv\", index=False)\n","\n","# -----------------------------\n","# Plots: arm pulls, reward, regret, mean reward by arm\n","# -----------------------------\n","def savefig(fname):\n","    pth = os.path.join(FIG_DIR, fname)\n","    plt.tight_layout(); plt.savefig(pth, dpi=150); plt.close(); return pth\n","\n","# Arm pulls over time\n","plt.figure(figsize=(8,3))\n","plt.plot(log_df[\"arm\"].values, lw=1)\n","plt.yticks(range(n_arms), [f\"{arms[i][0]}-H{arms[i][1]}\" for i in range(n_arms)])\n","plt.xlabel(\"Round\"); plt.ylabel(\"Selected arm\"); plt.title(\"Bandit arm selection over time (UCB1)\")\n","f_sel_over_time = savefig(\"acg_arm_selection_over_time.png\")\n","\n","# Selection frequencies\n","plt.figure(figsize=(6,3))\n","labels = [f\"{a}-H{h}\" for a,h in arms]\n","plt.bar(range(n_arms), sel_df[\"pulls\"].values)\n","plt.xticks(range(n_arms), labels, rotation=45, ha=\"right\")\n","plt.ylabel(\"Pulls\"); plt.title(\"Selection counts by arm\")\n","f_sel_counts = savefig(\"acg_selection_counts.png\")\n","\n","# Reward trajectory\n","plt.figure(figsize=(7,3))\n","plt.plot(pd.Series(log_df[\"reward_lp\"]).rolling(10).mean(), label=\"Rolling-10 mean reward\")\n","plt.axhline(0.0, color=\"black\", lw=1)\n","plt.legend(); plt.title(\"Learning progress (Δ val MSE) — rolling mean\")\n","plt.xlabel(\"Round\"); plt.ylabel(\"Reward (↓MSE)\");\n","f_reward_curve = savefig(\"acg_reward_curve.png\")\n","\n","# Cumulative regret\n","plt.figure(figsize=(7,3))\n","plt.plot(regret)\n","plt.title(\"Cumulative regret (approx.)\")\n","plt.xlabel(\"Round\"); plt.ylabel(\"Regret\")\n","f_regret = savefig(\"acg_cumulative_regret.png\")\n","\n","# Mean reward by arm\n","plt.figure(figsize=(6,3))\n","plt.bar(range(n_arms), sel_df[\"mean_reward\"].values)\n","plt.xticks(range(n_arms), labels, rotation=45, ha=\"right\")\n","plt.ylabel(\"Mean reward\"); plt.title(\"Per-arm mean learning progress\")\n","f_mean_reward = savefig(\"acg_mean_reward_by_arm.png\")\n","\n","# -----------------------------\n","# Validation MSE per arm before/after (last snapshot)\n","# -----------------------------\n","val_after_latest = []\n","for i in range(n_arms):\n","    val_mse = eval_val_mse(policy, i)\n","    val_after_latest.append(val_mse)\n","val_snap_df = pd.DataFrame({\n","    \"arm_idx\": list(range(n_arms)),\n","    \"asset\": [arms[i][0] for i in range(n_arms)],\n","    \"horizon\": [arms[i][1] for i in range(n_arms)],\n","    \"val_mse_final\": val_after_latest,\n","    \"pulls\": sel_df[\"pulls\"].values\n","})\n","val_snap_df.to_csv(f\"{TBL_DIR}/acg_val_mse_final.csv\", index=False)\n","\n","plt.figure(figsize=(6,3))\n","plt.bar(range(n_arms), val_snap_df[\"val_mse_final\"].values)\n","plt.xticks(range(n_arms), labels, rotation=45, ha=\"right\")\n","plt.ylabel(\"Val MSE\"); plt.title(\"Final validation MSE by arm\")\n","f_val_mse_final = savefig(\"acg_val_mse_final_bars.png\")\n","\n","# -----------------------------\n","# Model reports (summary, params, graph)\n","# -----------------------------\n","model_params = count_params(policy)\n","ms = torch_summary(policy, input_size=(1, DRLCFG.win, 1), verbose=0)\n","with open(os.path.join(TBL_DIR, \"acg_student_model_summary.txt\"), \"w\") as f:\n","    f.write(str(ms))\n","    f.write(f\"\\nTotal trainable parameters: {model_params}\\n\")\n","\n","if TORCHVIZ_OK:\n","    try:\n","        xdummy = torch.randn(1, DRLCFG.win, 1).to(device)\n","        mu, log_sigma = policy(xdummy)\n","        dot = make_dot(mu, params=dict(list(policy.named_parameters())))\n","        dot.render(os.path.join(FIG_DIR, \"acg_student_graph\"), format=\"png\", cleanup=True)\n","    except Exception as e:\n","        print(\"torchviz failed:\", e)\n","\n","# -----------------------------\n","# Test evaluation per arm (deterministic: use mu)\n","# -----------------------------\n","results = []\n","trace_figs = []\n","for i in range(n_arms):\n","    a, h = arms[i]\n","    Xte, Yte, LASTte = data_xy[(a,\"test\",h)]\n","    with torch.no_grad():\n","        xb = torch.from_numpy(Xte).float().to(device)[:, :, None]\n","        mu, _ = policy(xb)\n","        yhat = mu.squeeze(1).cpu().numpy()\n","    y_true = Yte\n","    y_prev = LASTte\n","\n","    res = dict(model=\"ACG-DRL\", asset=a, horizon=h,\n","               MAE=mae(y_true, yhat), RMSE=rmse(y_true, yhat),\n","               sMAPE=smape(y_true, yhat), DA=dir_acc(y_prev, y_true, yhat))\n","    results.append(res)\n","\n","    # Traces & residuals\n","    Nplot = min(DRLCFG.eval_n_trace, len(y_true))\n","    plt.figure(figsize=(8,3.5))\n","    plt.plot(y_true[:Nplot], label=\"true\")\n","    plt.plot(yhat[:Nplot], label=\"pred\")\n","    plt.title(f\"ACG-DRL Test Trace — {a}, H={h} (first {Nplot})\")\n","    plt.xlabel(\"Test index\"); plt.ylabel(\"Standardized close\"); plt.legend()\n","    trace_figs.append(savefig(f\"acg_trace_{a}_H{h}.png\"))\n","\n","    plt.figure(figsize=(6,3))\n","    resid = y_true - yhat\n","    plt.hist(resid, bins=40)\n","    plt.title(f\"ACG-DRL Residuals — {a}, H={h}\")\n","    plt.xlabel(\"Residual\"); plt.ylabel(\"Count\")\n","    savefig(f\"acg_resid_{a}_H{h}.png\")\n","\n","res_df = pd.DataFrame(results).sort_values([\"asset\",\"horizon\"])\n","res_csv = f\"{TBL_DIR}/acg_results_per_arm.csv\"\n","res_df.to_csv(res_csv, index=False)\n","\n","# Aggregate by horizon\n","agg_df = (res_df.groupby(\"horizon\")\n","          .agg(MAE=(\"MAE\",\"mean\"), RMSE=(\"RMSE\",\"mean\"), sMAPE=(\"sMAPE\",\"mean\"), DA=(\"DA\",\"mean\"))\n","          .reset_index().sort_values(\"horizon\"))\n","agg_csv = f\"{TBL_DIR}/acg_results_agg_by_horizon.csv\"\n","agg_df.to_csv(agg_csv, index=False)\n","\n","# -----------------------------\n","# Final summary printout\n","# -----------------------------\n","print(\"\\n=== §4.3 RESULTS SUMMARY (paste into chat) ===\")\n","print(\"Arms (asset,horizon):\", arms)\n","print(\"Selection counts & mean rewards (CSV):\", f\"{TBL_DIR}/acg_selection_counts.csv\")\n","print(sel_df.to_string(index=False))\n","print(\"\\nTeacher rounds:\", DRLCFG.teacher_rounds, \"| UCB1 c =\", DRLCFG.ucb_c)\n","print(\"Teacher log CSV:\", log_csv)\n","print(\"Cumulative regret plotted in:\", f_regret)\n","\n","print(\"\\nACG-DRL per-arm TEST results (CSV):\", res_csv)\n","print(res_df.head(len(res_df)).to_string(index=False))\n","\n","print(\"\\nAggregated by horizon (means across arms):\", agg_csv)\n","print(agg_df.to_string(index=False))\n","\n","# List key figures\n","figs = sorted([os.path.join(FIG_DIR, f) for f in os.listdir(FIG_DIR) if f.endswith(\".png\")])\n","print(\"\\n=== FIGURES SAVED (first 40) ===\")\n","for p in figs[:40]:\n","    print(p)\n","if len(figs) > 40:\n","    print(f\"... and {len(figs)-40} more figures\")\n","\n","print(\"\\nArtifacts:\")\n","print(\" - Student model summary:\", os.path.join(TBL_DIR, \"acg_student_model_summary.txt\"))\n","print(\" - Coverage windows:\", f\"{TBL_DIR}/acg_coverage_windows.csv\")\n","print(\" - Final val MSE by arm:\", f\"{TBL_DIR}/acg_val_mse_final.csv\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":56169,"status":"aborted","timestamp":1761543222630,"user":{"displayName":"David Kibaara","userId":"07870126442739887481"},"user_tz":-180},"id":"wRlQ9h4bPGtB"},"outputs":[],"source":["# === Chapter 4 §4.4: Curriculum Analysis & Difficulty Dynamics (Regime-aware ACG) ===\n","# Single, clean Colab cell.\n","# - Computes permutation entropy (PE) on rolling, past-only windows per asset\n","# - Labels regimes via train-split tertiles (low/med/high) to avoid leakage\n","# - Defines arms = (asset, horizon, regime) and re-runs UCB1 + REINFORCE\n","# - Exports regime-stratified metrics, selection logs, transitions, and many figures\n","\n","!pip -q install torchinfo torchviz > /dev/null\n","\n","import os, json, math, random, warnings\n","from dataclasses import dataclass, asdict\n","from typing import Dict, Tuple, List\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torchinfo import summary as torch_summary\n","try:\n","    from torchviz import make_dot\n","    TORCHVIZ_OK = True\n","except Exception:\n","    TORCHVIZ_OK = False\n","\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","\n","# -----------------------------\n","# Paths & folders\n","# -----------------------------\n","BASE_DIR = \"/content\"\n","TBL_DIR  = f\"{BASE_DIR}/export/tables\"\n","FIG_DIR  = f\"{BASE_DIR}/export/figures\"\n","LOG_DIR  = f\"{BASE_DIR}/export/logs\"\n","for d in [TBL_DIR, FIG_DIR, LOG_DIR]:\n","    os.makedirs(d, exist_ok=True)\n","\n","# -----------------------------\n","# Config\n","# -----------------------------\n","@dataclass\n","class RegimeACGConfig:\n","    ds_csv: str = f\"{BASE_DIR}/export/tables/dataset_long_1D.csv\"\n","    assets_max: int = 5                 # top-N assets by coverage\n","    horizons: Tuple[int,...] = (1,3,7)\n","    win: int = 64                       # model input window\n","    pe_win: int = 64                    # permutation-entropy window (past-only)\n","    pe_m: int = 4                       # embedding dimension (m!)\n","    pe_tau: int = 1                     # delay\n","    min_samples_per_arm: int = 128      # skip arms with too few samples\n","    batch_size: int = 256\n","    teacher_rounds: int = 240\n","    train_steps_per_pull: int = 1\n","    lr: float = 2e-3\n","    entropy_beta: float = 1e-3\n","    aux_sup_weight: float = 1e-1\n","    ucb_c: float = 1.2\n","    seed: int = 1337\n","    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    lstm_hidden: int = 64\n","    lstm_layers: int = 1\n","    lstm_dropout: float = 0.0\n","    eval_n_trace: int = 400\n","\n","CFG = RegimeACGConfig()\n","random.seed(CFG.seed); np.random.seed(CFG.seed); torch.manual_seed(CFG.seed)\n","print(\"§4.4 Config\\n\", json.dumps(asdict(CFG), indent=2))\n","\n","# -----------------------------\n","# Load dataset & standardize (train-only stats)\n","# -----------------------------\n","df = pd.read_csv(CFG.ds_csv, parse_dates=[\"timestamp\"])\n","assert {\"timestamp\",\"symbol\",\"close\",\"split\"}.issubset(df.columns), \"Dataset missing required columns.\"\n","\n","# top-N by coverage\n","top_assets = (df.groupby(\"symbol\").size()\n","              .sort_values(ascending=False)\n","              .head(CFG.assets_max).index.tolist())\n","df = df[df[\"symbol\"].isin(top_assets)].copy()\n","df = df.sort_values([\"symbol\",\"timestamp\"])\n","\n","scalers: Dict[str, Tuple[float,float]] = {}\n","for a, g in df[df[\"split\"]==\"train\"].groupby(\"symbol\"):\n","    mu, sd = g[\"close\"].mean(), g[\"close\"].std(ddof=0)\n","    scalers[a] = (float(mu), float(sd if sd>0 else 1.0))\n","\n","def zscore(asset, x):\n","    mu, sd = scalers[asset]; return (x - mu) / sd\n","df[\"z\"] = df.apply(lambda r: zscore(r[\"symbol\"], r[\"close\"]), axis=1)\n","\n","# -----------------------------\n","# Permutation Entropy utilities\n","# -----------------------------\n","from math import factorial\n","\n","def _perm_index(perm: np.ndarray) -> int:\n","    \"\"\"Map a permutation (e.g., [0,2,1]) to a unique index in [0, m!-1].\"\"\"\n","    # Lehmer code / factoradic mapping\n","    m = len(perm)\n","    code = 0\n","    for i in range(m):\n","        c = 0\n","        for j in range(i+1, m):\n","            if perm[j] < perm[i]:\n","                c += 1\n","        code = code * (m - i) + c\n","    return code\n","\n","def permutation_entropy_window(x: np.ndarray, m: int = 4) -> float:\n","    \"\"\"\n","    Compute normalized permutation entropy for a 1D array x using overlapping\n","    ordinal patterns of length m (tau=1) inside this window.\n","    \"\"\"\n","    n = len(x)\n","    if n < m:\n","        return np.nan\n","    counts = np.zeros(factorial(m), dtype=np.int64)\n","    # Use stable argsort to break ties by index (consistent)\n","    for i in range(n - m + 1):\n","        pat = np.argsort(x[i:i+m], kind=\"mergesort\")\n","        idx = _perm_index(pat)\n","        counts[idx] += 1\n","    total = counts.sum()\n","    if total == 0:\n","        return np.nan\n","    p = counts[counts>0].astype(np.float64) / total\n","    H = -np.sum(p * np.log(p))\n","    Hmax = np.log(factorial(m))\n","    return float(H / Hmax)  # normalized to [0,1]\n","\n","def permutation_entropy_series(x: np.ndarray, win: int = 64, m: int = 4) -> np.ndarray:\n","    \"\"\"\n","    For each index t, compute PE on the past-only window x[max(0,t-win+1):t+1].\n","    For t < win-1, returns NaN.\n","    \"\"\"\n","    n = len(x)\n","    out = np.full(n, np.nan, dtype=np.float32)\n","    for t in range(win-1, n):\n","        xw = x[t - win + 1 : t + 1]\n","        out[t] = permutation_entropy_window(xw, m=m)\n","    return out\n","\n","# -----------------------------\n","# Compute PE and regime tiers per asset (train-only tertiles)\n","# -----------------------------\n","pe_records = []\n","for a, g in df.groupby(\"symbol\"):\n","    z_all = g[\"z\"].values.astype(np.float32)\n","    H = permutation_entropy_series(z_all, win=CFG.pe_win, m=CFG.pe_m)\n","    g = g.copy()\n","    g[\"pe_norm\"] = H\n","\n","    # thresholds from TRAIN split only (avoid leakage)\n","    g_train = g[g[\"split\"]==\"train\"].dropna(subset=[\"pe_norm\"])\n","    if len(g_train) < 32:\n","        # fallback: global tertiles if train too small\n","        thr1, thr2 = np.nanpercentile(g[\"pe_norm\"], [33, 67])\n","    else:\n","        thr1, thr2 = np.nanpercentile(g_train[\"pe_norm\"], [33, 67])\n","\n","    def tier(h):\n","        if np.isnan(h): return np.nan\n","        if h <= thr1: return \"low\"\n","        if h <= thr2: return \"mid\"\n","        return \"high\"\n","\n","    g[\"regime\"] = g[\"pe_norm\"].apply(tier)\n","    pe_records.append(g)\n","\n","df_pe = pd.concat(pe_records, ignore_index=True)\n","# Save PE artifacts\n","df_pe.to_csv(f\"{TBL_DIR}/perm_entropy_with_regimes.csv\", index=False)\n","\n","# Example figure: BTC segmentation (price + PE + regime shades)\n","def plot_btc_segmentation():\n","    g = df_pe[df_pe[\"symbol\"]==\"BTC\"].dropna(subset=[\"pe_norm\"]).copy()\n","    if len(g)==0:\n","        return None\n","    fig, ax = plt.subplots(2, 1, figsize=(10,5), sharex=True)\n","    ax[0].plot(g[\"timestamp\"], g[\"z\"], lw=0.8)\n","    ax[0].set_title(\"BTC standardized close\")\n","    ax[1].plot(g[\"timestamp\"], g[\"pe_norm\"], lw=0.8)\n","    ax[1].set_title(f\"BTC permutation entropy (m={CFG.pe_m}, win={CFG.pe_win})\")\n","    # shade regimes\n","    for tier_name, color in [(\"low\",\"#d0f0c0\"), (\"mid\",\"#fff3b0\"), (\"high\",\"#f4cccc\")]:\n","        mask = (g[\"regime\"]==tier_name)\n","        if mask.any():\n","            # Create spans per contiguous block\n","            idx = np.where(mask.values)[0]\n","            # find contiguous ranges\n","            start = None\n","            for i in range(len(idx)):\n","                if start is None: start = idx[i]\n","                if i==len(idx)-1 or idx[i+1] != idx[i]+1:\n","                    s, e = start, idx[i]\n","                    ax[1].axvspan(g[\"timestamp\"].iloc[s], g[\"timestamp\"].iloc[e], color=color, alpha=0.3)\n","                    start = None\n","    for a in ax: a.grid(True, alpha=0.2)\n","    plt.tight_layout()\n","    p = os.path.join(FIG_DIR, \"regime_segmentation_BTC.png\")\n","    plt.savefig(p, dpi=150); plt.close()\n","    return p\n","\n","seg_fig = plot_btc_segmentation()\n","\n","# -----------------------------\n","# Build regime-aware sliding-window datasets\n","# -----------------------------\n","def make_xy_with_regime(series: pd.Series, regimes: pd.Series, horizon: int, win: int):\n","    \"\"\"\n","    Returns dict: regime -> (X,Y,LAST) where regime in {\"low\",\"mid\",\"high\"}\n","    Uses window ending at t (past-only). Regime label is regimes.iloc[t] at window end.\n","    \"\"\"\n","    vals = series.values\n","    regs = regimes.values\n","    out = {\"low\":[], \"mid\":[], \"high\":[]}\n","    last_out = {\"low\":[], \"mid\":[], \"high\":[]}\n","    y_out = {\"low\":[], \"mid\":[], \"high\":[]}\n","    for t in range(win-1, len(vals)-horizon):\n","        r = regs[t]\n","        if r not in out:\n","            continue\n","        x = vals[t-win+1:t+1].astype(np.float32)\n","        y = np.float32(vals[t+horizon])\n","        out[r].append(x)\n","        y_out[r].append(y)\n","        last_out[r].append(np.float32(vals[t]))\n","    # convert lists to arrays\n","    out2 = {}\n","    for r in [\"low\",\"mid\",\"high\"]:\n","        if len(out[r])>0:\n","            X = np.stack(out[r])[:, :, None].astype(np.float32)\n","            Y = np.stack(y_out[r])[:, None].astype(np.float32)\n","            LAST = np.stack(last_out[r])[:, None].astype(np.float32)\n","            out2[r] = (X, Y, LAST)\n","    return out2\n","\n","data_xy = {}\n","coverage = []\n","for a, g in df_pe.groupby(\"symbol\"):\n","    g = g.sort_values(\"timestamp\").copy()\n","    for split in [\"train\",\"val\",\"test\"]:\n","        gz = g[g[\"split\"]==split]\n","        z = gz[\"z\"].reset_index(drop=True)\n","        reg = gz[\"regime\"].reset_index(drop=True)\n","        for h in CFG.horizons:\n","            buckets = make_xy_with_regime(z, reg, h, CFG.win)\n","            for r, tup in buckets.items():\n","                X, Y, LAST = tup\n","                key = (a, split, h, r)\n","                data_xy[key] = (X, Y, LAST)\n","                coverage.append((a, split, h, r, len(Y)))\n","\n","cov_df = pd.DataFrame(coverage, columns=[\"asset\",\"split\",\"horizon\",\"regime\",\"samples\"])\n","cov_df.to_csv(f\"{TBL_DIR}/acg_regime_coverage_windows.csv\", index=False)\n","\n","# Arms: (asset,horizon,regime) with sufficient train/val/test samples\n","arms: List[Tuple[str,int,str]] = []\n","for a in top_assets:\n","    for h in CFG.horizons:\n","        for r in [\"low\",\"mid\",\"high\"]:\n","            ok = True\n","            for sp in [\"train\",\"val\",\"test\"]:\n","                key = (a, sp, h, r)\n","                if key not in data_xy: ok=False; break\n","                if data_xy[key][1].shape[0] < CFG.min_samples_per_arm: ok=False; break\n","            if ok:\n","                arms.append((a,h,r))\n","assert len(arms)>0, \"No regime-aware arms met the sample threshold—try lowering CFG.min_samples_per_arm.\"\n","\n","n_arms = len(arms)\n","print(f\"Arms (asset,horizon,regime): {arms}\")\n","\n","# -----------------------------\n","# Student model (same as §4.3)\n","# -----------------------------\n","class PolicyLSTM(nn.Module):\n","    def __init__(self, hidden=64, layers=1, dropout=0.0):\n","        super().__init__()\n","        self.lstm = nn.LSTM(input_size=1, hidden_size=hidden, num_layers=layers,\n","                            dropout=(dropout if layers>1 else 0.0), batch_first=True)\n","        self.mu = nn.Sequential(nn.Linear(hidden, hidden), nn.ReLU(), nn.Linear(hidden, 1))\n","        self.log_sigma = nn.Parameter(torch.tensor(-0.5))  # global log-std\n","    def forward(self, x):\n","        o, _ = self.lstm(x)\n","        last = o[:, -1, :]\n","        mu = self.mu(last)\n","        log_sigma = self.log_sigma.expand_as(mu)\n","        return mu, log_sigma\n","\n","def count_params(m): return sum(p.numel() for p in m.parameters() if p.requires_grad)\n","\n","device = torch.device(CFG.device)\n","policy = PolicyLSTM(hidden=CFG.lstm_hidden, layers=CFG.lstm_layers, dropout=CFG.lstm_dropout).to(device)\n","opt = torch.optim.AdamW(policy.parameters(), lr=CFG.lr)\n","\n","# -----------------------------\n","# Teacher: UCB1 w/ learning-progress reward\n","# -----------------------------\n","class UCB1Teacher:\n","    def __init__(self, n_arms, c=1.2):\n","        self.c = c; self.n = np.zeros(n_arms, dtype=np.int64)\n","        self.mean = np.zeros(n_arms, dtype=np.float64); self.t = 0\n","    def select(self):\n","        self.t += 1\n","        for i in range(n_arms):\n","            if self.n[i]==0: return i\n","        ucb = self.mean + self.c * np.sqrt(2.0 * math.log(self.t) / self.n)\n","        return int(np.argmax(ucb))\n","    def update(self, i, reward):\n","        self.n[i] += 1\n","        self.mean[i] += (reward - self.mean[i]) / self.n[i]\n","\n","teacher = UCB1Teacher(n_arms, CFG.ucb_c)\n","\n","# -----------------------------\n","# Batching, eval, metrics\n","# -----------------------------\n","def sample_train_minibatch(arm_idx: int, batch_size: int):\n","    a,h,r = arms[arm_idx]\n","    X,Y,L = data_xy[(a,\"train\",h,r)]\n","    idx = np.random.randint(0, len(Y), size=(min(batch_size, len(Y)),))\n","    xb = torch.from_numpy(X[idx]).float().to(device)\n","    yb = torch.from_numpy(Y[idx]).float().to(device)\n","    lb = torch.from_numpy(L[idx]).float().to(device)\n","    return xb, yb, lb\n","\n","def eval_val_mse(policy, arm_idx: int) -> float:\n","    a,h,r = arms[arm_idx]\n","    X,Y,_ = data_xy[(a,\"val\",h,r)]\n","    with torch.no_grad():\n","        xb = torch.from_numpy(X).float().to(device)\n","        mu, _ = policy(xb)\n","        yhat = mu.squeeze(1).cpu().numpy()\n","    return float(np.mean((Y.squeeze(1) - yhat)**2))\n","\n","def mae(y, yhat): return float(np.mean(np.abs(y - yhat)))\n","def rmse(y, yhat): return float(np.sqrt(np.mean((y - yhat)**2)))\n","def smape(y, yhat, eps=1e-8):\n","    denom = (np.abs(y) + np.abs(yhat) + eps)/2.0\n","    return float(np.mean(np.abs(y - yhat)/denom) * 100.0)\n","def dir_acc(y_prev, y_true, yhat):\n","    s_true = np.sign(y_true - y_prev); s_pred = np.sign(yhat - y_prev)\n","    return float(np.mean((s_true == s_pred).astype(np.float32)))\n","\n","# -----------------------------\n","# REINFORCE step\n","# -----------------------------\n","baseline_reward = 0.0\n","baseline_momentum = 0.9\n","def reinforce_step(xb, yb):\n","    policy.train()\n","    opt.zero_grad()\n","    mu, log_sigma = policy(xb)\n","    sigma = torch.exp(log_sigma)\n","    dist = torch.distributions.Normal(mu, sigma)\n","    a = dist.rsample()\n","    r = - (a - yb)**2\n","    global baseline_reward\n","    avg_r = r.mean().detach()\n","    advantage = r - baseline_reward\n","    baseline_reward = baseline_momentum * baseline_reward + (1.0 - baseline_momentum) * avg_r\n","    loss_policy = - (dist.log_prob(a) * advantage.detach()).mean()\n","    loss_entropy = - CFG.entropy_beta * dist.entropy().mean()\n","    loss_aux = CFG.aux_sup_weight * nn.MSELoss()(mu, yb)\n","    loss = loss_policy + loss_entropy + loss_aux\n","    loss.backward()\n","    nn.utils.clip_grad_norm_(policy.parameters(), 1.0)\n","    opt.step()\n","\n","# Warm start: one pass across arms\n","for i in range(n_arms):\n","    xb, yb, _ = sample_train_minibatch(i, CFG.batch_size)\n","    reinforce_step(xb, yb)\n","\n","# -----------------------------\n","# Bandit loop with regime-aware arms\n","# -----------------------------\n","log = {\"round\":[], \"arm\":[], \"asset\":[], \"horizon\":[], \"regime\":[],\n","       \"reward_lp\":[], \"val_before\":[], \"val_after\":[], \"mean_reward_est\":[], \"pulls_arm\":[]}\n","\n","for t in range(1, CFG.teacher_rounds + 1):\n","    i = teacher.select()\n","    a,h,r = arms[i]\n","    val_before = eval_val_mse(policy, i)\n","    xb, yb, _ = sample_train_minibatch(i, CFG.batch_size)\n","    for _ in range(CFG.train_steps_per_pull): reinforce_step(xb, yb)\n","    val_after = eval_val_mse(policy, i)\n","    reward_lp = float(val_before - val_after)\n","    teacher.update(i, reward_lp)\n","\n","    log[\"round\"].append(t); log[\"arm\"].append(i)\n","    log[\"asset\"].append(a); log[\"horizon\"].append(h); log[\"regime\"].append(r)\n","    log[\"reward_lp\"].append(reward_lp)\n","    log[\"val_before\"].append(val_before); log[\"val_after\"].append(val_after)\n","    log[\"mean_reward_est\"].append(float(teacher.mean[i])); log[\"pulls_arm\"].append(int(teacher.n[i]))\n","\n","log_df = pd.DataFrame(log)\n","log_csv = f\"{TBL_DIR}/acg_regime_teacher_log.csv\"\n","log_df.to_csv(log_csv, index=False)\n","\n","# Selection counts\n","sel_df = (log_df.groupby([\"arm\",\"asset\",\"horizon\",\"regime\"])\n","          .size().reset_index(name=\"pulls\").sort_values(\"arm\"))\n","sel_df[\"mean_reward\"] = [float(teacher.mean[int(i)]) for i in sel_df[\"arm\"].values]\n","sel_df.to_csv(f\"{TBL_DIR}/acg_regime_selection_counts.csv\", index=False)\n","\n","# -----------------------------\n","# Regime transitions heatmap\n","# -----------------------------\n","reg_map = {\"low\":0,\"mid\":1,\"high\":2}\n","reg_list = log_df[\"regime\"].map(reg_map).values\n","trans = np.zeros((3,3), dtype=np.int64)\n","for i in range(1, len(reg_list)):\n","    trans[reg_list[i-1], reg_list[i]] += 1\n","trans_df = pd.DataFrame(trans, index=[\"low\",\"mid\",\"high\"], columns=[\"low\",\"mid\",\"high\"])\n","trans_df.to_csv(f\"{TBL_DIR}/acg_regime_transitions.csv\")\n","\n","plt.figure(figsize=(4.5,4))\n","plt.imshow(trans, cmap=\"Blues\")\n","plt.xticks([0,1,2], [\"low\",\"mid\",\"high\"]); plt.yticks([0,1,2], [\"low\",\"mid\",\"high\"])\n","plt.title(\"Regime transitions (teacher selections)\")\n","for i in range(3):\n","    for j in range(3):\n","        plt.text(j, i, str(trans[i,j]), ha=\"center\", va=\"center\")\n","plt.tight_layout()\n","plt.savefig(os.path.join(FIG_DIR, \"acg_regime_transition_heatmap.png\"), dpi=150); plt.close()\n","\n","# -----------------------------\n","# Plots: selections, rewards, regret, val MSE\n","# -----------------------------\n","def savefig(fname):\n","    p = os.path.join(FIG_DIR, fname); plt.tight_layout(); plt.savefig(p, dpi=150); plt.close(); return p\n","\n","labels = [f\"{a}-H{h}-{r}\" for (a,h,r) in arms]\n","# Arm index over time\n","plt.figure(figsize=(9,3))\n","plt.plot(log_df[\"arm\"].values, lw=0.8)\n","plt.yticks(range(len(labels)), labels)\n","plt.xlabel(\"Round\"); plt.ylabel(\"Selected arm\")\n","plt.title(\"Regime-aware arm selection over time\")\n","f_sel_over_time = savefig(\"acg_regime_arm_selection_over_time.png\")\n","\n","# Regime-only over time\n","plt.figure(figsize=(9,2.6))\n","plt.scatter(log_df[\"round\"], log_df[\"regime\"].map({\"low\":0,\"mid\":1,\"high\":2}), s=8)\n","plt.yticks([0,1,2], [\"low\",\"mid\",\"high\"])\n","plt.title(\"Selected regime over time\"); plt.xlabel(\"Round\"); plt.ylabel(\"Regime\")\n","f_reg_over_time = savefig(\"acg_selected_regime_over_time.png\")\n","\n","# Selection counts bar\n","plt.figure(figsize=(8,3))\n","plt.bar(range(len(labels)), sel_df.sort_values(\"arm\")[\"pulls\"].values)\n","plt.xticks(range(len(labels)), labels, rotation=60, ha=\"right\")\n","plt.ylabel(\"Pulls\"); plt.title(\"Selection counts by arm (regime-aware)\")\n","f_sel_counts = savefig(\"acg_regime_selection_counts.png\")\n","\n","# Mean reward by arm\n","plt.figure(figsize=(8,3))\n","plt.bar(range(len(labels)), sel_df.sort_values(\"arm\")[\"mean_reward\"].values)\n","plt.xticks(range(len(labels)), labels, rotation=60, ha=\"right\")\n","plt.ylabel(\"Mean reward (Δ val MSE)\"); plt.title(\"Mean learning progress by arm\")\n","f_mean_reward = savefig(\"acg_regime_mean_reward_by_arm.png\")\n","\n","# Rolling mean reward\n","plt.figure(figsize=(7,3))\n","plt.plot(pd.Series(log_df[\"reward_lp\"]).rolling(10).mean())\n","plt.axhline(0.0, color=\"black\", lw=1)\n","plt.title(\"Learning progress (rolling-10 mean)\"); plt.xlabel(\"Round\"); plt.ylabel(\"Δ val MSE\")\n","f_reward_curve = savefig(\"acg_regime_reward_curve.png\")\n","\n","# Cumulative regret (approx, using best-so-far mean)\n","inst_regret = []\n","for r in range(len(log_df)):\n","    means_snapshot = (\n","        log_df.iloc[:r+1]\n","        .groupby(\"arm\")[\"mean_reward_est\"]\n","        .last()\n","        .to_dict()\n","    )\n","    best_mean = max(means_snapshot.values()) if len(means_snapshot)>0 else 0.0\n","    inst_regret.append(best_mean - log_df.iloc[r][\"reward_lp\"])\n","regret = np.cumsum(inst_regret)\n","\n","plt.figure(figsize=(7,3))\n","plt.plot(regret); plt.title(\"Cumulative regret (approx.)\")\n","plt.xlabel(\"Round\"); plt.ylabel(\"Regret\")\n","f_regret = savefig(\"acg_regime_cumulative_regret.png\")\n","\n","# Final validation MSE by arm\n","val_after_latest = []\n","for i in range(n_arms):\n","    val_after_latest.append(eval_val_mse(policy, i))\n","val_snap_df = pd.DataFrame({\n","    \"arm_idx\": list(range(n_arms)),\n","    \"asset\": [a for a,_,_ in arms],\n","    \"horizon\": [h for _,h,_ in arms],\n","    \"regime\": [r for *_,r in arms],\n","    \"val_mse_final\": val_after_latest,\n","    \"pulls\": sel_df.sort_values(\"arm\")[\"pulls\"].values\n","})\n","val_snap_df.to_csv(f\"{TBL_DIR}/acg_regime_val_mse_final.csv\", index=False)\n","\n","plt.figure(figsize=(9,3))\n","plt.bar(range(n_arms), val_snap_df[\"val_mse_final\"].values)\n","plt.xticks(range(n_arms), labels, rotation=60, ha=\"right\")\n","plt.ylabel(\"Val MSE\"); plt.title(\"Final validation MSE by arm (regime-aware)\")\n","f_val_mse_final = savefig(\"acg_regime_val_mse_final_bars.png\")\n","\n","# Reward by regime\n","reg_reward = log_df.groupby(\"regime\")[\"reward_lp\"].mean().reindex([\"low\",\"mid\",\"high\"])\n","plt.figure(figsize=(5,3))\n","plt.bar(reg_reward.index, reg_reward.values)\n","plt.ylabel(\"Mean reward (Δ val MSE)\"); plt.title(\"Learning progress by regime\")\n","f_reward_by_regime = savefig(\"acg_mean_reward_by_regime.png\")\n","\n","# -----------------------------\n","# Test evaluation per (asset,horizon,regime)\n","# -----------------------------\n","results = []\n","for i in range(n_arms):\n","    a,h,r = arms[i]\n","    X,Y,L = data_xy[(a,\"test\",h,r)]\n","    with torch.no_grad():\n","        xb = torch.from_numpy(X).float().to(device)\n","        mu, _ = policy(xb)\n","        yhat = mu.squeeze(1).cpu().numpy()\n","    y_true = Y.squeeze(1)\n","    y_prev = L.squeeze(1)\n","    results.append(dict(model=\"ACG-DRL\", asset=a, horizon=h, regime=r,\n","                        MAE=mae(y_true, yhat), RMSE=rmse(y_true, yhat),\n","                        sMAPE=smape(y_true, yhat), DA=dir_acc(y_prev, y_true, yhat)))\n","\n","res_df = pd.DataFrame(results).sort_values([\"asset\",\"horizon\",\"regime\"])\n","res_csv = f\"{TBL_DIR}/acg_results_per_arm_regime.csv\"\n","res_df.to_csv(res_csv, index=False)\n","\n","# Aggregate by horizon & regime\n","agg_df = (res_df.groupby([\"horizon\",\"regime\"])\n","          .agg(MAE=(\"MAE\",\"mean\"), RMSE=(\"RMSE\",\"mean\"), sMAPE=(\"sMAPE\",\"mean\"), DA=(\"DA\",\"mean\"))\n","          .reset_index().sort_values([\"horizon\",\"regime\"]))\n","agg_csv = f\"{TBL_DIR}/acg_results_agg_by_horizon_regime.csv\"\n","agg_df.to_csv(agg_csv, index=False)\n","\n","# Plots: regime-stratified accuracy (MAE and DA)\n","def barplot_metric(df, metric, fname, title):\n","    plt.figure(figsize=(7,4))\n","    labels_h = sorted(df[\"horizon\"].unique())\n","    regs = [\"low\",\"mid\",\"high\"]\n","    width = 0.22; idx = np.arange(len(labels_h))\n","    for i, r in enumerate(regs):\n","        sub = (df[df[\"regime\"]==r]\n","               .set_index(\"horizon\")\n","               .reindex(labels_h))\n","        plt.bar(idx + i*width, sub[metric].values, width=width, label=r)\n","    plt.xticks(idx + width, [f\"H={h}\" for h in labels_h])\n","    plt.ylabel(metric); plt.title(title); plt.legend()\n","    p = os.path.join(FIG_DIR, fname); plt.tight_layout(); plt.savefig(p, dpi=150); plt.close(); return p\n","\n","p_mae = barplot_metric(agg_df, \"MAE\",   \"acg_regime_mae_bars.png\", \"MAE by horizon and regime\")\n","p_da  = barplot_metric(agg_df, \"DA\",    \"acg_regime_da_bars.png\",  \"Directional Accuracy by horizon and regime\")\n","p_rmse= barplot_metric(agg_df, \"RMSE\",  \"acg_regime_rmse_bars.png\",\"RMSE by horizon and regime\")\n","p_sm  = barplot_metric(agg_df, \"sMAPE\", \"acg_regime_smape_bars.png\",\"sMAPE by horizon and regime\")\n","\n","# -----------------------------\n","# Model reports (student)\n","# -----------------------------\n","model_params = count_params(policy)\n","ms = torch_summary(policy, input_size=(1, CFG.win, 1), verbose=0)\n","with open(os.path.join(TBL_DIR, \"acg_regime_student_model_summary.txt\"), \"w\") as f:\n","    f.write(str(ms)); f.write(f\"\\nTotal trainable parameters: {model_params}\\n\")\n","\n","if TORCHVIZ_OK:\n","    try:\n","        xdummy = torch.randn(1, CFG.win, 1).to(device)\n","        mu, _ = policy(xdummy)\n","        dot = make_dot(mu, params=dict(list(policy.named_parameters())))\n","        dot.render(os.path.join(FIG_DIR, \"acg_regime_student_graph\"), format=\"png\", cleanup=True)\n","    except Exception as e:\n","        print(\"torchviz failed:\", e)\n","\n","# -----------------------------\n","# Final print summary\n","# -----------------------------\n","print(\"\\n=== §4.4 RESULTS SUMMARY (paste into chat) ===\")\n","print(\"Regime coverage windows:\", f\"{TBL_DIR}/acg_regime_coverage_windows.csv\")\n","print(\"Arms (asset,horizon,regime):\", arms)\n","print(\"\\nSelection counts & mean rewards (CSV):\", f\"{TBL_DIR}/acg_regime_selection_counts.csv\")\n","print(sel_df.to_string(index=False))\n","print(\"\\nRegime transition matrix (teacher selections) CSV:\", f\"{TBL_DIR}/acg_regime_transitions.csv\")\n","print(trans_df.to_string())\n","print(\"\\nACG-DRL per-arm (asset,horizon,regime) TEST results CSV:\", res_csv)\n","print(res_df.to_string(index=False))\n","print(\"\\nAggregated by horizon × regime CSV:\", agg_csv)\n","print(agg_df.to_string(index=False))\n","print(\"\\nStudent params:\", model_params, \"| Summary saved to:\", os.path.join(TBL_DIR, \"acg_regime_student_model_summary.txt\"))\n","\n","# Figure inventory\n","figs = sorted([os.path.join(FIG_DIR, f) for f in os.listdir(FIG_DIR) if f.endswith(\".png\")])\n","print(\"\\n=== FIGURES SAVED (first 40) ===\")\n","for p in figs[:40]:\n","    print(p)\n","if len(figs) > 40:\n","    print(f\"... and {len(figs)-40} more figures\")\n","\n","print(\"\\nKey figures:\")\n","print(\" - Regime segmentation example:\", seg_fig)\n","print(\" - Arm selection over time (regime-aware):\", f_sel_over_time)\n","print(\" - Selected regime over time:\", f_reg_over_time)\n","print(\" - Selection counts:\", f_sel_counts)\n","print(\" - Mean reward by arm:\", f_mean_reward)\n","print(\" - Rolling reward:\", f_reward_curve)\n","print(\" - Cumulative regret:\", f_regret)\n","print(\" - Final val MSE bars:\", f_val_mse_final)\n","print(\" - Reward by regime:\", f_reward_by_regime)\n","print(\" - Regime-stratified bars (MAE/DA/RMSE/sMAPE):\", p_mae, p_da, p_rmse, p_sm)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zINwuMn3ZEAc","executionInfo":{"status":"aborted","timestamp":1761543222820,"user_tz":-180,"elapsed":56354,"user":{"displayName":"David Kibaara","userId":"07870126442739887481"}}},"outputs":[],"source":["# === Chapter 4 §4.5: Robustness & Ablations (CLEAN FIXED CELL) ===\n","# Variants:\n","#   V1: ACG-UCB1 + price-only features\n","#   V2: ACG-UCB1 + augmented features (returns/vol/RSI/MA deltas)\n","#   V3: Uniform Teacher + augmented features\n","#   V4: No-Curriculum Supervised (pooled MSE) + augmented features\n","#\n","# Fixes vs previous:\n","# - Strict NaN/Inf handling in feature engineering and windowing (impute with 0 after z-standardization).\n","# - Skip any training/eval window containing non-finite values.\n","# - Defensive clipping of standardized engineered features to [-10, 10] to avoid numeric blow-ups.\n","# - Added sanity summaries for dropped windows.\n","\n","!pip -q install torchinfo torchviz > /dev/null\n","\n","import os, json, math, random, warnings\n","from dataclasses import dataclass, asdict\n","from typing import Dict, Tuple, List\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torchinfo import summary as torch_summary\n","try:\n","    from torchviz import make_dot\n","    TORCHVIZ_OK = True\n","except Exception:\n","    TORCHVIZ_OK = False\n","\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","\n","# -----------------------------\n","# Paths & folders\n","# -----------------------------\n","BASE_DIR = \"/content\"\n","TBL_DIR  = f\"{BASE_DIR}/export/tables\"\n","FIG_DIR  = f\"{BASE_DIR}/export/figures\"\n","LOG_DIR  = f\"{BASE_DIR}/export/logs\"\n","for d in [TBL_DIR, FIG_DIR, LOG_DIR]: os.makedirs(d, exist_ok=True)\n","\n","# -----------------------------\n","# Config\n","# -----------------------------\n","@dataclass\n","class AblationCfg:\n","    ds_csv: str = f\"{BASE_DIR}/export/tables/dataset_long_1D.csv\"\n","    assets_max: int = 5                 # top-N assets by coverage\n","    horizons: Tuple[int,...] = (1,3,7)\n","    win: int = 64\n","    batch_size: int = 256\n","    epochs_supervised: int = 4          # for V4 pooled MSE\n","    teacher_rounds: int = 150           # per bandit run\n","    train_steps_per_pull: int = 1\n","    lr: float = 2e-3\n","    entropy_beta: float = 1e-3\n","    aux_sup_weight: float = 1e-1\n","    ucb_c: float = 1.2\n","    seed: int = 1337\n","    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    lstm_hidden: int = 64\n","    lstm_layers: int = 1\n","    lstm_dropout: float = 0.0\n","    eval_n_trace: int = 400\n","CFG = AblationCfg()\n","random.seed(CFG.seed); np.random.seed(CFG.seed); torch.manual_seed(CFG.seed)\n","\n","print(\"§4.5 Config\\n\", json.dumps(asdict(CFG), indent=2))\n","\n","# -----------------------------\n","# Load dataset and standardize close (train-only stats)\n","# -----------------------------\n","df = pd.read_csv(CFG.ds_csv, parse_dates=[\"timestamp\"])\n","assert {\"timestamp\",\"symbol\",\"close\",\"split\"}.issubset(df.columns), \"Dataset missing required columns.\"\n","\n","# Select top assets by coverage\n","top_assets = (df.groupby(\"symbol\").size()\n","              .sort_values(ascending=False)\n","              .head(CFG.assets_max).index.tolist())\n","df = df[df[\"symbol\"].isin(top_assets)].copy()\n","df = df.sort_values([\"symbol\",\"timestamp\"])\n","\n","# Per-asset z-score on close using TRAIN stats only\n","scalers_close: Dict[str, Tuple[float,float]] = {}\n","for a, g in df[df[\"split\"]==\"train\"].groupby(\"symbol\"):\n","    mu, sd = g[\"close\"].mean(), g[\"close\"].std(ddof=0)\n","    scalers_close[a] = (float(mu), float(sd if sd>0 else 1.0))\n","\n","def z_close(asset, x):\n","    mu, sd = scalers_close[asset]; return (x - mu) / sd\n","\n","df[\"z\"] = df.apply(lambda r: z_close(r[\"symbol\"], r[\"close\"]), axis=1)\n","\n","# -----------------------------\n","# Feature engineering (leakage-safe; past-only) + robust sanitization\n","# -----------------------------\n","def rsi(series: pd.Series, period: int = 14) -> pd.Series:\n","    delta = series.diff()\n","    up = delta.clip(lower=0.0)\n","    down = -delta.clip(upper=0.0)\n","    roll_up = up.rolling(period, min_periods=period).mean()\n","    roll_down = down.rolling(period, min_periods=period).mean()\n","    rs = roll_up / (roll_down + 1e-12)\n","    rsi = 100.0 - (100.0 / (1.0 + rs))\n","    return rsi\n","\n","def _sanitize_cols(df_in: pd.DataFrame, cols: List[str]) -> pd.DataFrame:\n","    out = df_in.copy()\n","    out[cols] = out[cols].replace([np.inf, -np.inf], np.nan)\n","    out[cols] = out[cols].fillna(0.0)\n","    # defensive clipping to avoid extreme standardized values from tiny stds\n","    out[cols] = out[cols].clip(-10.0, 10.0)\n","    return out\n","\n","def build_features(g: pd.DataFrame, augmented: bool) -> pd.DataFrame:\n","    \"\"\"\n","    Build per-asset features (past-only). Returns dataframe with columns:\n","    timestamp, symbol, split, z, and additional standardized features if augmented=True.\n","    Standardization for engineered features uses TRAIN split stats per asset/feature.\n","    NaN/Inf values are imputed with 0 after standardization (i.e., the train-mean baseline).\n","    \"\"\"\n","    g = g.sort_values(\"timestamp\").copy()\n","    if not augmented:\n","        # Ensure z has no NaN/Inf (shouldn't, but sanitize anyway)\n","        return _sanitize_cols(g[[\"timestamp\",\"symbol\",\"split\",\"z\"]].copy(), [\"z\"])\n","\n","    # Price-derived features (all past-only by construction)\n","    z = g[\"z\"].astype(float)\n","    ret = z.diff()                                    # Δz (proxy for standardized return)\n","    vol14 = ret.rolling(14, min_periods=14).std()\n","    rsi14 = rsi(g[\"close\"], period=14)\n","    ma5 = z.rolling(5, min_periods=5).mean()\n","    ma20 = z.rolling(20, min_periods=20).mean()\n","    macd = ma5 - ma20\n","    dist_ma20 = z - ma20\n","\n","    feats = pd.DataFrame({\n","        \"timestamp\": g[\"timestamp\"].values,\n","        \"symbol\": g[\"symbol\"].values,\n","        \"split\": g[\"split\"].values,\n","        \"z\": z.values,\n","        \"dz\": ret.values,\n","        \"vol14\": vol14.values,\n","        \"rsi14\": rsi14.values,\n","        \"macd_z\": macd.values,\n","        \"dist_ma20_z\": dist_ma20.values\n","    })\n","\n","    # Per-asset, per-feature standardization using TRAIN stats only\n","    std_cols = [\"dz\",\"vol14\",\"rsi14\",\"macd_z\",\"dist_ma20_z\"]\n","    out_list = []\n","    for a, ga in feats.groupby(\"symbol\"):\n","        ga = ga.copy()\n","        train_mask = (ga[\"split\"]==\"train\")\n","        # compute stats ignoring NaN\n","        stats = {}\n","        for col in std_cols:\n","            mu = ga.loc[train_mask, col].mean()\n","            sd = ga.loc[train_mask, col].std(ddof=0)\n","            if not pd.notna(sd) or sd <= 0: sd = 1.0\n","            stats[col] = (float(mu if pd.notna(mu) else 0.0), float(sd))\n","        # standardize\n","        for col in std_cols:\n","            mu, sd = stats[col]\n","            ga[col] = (ga[col] - mu) / sd\n","        # sanitize both engineered and z (just in case)\n","        ga = _sanitize_cols(ga, [\"z\"] + std_cols)\n","        out_list.append(ga)\n","\n","    feats_std = pd.concat(out_list, ignore_index=True)\n","    return feats_std\n","\n","# Two feature sets\n","df_price = build_features(df, augmented=False)  # z only, sanitized\n","df_aug   = build_features(df, augmented=True)   # augmented, sanitized\n","\n","# -----------------------------\n","# Sliding-window dataset builder (skip any non-finite window)\n","# -----------------------------\n","def make_xy_features(g: pd.DataFrame, feature_cols: List[str], horizon: int, win: int):\n","    \"\"\"\n","    Returns X, Y, PREV arrays with only finite windows:\n","    X: (N, win, D), Y: (N,1) standardized target z at t+h, PREV: last z in window for DA.\n","    \"\"\"\n","    vals = g[feature_cols].values.astype(np.float32)\n","    z = g[\"z\"].values.astype(np.float32)\n","    x_list, y_list, p_list = [], [], []\n","    dropped = 0\n","    for t in range(win-1, len(vals)-horizon):\n","        x = vals[t-win+1:t+1, :]                 # past-only window\n","        y = z[t+horizon]                         # future standardized close\n","        p = z[t]                                 # last standardized close (for DA)\n","        if not (np.isfinite(x).all() and np.isfinite(y) and np.isfinite(p)):\n","            dropped += 1\n","            continue\n","        x_list.append(x); y_list.append([y]); p_list.append([p])\n","    if dropped > 0:\n","        print(f\"[make_xy_features] Dropped {dropped} windows due to non-finite values (win={win}, h={horizon}).\")\n","    if not x_list:\n","        return None, None, None\n","    return (np.stack(x_list), np.stack(y_list), np.stack(p_list))\n","\n","def build_arm_data(df_feat: pd.DataFrame, horizons, win) -> Tuple[Dict, List[Tuple[str,int]], pd.DataFrame]:\n","    \"\"\"\n","    Build data_xy[(asset, split, h)] = (X,Y,PREV) for feature set df_feat.\n","    Returns (data_xy, arms, coverage_table)\n","    \"\"\"\n","    all_cols = [\"z\",\"dz\",\"vol14\",\"rsi14\",\"macd_z\",\"dist_ma20_z\"]\n","    feature_cols = [c for c in df_feat.columns if c in all_cols]\n","    data_xy = {}\n","    coverage = []\n","    arms = []\n","    for a, g in df_feat.groupby(\"symbol\"):\n","        g = g.sort_values(\"timestamp\")\n","        for split in [\"train\",\"val\",\"test\"]:\n","            gs = g[g[\"split\"]==split].reset_index(drop=True)\n","            for h in horizons:\n","                X,Y,P = make_xy_features(gs, feature_cols, h, win)\n","                if X is not None:\n","                    data_xy[(a,split,h)] = (X,Y,P)\n","                    coverage.append((a,split,h,len(Y)))\n","        # arm is feasible if all splits exist and have >0 samples\n","        for h in horizons:\n","            ok = all(((a,sp,h) in data_xy and data_xy[(a,sp,h)][1].shape[0] > 0) for sp in [\"train\",\"val\",\"test\"])\n","            if ok: arms.append((a,h))\n","    cov_df = pd.DataFrame(coverage, columns=[\"asset\",\"split\",\"horizon\",\"samples\"])\n","    return data_xy, arms, cov_df\n","\n","data_xy_price, arms_price, cov_price = build_arm_data(df_price, CFG.horizons, CFG.win)\n","data_xy_aug,   arms_aug,   cov_aug   = build_arm_data(df_aug,   CFG.horizons, CFG.win)\n","\n","# Ensure we compare on the intersection of feasible arms across feature sets\n","arms_common = sorted(list(set(arms_price).intersection(set(arms_aug))))\n","assert len(arms_common)>0, \"No common feasible arms found.\"\n","print(\"Common feasible arms:\", arms_common)\n","\n","# -----------------------------\n","# Models\n","# -----------------------------\n","class PolicyLSTM(nn.Module):\n","    def __init__(self, input_dim=1, hidden=64, layers=1, dropout=0.0):\n","        super().__init__()\n","        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden, num_layers=layers,\n","                            dropout=(dropout if layers>1 else 0.0), batch_first=True)\n","        self.mu = nn.Sequential(nn.Linear(hidden, hidden), nn.ReLU(), nn.Linear(hidden, 1))\n","        self.log_sigma = nn.Parameter(torch.tensor(-0.5))  # global log-std\n","    def forward(self, x):\n","        o, _ = self.lstm(x)\n","        last = o[:, -1, :]\n","        mu = self.mu(last)\n","        log_sigma = self.log_sigma.expand_as(mu)\n","        # final safety: replace any NaN/Inf in mu with zeros (shouldn't happen after sanitization)\n","        mu = torch.nan_to_num(mu, nan=0.0, posinf=0.0, neginf=0.0)\n","        return mu, log_sigma\n","\n","def count_params(m): return sum(p.numel() for p in m.parameters() if p.requires_grad)\n","\n","# -----------------------------\n","# Teacher policies\n","# -----------------------------\n","class UCB1Teacher:\n","    def __init__(self, n_arms, c=1.2):\n","        self.c=c; self.n=np.zeros(n_arms, dtype=np.int64); self.mean=np.zeros(n_arms, dtype=np.float64); self.t=0\n","    def select(self):\n","        self.t+=1\n","        for i in range(len(self.n)):\n","            if self.n[i]==0: return i\n","        ucb = self.mean + self.c*np.sqrt(2.0*math.log(self.t)/self.n)\n","        return int(np.argmax(ucb))\n","    def update(self, i, reward):\n","        self.n[i]+=1\n","        self.mean[i]+= (reward - self.mean[i])/self.n[i]\n","\n","class UniformTeacher:\n","    def __init__(self, n_arms):\n","        self.n = np.zeros(n_arms, dtype=np.int64)\n","        self.mean = np.zeros(n_arms, dtype=np.float64)\n","        self.t=0\n","    def select(self):\n","        self.t+=1\n","        return int(np.random.randint(0,len(self.n)))\n","    def update(self, i, reward):\n","        self.n[i]+=1\n","        self.mean[i]+= (reward - self.mean[i])/self.n[i]\n","\n","# -----------------------------\n","# Metrics\n","# -----------------------------\n","def mae(y, yhat): return float(np.mean(np.abs(y - yhat)))\n","def rmse(y, yhat): return float(np.sqrt(np.mean((y - yhat)**2)))\n","def smape(y, yhat, eps=1e-8):\n","    denom = (np.abs(y) + np.abs(yhat) + eps)/2.0\n","    return float(np.mean(np.abs(y - yhat)/denom) * 100.0)\n","def dir_acc(y_prev, y_true, yhat):\n","    s_true = np.sign(y_true - y_prev); s_pred = np.sign(yhat - y_prev)\n","    return float(np.mean((s_true == s_pred).astype(np.float32)))\n","\n","# -----------------------------\n","# Training utilities\n","# -----------------------------\n","device = torch.device(CFG.device)\n","\n","def sample_minibatch(data_xy, arms, arm_idx, bs):\n","    a,h = arms[arm_idx]\n","    X,Y,P = data_xy[(a,\"train\",h)]\n","    idx = np.random.randint(0,len(Y), size=(min(bs, len(Y)),))\n","    xb = torch.from_numpy(X[idx]).float().to(device)\n","    yb = torch.from_numpy(Y[idx]).float().to(device)\n","    pb = torch.from_numpy(P[idx]).float().to(device)\n","    return xb, yb, pb\n","\n","def eval_val_mse(model, data_xy, arms, arm_idx):\n","    a,h = arms[arm_idx]\n","    X,Y,_ = data_xy[(a,\"val\",h)]\n","    with torch.no_grad():\n","        xb = torch.from_numpy(X).float().to(device)\n","        mu, _ = model(xb)\n","        yhat = mu.squeeze(1).cpu().numpy()\n","    return float(np.mean((Y.squeeze(1) - yhat)**2))\n","\n","def bandit_run(run_name: str, data_xy, arms, input_dim: int, teacher_type=\"ucb1\", feat_tag=\"price\"):\n","    \"\"\"\n","    Train REINFORCE+aux with either UCB1 or Uniform teacher.\n","    Returns per-arm test results, selection stats, logs, and figures.\n","    \"\"\"\n","    model = PolicyLSTM(input_dim=input_dim, hidden=CFG.lstm_hidden, layers=CFG.lstm_layers, dropout=CFG.lstm_dropout).to(device)\n","    opt = torch.optim.AdamW(model.parameters(), lr=CFG.lr)\n","    if teacher_type==\"ucb1\":\n","        teacher = UCB1Teacher(len(arms), c=CFG.ucb_c)\n","    else:\n","        teacher = UniformTeacher(len(arms))\n","\n","    # Warm start: one pass\n","    def reinforce_step(xb, yb):\n","        model.train(); opt.zero_grad()\n","        mu, log_sigma = model(xb)\n","        sigma = torch.exp(log_sigma)\n","        dist = torch.distributions.Normal(mu, sigma)\n","        a = dist.rsample()\n","        r = - (a - yb)**2\n","        # simple baseline via moving average\n","        reinforce_step.baseline = 0.9*reinforce_step.baseline + 0.1*r.mean().detach() if hasattr(reinforce_step,\"baseline\") else r.mean().detach()\n","        adv = r - reinforce_step.baseline\n","        loss = - (dist.log_prob(a) * adv.detach()).mean()\n","        loss += - CFG.entropy_beta * dist.entropy().mean()\n","        loss += CFG.aux_sup_weight * nn.MSELoss()(mu, yb)\n","        loss.backward()\n","        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        opt.step()\n","\n","    for i in range(len(arms)):\n","        xb,yb,_ = sample_minibatch(data_xy, arms, i, CFG.batch_size)\n","        reinforce_step(xb,yb)\n","\n","    # Main teacher loop\n","    log = {\"round\":[], \"arm\":[], \"asset\":[], \"horizon\":[], \"reward_lp\":[], \"val_before\":[], \"val_after\":[], \"mean_reward_est\":[], \"pulls_arm\":[]}\n","    for t in range(1, CFG.teacher_rounds+1):\n","        i = teacher.select()\n","        a,h = arms[i]\n","        vb = eval_val_mse(model, data_xy, arms, i)\n","        for _ in range(CFG.train_steps_per_pull):\n","            xb,yb,_ = sample_minibatch(data_xy, arms, i, CFG.batch_size)\n","            reinforce_step(xb,yb)\n","        va = eval_val_mse(model, data_xy, arms, i)\n","        rw = float(vb - va)\n","        teacher.update(i, rw)\n","\n","        log[\"round\"].append(t); log[\"arm\"].append(i); log[\"asset\"].append(a); log[\"horizon\"].append(h)\n","        log[\"reward_lp\"].append(rw); log[\"val_before\"].append(vb); log[\"val_after\"].append(va)\n","        log[\"mean_reward_est\"].append(float(teacher.mean[i])); log[\"pulls_arm\"].append(int(teacher.n[i]))\n","\n","    # Save logs\n","    log_df = pd.DataFrame(log)\n","    log_csv = os.path.join(TBL_DIR, f\"ablate_{run_name}_{feat_tag}_teacherlog.csv\")\n","    log_df.to_csv(log_csv, index=False)\n","\n","    # Selection counts\n","    sel_df = (log_df.groupby([\"arm\",\"asset\",\"horizon\"]).size().reset_index(name=\"pulls\").sort_values(\"arm\"))\n","    sel_df[\"mean_reward\"] = [float(teacher.mean[int(i)]) for i in sel_df[\"arm\"].values]\n","    sel_csv = os.path.join(TBL_DIR, f\"ablate_{run_name}_{feat_tag}_selcounts.csv\")\n","    sel_df.to_csv(sel_csv, index=False)\n","\n","    # Plots: selections, rolling reward, regret, mean reward\n","    def savefig(fname):\n","        p = os.path.join(FIG_DIR, fname); plt.tight_layout(); plt.savefig(p, dpi=150); plt.close(); return p\n","\n","    labels = [f\"{a}-H{h}\" for (a,h) in arms]\n","    plt.figure(figsize=(8,3)); plt.plot(log_df[\"arm\"].values, lw=0.8)\n","    plt.yticks(range(len(labels)), labels); plt.xlabel(\"Round\"); plt.ylabel(\"Arm\")\n","    plt.title(f\"{run_name} ({feat_tag}) arm selection over time\")\n","    f_arm_time = savefig(f\"ablate_{run_name}_{feat_tag}_arm_over_time.png\")\n","\n","    plt.figure(figsize=(6,3))\n","    plt.bar(range(len(labels)), sel_df[\"pulls\"].values)\n","    plt.xticks(range(len(labels)), labels, rotation=45, ha=\"right\")\n","    plt.ylabel(\"Pulls\"); plt.title(f\"{run_name} ({feat_tag}) selection counts\")\n","    f_sel_counts = savefig(f\"ablate_{run_name}_{feat_tag}_selcounts.png\")\n","\n","    plt.figure(figsize=(7,3))\n","    plt.plot(pd.Series(log_df[\"reward_lp\"]).rolling(10).mean()); plt.axhline(0.0, color=\"black\", lw=1)\n","    plt.title(f\"{run_name} ({feat_tag}) rolling mean Δ val MSE\"); plt.xlabel(\"Round\"); plt.ylabel(\"Reward\")\n","    f_reward = savefig(f\"ablate_{run_name}_{feat_tag}_reward_curve.png\")\n","\n","    inst_regret = []\n","    for r in range(len(log_df)):\n","        means_snapshot = log_df.iloc[:r+1].groupby(\"arm\")[\"mean_reward_est\"].last().to_dict()\n","        best_mean = max(means_snapshot.values()) if means_snapshot else 0.0\n","        inst_regret.append(best_mean - log_df.iloc[r][\"reward_lp\"])\n","    regret = np.cumsum(inst_regret)\n","    plt.figure(figsize=(7,3)); plt.plot(regret); plt.title(f\"{run_name} ({feat_tag}) cumulative regret\"); plt.xlabel(\"Round\"); plt.ylabel(\"Regret\")\n","    f_regret = savefig(f\"ablate_{run_name}_{feat_tag}_regret.png\")\n","\n","    # Model report\n","    params = count_params(model)\n","    ms = torch_summary(model, input_size=(1, CFG.win, input_dim), verbose=0)\n","    with open(os.path.join(TBL_DIR, f\"ablate_{run_name}_{feat_tag}_model_summary.txt\"), \"w\") as f:\n","        f.write(str(ms)); f.write(f\"\\nTotal trainable parameters: {params}\\n\")\n","    if TORCHVIZ_OK:\n","        try:\n","            xdummy = torch.randn(1, CFG.win, input_dim).to(device)\n","            mu,_ = model(xdummy)\n","            dot = make_dot(mu, params=dict(list(model.named_parameters())))\n","            dot.render(os.path.join(FIG_DIR, f\"ablate_{run_name}_{feat_tag}_graph\"), format=\"png\", cleanup=True)\n","        except Exception as e:\n","            print(\"torchviz failed:\", e)\n","\n","    # Test evaluation (deterministic: use mu)\n","    results = []\n","    for i,(a,h) in enumerate(arms):\n","        Xte,Yte,Prev = data_xy[(a,\"test\",h)]\n","        with torch.no_grad():\n","            xb = torch.from_numpy(Xte).float().to(device)\n","            mu,_ = model(xb)\n","            yhat = mu.squeeze(1).cpu().numpy()\n","        y_true = Yte.squeeze(1); y_prev = Prev.squeeze(1)\n","\n","        results.append(dict(run=run_name, features=feat_tag, asset=a, horizon=h,\n","                            MAE=mae(y_true, yhat), RMSE=rmse(y_true, yhat),\n","                            sMAPE=smape(y_true, yhat), DA=dir_acc(y_prev, y_true, yhat)))\n","        # Plot trace & residuals\n","        Nplot = min(CFG.eval_n_trace, len(y_true))\n","        plt.figure(figsize=(8,3.5))\n","        plt.plot(y_true[:Nplot], label=\"true\"); plt.plot(yhat[:Nplot], label=\"pred\")\n","        plt.title(f\"{run_name} ({feat_tag}) — Test trace {a}, H={h}\")\n","        plt.xlabel(\"Index\"); plt.ylabel(\"Standardized close\"); plt.legend()\n","        _ = os.path.join(FIG_DIR, f\"ablate_{run_name}_{feat_tag}_trace_{a}_H{h}.png\"); plt.tight_layout(); plt.savefig(_, dpi=150); plt.close()\n","\n","        plt.figure(figsize=(6,3))\n","        resid = y_true - yhat\n","        plt.hist(resid, bins=40)\n","        plt.title(f\"{run_name} ({feat_tag}) — Residuals {a}, H={h}\")\n","        plt.xlabel(\"Residual\"); plt.ylabel(\"Count\")\n","        _ = os.path.join(FIG_DIR, f\"ablate_{run_name}_{feat_tag}_resid_{a}_H{h}.png\"); plt.tight_layout(); plt.savefig(_, dpi=150); plt.close()\n","\n","    res_df = pd.DataFrame(results).sort_values([\"horizon\",\"asset\"])\n","    res_csv = os.path.join(TBL_DIR, f\"ablate_{run_name}_{feat_tag}_results_per_arm.csv\")\n","    res_df.to_csv(res_csv, index=False)\n","\n","    # Aggregates by horizon\n","    agg_df = (res_df.groupby(\"horizon\")\n","              .agg(MAE=(\"MAE\",\"mean\"), RMSE=(\"RMSE\",\"mean\"), sMAPE=(\"sMAPE\",\"mean\"), DA=(\"DA\",\"mean\"))\n","              .reset_index().sort_values(\"horizon\"))\n","    agg_csv = os.path.join(TBL_DIR, f\"ablate_{run_name}_{feat_tag}_results_agg_by_horizon.csv\")\n","    agg_df.to_csv(agg_csv, index=False)\n","\n","    # Horizon-wise barplots\n","    def barplot_metric(df, metric, fname, title):\n","        plt.figure(figsize=(6.8,3.8))\n","        plt.bar([f\"H={h}\" for h in df[\"horizon\"]], df[metric].values)\n","        plt.ylabel(metric); plt.title(title);\n","        p = os.path.join(FIG_DIR, fname); plt.tight_layout(); plt.savefig(p, dpi=150); plt.close(); return p\n","\n","    p_mae = barplot_metric(agg_df, \"MAE\",   f\"ablate_{run_name}_{feat_tag}_mae_bars.png\",   f\"{run_name} ({feat_tag}) MAE by horizon\")\n","    p_rmse= barplot_metric(agg_df, \"RMSE\",  f\"ablate_{run_name}_{feat_tag}_rmse_bars.png\",  f\"{run_name} ({feat_tag}) RMSE by horizon\")\n","    p_sm  = barplot_metric(agg_df, \"sMAPE\", f\"ablate_{run_name}_{feat_tag}_smape_bars.png\", f\"{run_name} ({feat_tag}) sMAPE by horizon\")\n","    p_da  = barplot_metric(agg_df, \"DA\",    f\"ablate_{run_name}_{feat_tag}_da_bars.png\",    f\"{run_name} ({feat_tag}) Directional Accuracy by horizon\")\n","\n","    outputs = {\n","        \"log_csv\": log_csv, \"sel_csv\": sel_csv, \"res_csv\": res_csv, \"agg_csv\": agg_csv,\n","        \"figs\": [f_arm_time, f_sel_counts, f_reward, f_regret, p_mae, p_rmse, p_sm, p_da]\n","    }\n","    return outputs, res_df, agg_df, sel_df\n","\n","def supervised_run(run_name: str, data_xy, arms, input_dim: int, feat_tag=\"aug\"):\n","    \"\"\"\n","    No-curriculum pooled supervised training (pure MSE) over all arms combined.\n","    \"\"\"\n","    model = PolicyLSTM(input_dim=input_dim, hidden=CFG.lstm_hidden, layers=CFG.lstm_layers, dropout=CFG.lstm_dropout).to(device)\n","    opt = torch.optim.AdamW(model.parameters(), lr=CFG.lr)\n","    loss_fn = nn.MSELoss()\n","\n","    # Build pooled train/val loaders\n","    Xtr = []; Ytr = []\n","    for a,h in arms:\n","        X,Y,_ = data_xy[(a,\"train\",h)]\n","        Xtr.append(X); Ytr.append(Y)\n","    Xtr = torch.from_numpy(np.concatenate(Xtr, axis=0)).float().to(device)\n","    Ytr = torch.from_numpy(np.concatenate(Ytr, axis=0)).float().to(device)\n","\n","    Xva = []; Yva = []\n","    for a,h in arms:\n","        X,Y,_ = data_xy[(a,\"val\",h)]\n","        Xva.append(X); Yva.append(Y)\n","    Xva = torch.from_numpy(np.concatenate(Xva, axis=0)).float().to(device)\n","    Yva = torch.from_numpy(np.concatenate(Yva, axis=0)).float().to(device)\n","\n","    # Simple epoch loop\n","    best = (1e9, None)\n","    for ep in range(1, CFG.epochs_supervised+1):\n","        model.train()\n","        # mini-batch SGD\n","        idx = torch.randperm(Xtr.shape[0])\n","        for start in range(0, len(idx), CFG.batch_size):\n","            sel = idx[start:start+CFG.batch_size]\n","            xb, yb = Xtr[sel], Ytr[sel]\n","            opt.zero_grad()\n","            mu,_ = model(xb)\n","            loss = loss_fn(mu, yb)\n","            loss.backward()\n","            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","            opt.step()\n","        model.eval()\n","        with torch.no_grad():\n","            mu,_ = model(Xva)\n","            va = loss_fn(mu, Yva).item()\n","        if va < best[0]:\n","            best = (va, {k:v.cpu().clone() for k,v in model.state_dict().items()})\n","    if best[1] is not None:\n","        model.load_state_dict(best[1])\n","\n","    # Model report\n","    params = count_params(model)\n","    ms = torch_summary(model, input_size=(1, CFG.win, input_dim), verbose=0)\n","    with open(os.path.join(TBL_DIR, f\"ablate_{run_name}_{feat_tag}_model_summary.txt\"), \"w\") as f:\n","        f.write(str(ms)); f.write(f\"\\nTotal trainable parameters: {params}\\n\")\n","    if TORCHVIZ_OK:\n","        try:\n","            xdummy = torch.randn(1, CFG.win, input_dim).to(device)\n","            mu,_ = model(xdummy)\n","            dot = make_dot(mu, params=dict(list(model.named_parameters())))\n","            dot.render(os.path.join(FIG_DIR, f\"ablate_{run_name}_{feat_tag}_graph\"), format=\"png\", cleanup=True)\n","        except Exception as e:\n","            print(\"torchviz failed:\", e)\n","\n","    # Test evaluation per arm\n","    results = []\n","    for a,h in arms:\n","        Xte,Yte,Prev = data_xy[(a,\"test\",h)]\n","        with torch.no_grad():\n","            xb = torch.from_numpy(Xte).float().to(device)\n","            mu,_ = model(xb)\n","            yhat = mu.squeeze(1).cpu().numpy()\n","        y_true = Yte.squeeze(1); y_prev = Prev.squeeze(1)\n","        results.append(dict(run=run_name, features=feat_tag, asset=a, horizon=h,\n","                            MAE=mae(y_true, yhat), RMSE=rmse(y_true, yhat),\n","                            sMAPE=smape(y_true, yhat), DA=dir_acc(y_prev, y_true, yhat)))\n","    res_df = pd.DataFrame(results).sort_values([\"horizon\",\"asset\"])\n","    res_csv = os.path.join(TBL_DIR, f\"ablate_{run_name}_{feat_tag}_results_per_arm.csv\")\n","    res_df.to_csv(res_csv, index=False)\n","\n","    agg_df = (res_df.groupby(\"horizon\")\n","              .agg(MAE=(\"MAE\",\"mean\"), RMSE=(\"RMSE\",\"mean\"), sMAPE=(\"sMAPE\",\"mean\"), DA=(\"DA\",\"mean\"))\n","              .reset_index().sort_values(\"horizon\"))\n","    agg_csv = os.path.join(TBL_DIR, f\"ablate_{run_name}_{feat_tag}_results_agg_by_horizon.csv\")\n","    agg_df.to_csv(agg_csv, index=False)\n","\n","    # Horizon-wise bars\n","    def barplot(df, metric, fname, title):\n","        plt.figure(figsize=(6.8,3.8))\n","        plt.bar([f\"H={h}\" for h in df[\"horizon\"]], df[metric].values)\n","        plt.ylabel(metric); plt.title(title)\n","        p = os.path.join(FIG_DIR, fname); plt.tight_layout(); plt.savefig(p, dpi=150); plt.close(); return p\n","    p_mae = barplot(agg_df, \"MAE\",   f\"ablate_{run_name}_{feat_tag}_mae_bars.png\",   f\"{run_name} ({feat_tag}) MAE by horizon\")\n","    p_rmse= barplot(agg_df, \"RMSE\",  f\"ablate_{run_name}_{feat_tag}_rmse_bars.png\",  f\"{run_name} ({feat_tag}) RMSE by horizon\")\n","    p_sm  = barplot(agg_df, \"sMAPE\", f\"ablate_{run_name}_{feat_tag}_smape_bars.png\", f\"{run_name} ({feat_tag}) sMAPE by horizon\")\n","    p_da  = barplot(agg_df, \"DA\",    f\"ablate_{run_name}_{feat_tag}_da_bars.png\",    f\"{run_name} ({feat_tag}) Directional Accuracy by horizon\")\n","\n","    outputs = {\"res_csv\":res_csv, \"agg_csv\":agg_csv,\n","               \"figs\":[p_mae,p_rmse,p_sm,p_da]}\n","    return outputs, res_df, agg_df\n","\n","# -----------------------------\n","# Run the four variants\n","# -----------------------------\n","# Variant 1: ACG-UCB1 + price-only\n","input_dim_price = 1\n","out_v1, res_v1, agg_v1, sel_v1 = bandit_run(\"ACG_UCB1\", data_xy_price, arms_common, input_dim_price, teacher_type=\"ucb1\", feat_tag=\"price\")\n","\n","# Variant 2: ACG-UCB1 + augmented\n","input_dim_aug = len([c for c in df_aug.columns if c in [\"z\",\"dz\",\"vol14\",\"rsi14\",\"macd_z\",\"dist_ma20_z\"]])\n","out_v2, res_v2, agg_v2, sel_v2 = bandit_run(\"ACG_UCB1\", data_xy_aug, arms_common, input_dim_aug, teacher_type=\"ucb1\", feat_tag=\"aug\")\n","\n","# Variant 3: Uniform + augmented\n","out_v3, res_v3, agg_v3, sel_v3 = bandit_run(\"UNIFORM\", data_xy_aug, arms_common, input_dim_aug, teacher_type=\"uniform\", feat_tag=\"aug\")\n","\n","# Variant 4: No-curriculum supervised (pooled MSE) + augmented\n","out_v4, res_v4, agg_v4 = supervised_run(\"SUPERVISED\", data_xy_aug, arms_common, input_dim_aug, feat_tag=\"aug\")\n","\n","# -----------------------------\n","# Cross-variant comparison tables & plots\n","# -----------------------------\n","# Merge aggregates by horizon for plotting\n","agg_v1[\"run\"]=\"ACG-UCB1(price)\"; agg_v2[\"run\"]=\"ACG-UCB1(aug)\"\n","agg_v3[\"run\"]=\"UNIFORM(aug)\";    agg_v4[\"run\"]=\"SUPERVISED(aug)\"\n","agg_all = pd.concat([agg_v1,agg_v2,agg_v3,agg_v4], ignore_index=True)\n","\n","agg_all_csv = os.path.join(TBL_DIR, \"ablate_agg_horizon_cross_variant.csv\")\n","agg_all.to_csv(agg_all_csv, index=False)\n","\n","# Grouped bars per metric\n","def grouped_bars(df, metric, fname, title):\n","    plt.figure(figsize=(8.8,4.2))\n","    horizons = sorted(df[\"horizon\"].unique())\n","    runs = [\"ACG-UCB1(price)\",\"ACG-UCB1(aug)\",\"UNIFORM(aug)\",\"SUPERVISED(aug)\"]\n","    width = 0.18; idx = np.arange(len(horizons))\n","    for i, r in enumerate(runs):\n","        sub = df[df[\"run\"]==r].set_index(\"horizon\").reindex(horizons)\n","        plt.bar(idx + i*width, sub[metric].values, width=width, label=r)\n","    plt.xticks(idx + width*1.5, [f\"H={h}\" for h in horizons])\n","    plt.ylabel(metric); plt.title(title); plt.legend()\n","    p = os.path.join(FIG_DIR, fname); plt.tight_layout(); plt.savefig(p, dpi=150); plt.close(); return p\n","\n","g_mae  = grouped_bars(agg_all, \"MAE\",   \"ablate_cross_variant_mae.png\",   \"MAE by horizon — cross-variant\")\n","g_rmse = grouped_bars(agg_all, \"RMSE\",  \"ablate_cross_variant_rmse.png\",  \"RMSE by horizon — cross-variant\")\n","g_sm   = grouped_bars(agg_all, \"sMAPE\", \"ablate_cross_variant_smape.png\", \"sMAPE by horizon — cross-variant\")\n","g_da   = grouped_bars(agg_all, \"DA\",    \"ablate_cross_variant_da.png\",    \"Directional Accuracy by horizon — cross-variant\")\n","\n","# -----------------------------\n","# Print summary & figure inventory\n","# -----------------------------\n","def head_table(df, n=10):\n","    try: return df.head(n).to_string(index=False)\n","    except: return str(df.head(n))\n","\n","print(\"\\n=== §4.5 RESULTS SUMMARY (paste into chat) ===\")\n","print(\"Common arms evaluated:\", arms_common)\n","print(\"\\nAggregates by horizon (ACG-UCB1 price-only):\", out_v1[\"agg_csv\"])\n","print(head_table(agg_v1, 10))\n","print(\"\\nAggregates by horizon (ACG-UCB1 augmented):\", out_v2[\"agg_csv\"])\n","print(head_table(agg_v2, 10))\n","print(\"\\nAggregates by horizon (UNIFORM augmented):\", out_v3[\"agg_csv\"])\n","print(head_table(agg_v3, 10))\n","print(\"\\nAggregates by horizon (SUPERVISED augmented):\", out_v4[\"agg_csv\"])\n","print(head_table(agg_v4, 10))\n","print(\"\\nCross-variant aggregate CSV:\", agg_all_csv)\n","print(head_table(agg_all, 12))\n","\n","print(\"\\nSelection counts (ACG-UCB1 price-only):\", out_v1[\"sel_csv\"])\n","print(sel_v1.to_string(index=False))\n","print(\"\\nSelection counts (ACG-UCB1 augmented):\", out_v2[\"sel_csv\"])\n","print(sel_v2.to_string(index=False))\n","print(\"\\nSelection counts (UNIFORM augmented):\", out_v3[\"sel_csv\"])\n","print(sel_v3.to_string(index=False))\n","\n","# Figure inventory\n","figs = sorted([os.path.join(FIG_DIR, f) for f in os.listdir(FIG_DIR) if f.endswith(\".png\")])\n","print(\"\\n=== FIGURES SAVED (first 40) ===\")\n","for p in figs[:40]:\n","    print(p)\n","if len(figs) > 40:\n","    print(f\"... and {len(figs)-40} more figures\")\n","\n","print(\"\\nArtifacts:\")\n","print(\" - V1 logs:\", out_v1[\"log_csv\"])\n","print(\" - V2 logs:\", out_v2[\"log_csv\"])\n","print(\" - V3 logs:\", out_v3[\"log_csv\"])\n","print(\" - Per-arm results CSVs:\",\n","      out_v1[\"res_csv\"], out_v2[\"res_csv\"], out_v3[\"res_csv\"], out_v4[\"res_csv\"])\n","print(\" - Cross-variant aggregate CSV:\", agg_all_csv)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BPu6kj6_s0W0","executionInfo":{"status":"aborted","timestamp":1761543222822,"user_tz":-180,"elapsed":56350,"user":{"displayName":"David Kibaara","userId":"07870126442739887481"}}},"outputs":[],"source":["# === Chapter 4 §4.6: Comparative Analysis Experiment (CLEAN ONE-CELL) ===\n","# Purpose:\n","# - Load per-arm results from baselines (§4.2) and ACG-DRL (§4.3)\n","# - Run paired Wilcoxon tests ACG vs {LSTM, SMA, Naive} per horizon\n","# - Run Friedman+Nemenyi across {ACG, LSTM, SMA, Naive} per horizon\n","# - Plot horizon-wise boxplots and a Critical Difference (CD) diagram\n","# - (Optional) Denormalize MAE to price-scale using train-split std per asset\n","#\n","# Inputs expected (already produced by earlier cells):\n","#   /content/export/tables/baseline_results_per_asset_horizon.csv\n","#   /content/export/tables/acg_results_per_arm.csv\n","#   /content/export/tables/dataset_long_1D.csv  (for optional denormalization)\n","#\n","# Outputs:\n","#   Tables:\n","#     /content/export/tables/compare_per_arm_joined.csv\n","#     /content/export/tables/wilcoxon_acg_vs_baselines_by_horizon.csv\n","#     /content/export/tables/friedman_nemenyi_by_horizon.csv\n","#   Figures:\n","#     /content/export/figures/comp_boxplot_mae_H{1,3,7}.png\n","#     /content/export/figures/comp_boxplot_rmse_H{1,3,7}.png\n","#     /content/export/figures/comp_cd_diagram_H{1,3,7}.png\n","#     /content/export/figures/comp_scatter_da_vs_mae_diff_H{1,3,7}.png\n","\n","import os, json, math\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Stats\n","try:\n","    import scipy.stats as stats\n","except Exception:\n","    !pip -q install scipy > /dev/null\n","    import scipy.stats as stats\n","\n","# -----------------------------\n","# Paths\n","# -----------------------------\n","BASE_DIR = \"/content\"\n","TBL_DIR  = f\"{BASE_DIR}/export/tables\"\n","FIG_DIR  = f\"{BASE_DIR}/export/figures\"\n","for d in [TBL_DIR, FIG_DIR]: os.makedirs(d, exist_ok=True)\n","\n","CSV_BASE = f\"{TBL_DIR}/baseline_results_per_asset_horizon.csv\"\n","CSV_ACG  = f\"{TBL_DIR}/acg_results_per_arm.csv\"\n","CSV_DS   = f\"{BASE_DIR}/export/tables/dataset_long_1D.csv\"\n","\n","assert os.path.exists(CSV_BASE), f\"Missing {CSV_BASE}\"\n","assert os.path.exists(CSV_ACG),  f\"Missing {CSV_ACG}\"\n","assert os.path.exists(CSV_DS),   f\"Missing {CSV_DS}\"\n","\n","# -----------------------------\n","# Load data\n","# -----------------------------\n","b = pd.read_csv(CSV_BASE)  # columns: model,asset,horizon,MAE,RMSE,sMAPE,DA\n","a = pd.read_csv(CSV_ACG)   # columns: model,asset,horizon,MAE,RMSE,sMAPE,DA\n","a[\"model\"] = \"ACG-DRL\"     # normalize name\n","\n","# Keep the 4 models we compare directly\n","keep_models = {\"ACG-DRL\",\"LSTM\",\"SMA\",\"Naive\"}\n","b = b[b[\"model\"].isin(keep_models - {\"ACG-DRL\"})].copy()\n","a = a[a[\"model\"].isin({\"ACG-DRL\"})].copy()\n","\n","# Optional: compute train-split std(close) per asset for denormalization\n","df_all = pd.read_csv(CSV_DS, parse_dates=[\"timestamp\"])\n","std_train = (df_all[df_all[\"split\"]==\"train\"]\n","             .groupby(\"symbol\")[\"close\"].std(ddof=0).rename(\"std_close_train\"))\n","std_train = std_train.replace({0:1.0}).fillna(1.0)  # safety\n","\n","# -----------------------------\n","# Build joined per-arm table for horizons H in {1,3,7}\n","# -----------------------------\n","def join_per_arm(h):\n","    acg_h = a[a[\"horizon\"]==h][[\"asset\",\"horizon\",\"MAE\",\"RMSE\",\"sMAPE\",\"DA\"]].rename(\n","        columns={\"MAE\":\"MAE_ACG\",\"RMSE\":\"RMSE_ACG\",\"sMAPE\":\"sMAPE_ACG\",\"DA\":\"DA_ACG\"})\n","    rows = []\n","    for m in [\"LSTM\",\"SMA\",\"Naive\"]:\n","        bm = b[(b[\"horizon\"]==h) & (b[\"model\"]==m)][[\"asset\",\"horizon\",\"MAE\",\"RMSE\",\"sMAPE\",\"DA\"]].rename(\n","            columns={\"MAE\":f\"MAE_{m}\", \"RMSE\":f\"RMSE_{m}\", \"sMAPE\":f\"sMAPE_{m}\", \"DA\":f\"DA_{m}\"})\n","        rows.append(bm)\n","    base_wide = rows[0]\n","    for r in rows[1:]:\n","        base_wide = base_wide.merge(r, on=[\"asset\",\"horizon\"], how=\"inner\")\n","    joined = base_wide.merge(acg_h, on=[\"asset\",\"horizon\"], how=\"inner\")\n","    # attach std for denorm\n","    joined = joined.merge(std_train, left_on=\"asset\", right_index=True, how=\"left\")\n","    joined[\"std_close_train\"] = joined[\"std_close_train\"].fillna(1.0)\n","    return joined\n","\n","J = {h: join_per_arm(h) for h in [1,3,7]}\n","\n","all_joined = pd.concat([J[1],J[3],J[7]], ignore_index=True)\n","all_joined.to_csv(f\"{TBL_DIR}/compare_per_arm_joined.csv\", index=False)\n","\n","print(\"Joined per-arm rows by horizon:\", {h: len(J[h]) for h in J})\n","print(\"Saved:\", f\"{TBL_DIR}/compare_per_arm_joined.csv\")\n","\n","# -----------------------------\n","# Helper: effect sizes and Wilcoxon\n","# -----------------------------\n","def paired_wilcoxon(x, y, alternative=\"two-sided\"):\n","    # returns statistic, p-value, n_eff, median_diff, cohen_d (paired)\n","    # x=ACG, y=baseline → we often test if ACG has LOWER MAE than baseline\n","    diff = y - x\n","    diff = diff[np.isfinite(diff)]\n","    n = diff.size\n","    if n == 0:\n","        return np.nan, np.nan, 0, np.nan, np.nan\n","    try:\n","        stat, p = stats.wilcoxon(x, y, zero_method=\"pratt\", alternative=alternative)\n","    except Exception:\n","        stat, p = np.nan, np.nan\n","    md = float(np.median(diff))\n","    # paired Cohen's d\n","    d = float(np.mean(diff) / (np.std(diff, ddof=1) + 1e-12))\n","    return stat, p, n, md, d\n","\n","# -----------------------------\n","# Paired tests: ACG vs {LSTM, SMA, Naive}\n","# -----------------------------\n","rows = []\n","for h in [1,3,7]:\n","    dfh = J[h].copy()\n","    # Tests on MAE (lower is better): test alternative='greater' on (baseline - ACG) > 0\n","    for m in [\"LSTM\",\"SMA\",\"Naive\"]:\n","        stat, p, n, md, d = paired_wilcoxon(dfh[\"MAE_ACG\"].values, dfh[f\"MAE_{m}\"].values, alternative=\"less\")\n","        rows.append(dict(horizon=h, metric=\"MAE\", baseline=m, n=n, median_diff=(dfh[f\"MAE_{m}\"]-dfh[\"MAE_ACG\"]).median(),\n","                         wilcoxon_stat=stat, p_value=p, cohen_d=d))\n","        stat, p, n, md, d = paired_wilcoxon(dfh[\"RMSE_ACG\"].values, dfh[f\"RMSE_{m}\"].values, alternative=\"less\")\n","        rows.append(dict(horizon=h, metric=\"RMSE\", baseline=m, n=n, median_diff=(dfh[f\"RMSE_{m}\"]-dfh[\"RMSE_ACG\"]).median(),\n","                         wilcoxon_stat=stat, p_value=p, cohen_d=d))\n","        # Directional Accuracy: higher is better for ACG → test alternative='greater' on (ACG - baseline)\n","        stat, p, n, md, d = paired_wilcoxon(dfh[\"DA_ACG\"].values, dfh[f\"DA_{m}\"].values, alternative=\"greater\")\n","        rows.append(dict(horizon=h, metric=\"DA\", baseline=m, n=n, median_diff=(dfh[\"DA_ACG\"]-dfh[f\"DA_{m}\"]).median(),\n","                         wilcoxon_stat=stat, p_value=p, cohen_d=d))\n","\n","wilcoxon_df = pd.DataFrame(rows)\n","wilcoxon_csv = f\"{TBL_DIR}/wilcoxon_acg_vs_baselines_by_horizon.csv\"\n","wilcoxon_df.to_csv(wilcoxon_csv, index=False)\n","print(\"Saved Wilcoxon summary:\", wilcoxon_csv)\n","print(wilcoxon_df.head(12).to_string(index=False))\n","\n","# -----------------------------\n","# Friedman + Nemenyi ranks per horizon\n","# -----------------------------\n","def avg_ranks(values_2d, model_names):\n","    # values_2d: N_datasets x K_models, smaller is better (MAE)\n","    # return avg ranks (1=best)\n","    ranks = []\n","    for row in values_2d:\n","        r = stats.rankdata(row, method=\"average\")  # 1=lowest\n","        ranks.append(r)\n","    ranks = np.array(ranks)\n","    # Ensure 1=best: rankdata already assigns 1 to smallest\n","    avg = ranks.mean(axis=0)\n","    return dict(zip(model_names, avg)), ranks\n","\n","def nemenyi_cd(k, N, alpha=0.05):\n","    # Critical value q_alpha for Nemenyi, infinite df approx\n","    # For alpha=0.05:\n","    q_table = {2:1.960, 3:2.343, 4:2.569, 5:2.728, 6:2.850, 7:2.948}\n","    q = q_table.get(k, 2.569)\n","    cd = q * math.sqrt(k*(k+1)/(6.0*N))\n","    return cd\n","\n","def plot_cd(avg_ranks_dict, cd, title, savepath, reverse=False):\n","    # Draw a simple CD diagram (Demsar-style)\n","    models = list(avg_ranks_dict.keys())\n","    ranks = np.array([avg_ranks_dict[m] for m in models])\n","    order = np.argsort(ranks)  # ascending rank\n","    models = [models[i] for i in order]\n","    ranks = ranks[order]\n","    min_r, max_r = min(ranks)-0.5, max(ranks)+0.5\n","\n","    plt.figure(figsize=(8, 1.8))\n","    y = 0.5\n","    # horizontal axis: ranks\n","    plt.hlines(y, min_r, max_r, color=\"black\")\n","    for r in np.arange(math.floor(min_r), math.ceil(max_r)+1):\n","        plt.vlines(r, y-0.05, y+0.05, color=\"black\")\n","        plt.text(r, y+0.12, f\"{r:.0f}\", ha=\"center\", va=\"bottom\", fontsize=9)\n","\n","    # draw model ticks\n","    for m, r in zip(models, ranks):\n","        plt.vlines(r, y-0.05, y+0.05, color=\"black\")\n","        plt.text(r, y-0.20, m, ha=\"center\", va=\"top\", fontsize=10, rotation=0)\n","\n","    # CD bar (right side)\n","    cd_left = max_r - cd\n","    plt.hlines(y+0.30, cd_left, max_r, color=\"black\", linewidth=2)\n","    plt.vlines(cd_left, y+0.25, y+0.35, color=\"black\")\n","    plt.vlines(max_r, y+0.25, y+0.35, color=\"black\")\n","    plt.text((cd_left+max_r)/2, y+0.38, f\"CD = {cd:.2f}\", ha=\"center\", va=\"bottom\", fontsize=9)\n","\n","    plt.title(title, fontsize=11)\n","    plt.yticks([]); plt.ylim(0, 1.0); plt.xlim(min_r, max_r)\n","    plt.tight_layout(); plt.savefig(savepath, dpi=150); plt.close()\n","\n","friedman_rows = []\n","for h in [1,3,7]:\n","    dfh = J[h].copy()\n","    # assemble N x K matrix for MAE\n","    models = [\"ACG-DRL\",\"LSTM\",\"SMA\",\"Naive\"]\n","    mat = np.vstack([\n","        dfh[\"MAE_ACG\"].values,\n","        dfh[\"MAE_LSTM\"].values,\n","        dfh[\"MAE_SMA\"].values,\n","        dfh[\"MAE_Naive\"].values\n","    ]).T\n","    # Remove rows with any non-finite\n","    mask = np.isfinite(mat).all(axis=1)\n","    mat = mat[mask]\n","    N = mat.shape[0]\n","    if N < 2:\n","        continue\n","    # Friedman\n","    try:\n","        F_stat, F_p = stats.friedmanchisquare(*[mat[:,i] for i in range(mat.shape[1])])\n","    except Exception:\n","        F_stat, F_p = np.nan, np.nan\n","    # Average ranks\n","    avg_r, _ = avg_ranks(mat, models)\n","    cd = nemenyi_cd(k=len(models), N=N, alpha=0.05)\n","    friedman_rows.append(dict(horizon=h, N=N, friedman_stat=F_stat, friedman_p=F_p, cd=cd, **{f\"rank_{m}\":avg_r[m] for m in models}))\n","    # Plot CD diagram\n","    plot_cd(avg_r, cd, title=f\"Critical Difference (MAE) — H={h} (N={N})\",\n","            savepath=f\"{FIG_DIR}/comp_cd_diagram_H{h}.png\")\n","\n","friedman_df = pd.DataFrame(friedman_rows)\n","friedman_csv = f\"{TBL_DIR}/friedman_nemenyi_by_horizon.csv\"\n","friedman_df.to_csv(friedman_csv, index=False)\n","print(\"Saved Friedman+Nemenyi summary:\", friedman_csv)\n","print(friedman_df.to_string(index=False))\n","\n","# -----------------------------\n","# Boxplots and DA vs MAE diff scatter\n","# -----------------------------\n","def savefig(p): plt.tight_layout(); plt.savefig(p, dpi=150); plt.close(); return p\n","\n","box_figs = []\n","scatter_figs = []\n","for h in [1,3,7]:\n","    dfh = J[h].copy()\n","    if len(dfh)==0: continue\n","    # Boxplot MAE\n","    data = [dfh[\"MAE_ACG\"].values, dfh[\"MAE_LSTM\"].values, dfh[\"MAE_SMA\"].values, dfh[\"MAE_Naive\"].values]\n","    labels = [\"ACG-DRL\",\"LSTM\",\"SMA\",\"Naive\"]\n","    plt.figure(figsize=(6.5,3.6))\n","    plt.boxplot(data, labels=labels, showmeans=True)\n","    plt.ylabel(\"MAE (standardized)\"); plt.title(f\"MAE by model — H={h}\")\n","    box_figs.append(savefig(f\"{FIG_DIR}/comp_boxplot_mae_H{h}.png\"))\n","\n","    # Boxplot RMSE\n","    data = [dfh[\"RMSE_ACG\"].values, dfh[\"RMSE_LSTM\"].values, dfh[\"RMSE_SMA\"].values, dfh[\"RMSE_Naive\"].values]\n","    plt.figure(figsize=(6.5,3.6))\n","    plt.boxplot(data, labels=labels, showmeans=True)\n","    plt.ylabel(\"RMSE (standardized)\"); plt.title(f\"RMSE by model — H={h}\")\n","    box_figs.append(savefig(f\"{FIG_DIR}/comp_boxplot_rmse_H{h}.png\"))\n","\n","    # Scatter: DA diff (ACG - baseline) vs. MAE diff (baseline - ACG) for LSTM\n","    x = (dfh[\"MAE_LSTM\"] - dfh[\"MAE_ACG\"]).values\n","    y = (dfh[\"DA_ACG\"] - dfh[\"DA_LSTM\"]).values\n","    plt.figure(figsize=(5.5,3.6))\n","    plt.axvline(0, color=\"k\", lw=1); plt.axhline(0, color=\"k\", lw=1)\n","    plt.scatter(x, y, s=18)\n","    plt.xlabel(\"ΔMAE (LSTM − ACG)  [>0 → ACG better]\")\n","    plt.ylabel(\"ΔDA  (ACG − LSTM)  [>0 → ACG better]\")\n","    plt.title(f\"Trade-off: Direction vs. Magnitude — H={h}\")\n","    scatter_figs.append(savefig(f\"{FIG_DIR}/comp_scatter_da_vs_mae_diff_H{h}.png\"))\n","\n","print(\"\\n=== §4.6 COMPARATIVE EXPERIMENT SUMMARY ===\")\n","print(\"Wilcoxon table :\", wilcoxon_csv)\n","print(\"Friedman table :\", friedman_csv)\n","print(\"Boxplots saved :\", \", \".join(os.path.basename(p) for p in box_figs))\n","print(\"CD diagrams    :\", \", \".join([f\"comp_cd_diagram_H{h}.png\" for h in [1,3,7]]))\n","print(\"Scatter plots  :\", \", \".join(os.path.basename(p) for p in scatter_figs))\n","\n","# -----------------------------\n","# Optional: price-scale MAE (denormalized) per horizon\n","# -----------------------------\n","denorm_rows = []\n","for h in [1,3,7]:\n","    dfh = J[h].copy()\n","    if len(dfh)==0: continue\n","    for m, col in [(\"ACG-DRL\",\"MAE_ACG\"),(\"LSTM\",\"MAE_LSTM\"),(\"SMA\",\"MAE_SMA\"),(\"Naive\",\"MAE_Naive\")]:\n","        tmp = dfh[[\"asset\", col, \"std_close_train\"]].copy()\n","        tmp[\"MAE_price\"] = tmp[col] * tmp[\"std_close_train\"]\n","        denorm_rows.append(tmp.assign(horizon=h, model=m)[[\"horizon\",\"model\",\"asset\",\"MAE_price\"]])\n","denorm_df = pd.concat(denorm_rows, ignore_index=True)\n","denorm_csv = f\"{TBL_DIR}/compare_denorm_mae_price_per_arm.csv\"\n","denorm_df.to_csv(denorm_csv, index=False)\n","print(\"Denormalized MAE (price-scale) saved:\", denorm_csv)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XirjtA-19NqK","executionInfo":{"status":"aborted","timestamp":1761543222824,"user_tz":-180,"elapsed":56351,"user":{"displayName":"David Kibaara","userId":"07870126442739887481"}}},"outputs":[],"source":["# === Chapter 4 §4.7: Sensitivity & Hyperparameter Studies (CLEAN ONE-CELL) ===\n","# Covers:\n","#  4.7.1 Seeds & early stopping (protocol)\n","#  4.7.2 Teacher sensitivity: UCB1 c, rounds, train-steps-per-pull\n","#  4.7.3 Student sensitivity: hidden, layers, dropout, learning rate\n","#  4.7.4 Window length w and horizon H dependence\n","#  4.7.5 Feature set sensitivity: price-only vs. augmented\n","#  4.7.6 Data coverage sensitivity: assets_max, sparse/downsampled train windows\n","#\n","# Inputs:\n","#   /content/export/tables/dataset_long_1D.csv\n","#\n","# Outputs (examples):\n","#   Tables:\n","#     /content/export/tables/sens_471_seeds.csv\n","#     /content/export/tables/sens_472_teacher.csv\n","#     /content/export/tables/sens_473_student_hidden.csv\n","#     /content/export/tables/sens_473_student_layers.csv\n","#     /content/export/tables/sens_473_student_dropout.csv\n","#     /content/export/tables/sens_473_student_lr.csv\n","#     /content/export/tables/sens_474_win.csv\n","#     /content/export/tables/sens_475_features.csv\n","#     /content/export/tables/sens_476_coverage_assetsmax.csv\n","#     /content/export/tables/sens_476_coverage_downsample.csv\n","#   Figures (examples):\n","#     /content/export/figures/sens_472_ucb_c_vs_mae.png\n","#     /content/export/figures/sens_472_rounds_vs_mae.png\n","#     /content/export/figures/sens_472_steps_vs_mae.png\n","#     /content/export/figures/sens_473_hidden_vs_mae.png\n","#     /content/export/figures/sens_473_layers_vs_mae.png\n","#     /content/export/figures/sens_473_dropout_vs_mae.png\n","#     /content/export/figures/sens_473_lr_vs_mae.png\n","#     /content/export/figures/sens_474_w_by_horizon_mae.png\n","#     /content/export/figures/sens_475_feature_set_bars.png\n","#     /content/export/figures/sens_476_assetsmax_bars.png\n","#     /content/export/figures/sens_476_downsample_line.png\n","\n","import os, json, math, random, warnings\n","from dataclasses import dataclass, asdict\n","from typing import Dict, Tuple, List\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","\n","# -----------------------------\n","# Paths & folders\n","# -----------------------------\n","BASE_DIR = \"/content\"\n","DS_CSV   = f\"{BASE_DIR}/export/tables/dataset_long_1D.csv\"\n","TBL_DIR  = f\"{BASE_DIR}/export/tables\"\n","FIG_DIR  = f\"{BASE_DIR}/export/figures\"\n","for d in [TBL_DIR, FIG_DIR]: os.makedirs(d, exist_ok=True)\n","\n","assert os.path.exists(DS_CSV), f\"Dataset not found: {DS_CSV}\"\n","\n","# -----------------------------\n","# Config (baseline + sweep grids)\n","# -----------------------------\n","@dataclass\n","class SensCfg:\n","    ds_csv: str = DS_CSV\n","    assets_max_base: int = 5\n","    horizons_base: Tuple[int,...] = (1,3,7)\n","    win_base: int = 64\n","    batch_size: int = 256\n","    teacher_rounds_base: int = 80\n","    train_steps_per_pull_base: int = 1\n","    ucb_c_base: float = 1.2\n","    seed_base: int = 1337\n","    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    lstm_hidden_base: int = 64\n","    lstm_layers_base: int = 1\n","    lstm_dropout_base: float = 0.0\n","    lr_base: float = 2e-3\n","    eval_n_trace: int = 200\n","    patience_rounds: int = 15     # early stopping on global val MSE\n","    eval_every: int = 10\n","    # sweep values\n","    seeds: Tuple[int,...] = (42, 1337, 2024)\n","    ucb_c_vals: Tuple[float,...] = (0.6, 1.2, 2.0)\n","    rounds_vals: Tuple[int,...] = (60, 120)\n","    steps_vals: Tuple[int,...] = (1, 2)\n","    hidden_vals: Tuple[int,...] = (32, 64)\n","    layers_vals: Tuple[int,...] = (1, 2)\n","    dropout_vals: Tuple[float,...] = (0.0, 0.2)\n","    lr_vals: Tuple[float,...] = (1e-3, 2e-3)\n","    win_vals: Tuple[int,...] = (32, 64, 128)\n","    feature_sets: Tuple[str,...] = (\"price\",\"aug\")\n","    assets_max_vals: Tuple[int,...] = (3, 5)\n","    downsample_fracs: Tuple[float,...] = (1.0, 0.5, 0.25)\n","\n","CFG = SensCfg()\n","print(\"§4.7 Config\\n\", json.dumps(asdict(CFG), indent=2))\n","\n","# -----------------------------\n","# Utilities\n","# -----------------------------\n","def set_seed(s):\n","    random.seed(s); np.random.seed(s); torch.manual_seed(s);\n","    if torch.cuda.is_available(): torch.cuda.manual_seed_all(s)\n","\n","device = torch.device(CFG.device)\n","\n","def savefig(path):\n","    plt.tight_layout(); plt.savefig(path, dpi=150); plt.close(); return path\n","\n","# -----------------------------\n","# Load dataset and build features (sanitize; train-only standardization)\n","# -----------------------------\n","df_all = pd.read_csv(CFG.ds_csv, parse_dates=[\"timestamp\"])\n","assert {\"timestamp\",\"symbol\",\"close\",\"split\"}.issubset(df_all.columns), \"Dataset missing required columns.\"\n","\n","# Select top assets by coverage (max cap applied per sweep)\n","def top_assets(df, k):\n","    return (df.groupby(\"symbol\").size().sort_values(ascending=False).head(k).index.tolist())\n","\n","# Train-split close z-score per asset\n","def fit_close_scalers(df, assets):\n","    scalers = {}\n","    for a, g in df[(df[\"split\"]==\"train\") & (df[\"symbol\"].isin(assets))].groupby(\"symbol\"):\n","        mu, sd = g[\"close\"].mean(), g[\"close\"].std(ddof=0)\n","        scalers[a] = (float(mu), float(sd if sd>0 else 1.0))\n","    return scalers\n","\n","def build_features(df, assets, augmented: bool):\n","    df = df[df[\"symbol\"].isin(assets)].sort_values([\"symbol\",\"timestamp\"]).copy()\n","    scalers = fit_close_scalers(df, assets)\n","    df[\"z\"] = df.apply(lambda r: (r[\"close\"] - scalers.get(r[\"symbol\"], (0.0,1.0))[0]) / scalers.get(r[\"symbol\"], (0.0,1.0))[1], axis=1)\n","\n","    if not augmented:\n","        out = df[[\"timestamp\",\"symbol\",\"split\",\"z\"]].copy()\n","        out[\"z\"] = out[\"z\"].replace([np.inf,-np.inf], np.nan).fillna(0.0).clip(-10,10)\n","        return out\n","\n","    # augmented (past-only)\n","    def rsi(series: pd.Series, period: int = 14) -> pd.Series:\n","        delta = series.diff()\n","        up = delta.clip(lower=0.0)\n","        down = -delta.clip(upper=0.0)\n","        roll_up = up.rolling(period, min_periods=period).mean()\n","        roll_down = down.rolling(period, min_periods=period).mean()\n","        rs = roll_up / (roll_down + 1e-12)\n","        return 100.0 - (100.0 / (1.0 + rs))\n","\n","    feats = []\n","    for a, g in df.groupby(\"symbol\"):\n","        g = g.sort_values(\"timestamp\").copy()\n","        z = g[\"z\"].astype(float)\n","        dz = z.diff()\n","        vol14 = dz.rolling(14, min_periods=14).std()\n","        rsi14 = rsi(g[\"close\"], 14)\n","        ma5, ma20 = z.rolling(5, min_periods=5).mean(), z.rolling(20, min_periods=20).mean()\n","        macd_z = ma5 - ma20\n","        dist_ma20_z = z - ma20\n","        tmp = pd.DataFrame({\n","            \"timestamp\": g[\"timestamp\"].values,\n","            \"symbol\": a,\n","            \"split\": g[\"split\"].values,\n","            \"z\": z.values, \"dz\": dz.values, \"vol14\": vol14.values,\n","            \"rsi14\": rsi14.values, \"macd_z\": macd_z.values, \"dist_ma20_z\": dist_ma20_z.values\n","        })\n","        # standardize engineered feats using TRAIN stats only (per asset)\n","        std_cols = [\"dz\",\"vol14\",\"rsi14\",\"macd_z\",\"dist_ma20_z\"]\n","        train_mask = (tmp[\"split\"]==\"train\")\n","        for c in std_cols:\n","            mu = tmp.loc[train_mask, c].mean()\n","            sd = tmp.loc[train_mask, c].std(ddof=0)\n","            if not pd.notna(sd) or sd<=0: sd=1.0\n","            mu = 0.0 if not pd.notna(mu) else mu\n","            tmp[c] = (tmp[c] - mu) / sd\n","        # sanitize\n","        tmp[[\"z\"]+std_cols] = tmp[[\"z\"]+std_cols].replace([np.inf,-np.inf], np.nan).fillna(0.0).clip(-10,10)\n","        feats.append(tmp)\n","    return pd.concat(feats, ignore_index=True)\n","\n","def make_xy_features(g: pd.DataFrame, feature_cols: List[str], horizon: int, win: int):\n","    vals = g[feature_cols].values.astype(np.float32)\n","    z = g[\"z\"].values.astype(np.float32)\n","    X,Y,Prev = [], [], []\n","    dropped = 0\n","    for t in range(win-1, len(vals)-horizon):\n","        x = vals[t-win+1:t+1, :]\n","        y = z[t+horizon]\n","        p = z[t]\n","        if not (np.isfinite(x).all() and np.isfinite(y) and np.isfinite(p)):\n","            dropped += 1; continue\n","        X.append(x); Y.append([y]); Prev.append([p])\n","    if dropped>0: print(f\"[make_xy_features] Dropped {dropped} windows (win={win}, h={horizon}).\")\n","    if not X: return None, None, None\n","    return np.stack(X), np.stack(Y), np.stack(Prev)\n","\n","def build_arm_data(df_feat: pd.DataFrame, horizons, win) -> Tuple[Dict, List[Tuple[str,int]], pd.DataFrame]:\n","    all_cols = [\"z\",\"dz\",\"vol14\",\"rsi14\",\"macd_z\",\"dist_ma20_z\"]\n","    feature_cols = [c for c in df_feat.columns if c in all_cols]\n","    data_xy, arms, coverage = {}, [], []\n","    for a, g in df_feat.groupby(\"symbol\"):\n","        g = g.sort_values(\"timestamp\")\n","        for split in [\"train\",\"val\",\"test\"]:\n","            gs = g[g[\"split\"]==split].reset_index(drop=True)\n","            for h in horizons:\n","                X,Y,P = make_xy_features(gs, feature_cols, h, win)\n","                if X is not None:\n","                    data_xy[(a,split,h)] = (X,Y,P)\n","                    coverage.append((a,split,h,len(Y)))\n","        for h in horizons:\n","            ok = all(((a,sp,h) in data_xy and data_xy[(a,sp,h)][1].shape[0]>0) for sp in [\"train\",\"val\",\"test\"])\n","            if ok: arms.append((a,h))\n","    cov_df = pd.DataFrame(coverage, columns=[\"asset\",\"split\",\"horizon\",\"samples\"])\n","    return data_xy, arms, cov_df\n","\n","def downsample_train(data_xy: Dict, arms: List[Tuple[str,int]], frac: float):\n","    if frac>=0.999: return data_xy\n","    out = {}\n","    rng = np.random.default_rng(123)\n","    for k,(a,split,h) in zip(list(data_xy.keys()), list(data_xy.keys())):\n","        X,Y,P = data_xy[k]\n","        if split==\"train\":\n","            n = len(Y); m = max(1, int(round(n*frac)))\n","            idx = rng.choice(n, size=m, replace=False)\n","            out[k] = (X[idx], Y[idx], P[idx])\n","        else:\n","            out[k] = (X,Y,P)\n","    return out\n","\n","# -----------------------------\n","# Models & training\n","# -----------------------------\n","class PolicyLSTM(nn.Module):\n","    def __init__(self, input_dim=1, hidden=64, layers=1, dropout=0.0):\n","        super().__init__()\n","        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden, num_layers=layers,\n","                            dropout=(dropout if layers>1 else 0.0), batch_first=True)\n","        self.mu = nn.Sequential(nn.Linear(hidden, hidden), nn.ReLU(), nn.Linear(hidden, 1))\n","        self.log_sigma = nn.Parameter(torch.tensor(-0.5))\n","    def forward(self, x):\n","        o,_ = self.lstm(x)\n","        last = o[:, -1, :]\n","        mu = self.mu(last)\n","        log_sigma = self.log_sigma.expand_as(mu)\n","        mu = torch.nan_to_num(mu, nan=0.0, posinf=0.0, neginf=0.0)\n","        return mu, log_sigma\n","\n","def count_params(m): return sum(p.numel() for p in m.parameters() if p.requires_grad)\n","\n","class UCB1Teacher:\n","    def __init__(self, n_arms, c=1.2):\n","        self.c=c; self.n=np.zeros(n_arms, dtype=np.int64); self.mean=np.zeros(n_arms, dtype=np.float64); self.t=0\n","    def select(self):\n","        self.t+=1\n","        for i in range(len(self.n)):\n","            if self.n[i]==0: return i\n","        ucb = self.mean + self.c*np.sqrt(2.0*math.log(self.t)/self.n)\n","        return int(np.argmax(ucb))\n","    def update(self, i, reward):\n","        self.n[i]+=1\n","        self.mean[i]+= (reward - self.mean[i])/self.n[i]\n","\n","class UniformTeacher:\n","    def __init__(self, n_arms):\n","        self.n = np.zeros(n_arms, dtype=np.int64); self.mean = np.zeros(n_arms, dtype=np.float64); self.t=0\n","    def select(self):\n","        self.t+=1\n","        return int(np.random.randint(0,len(self.n)))\n","    def update(self, i, reward):\n","        self.n[i]+=1\n","        self.mean[i]+= (reward - self.mean[i])/self.n[i]\n","\n","# metrics\n","def mae(y, yhat): return float(np.mean(np.abs(y - yhat)))\n","def rmse(y, yhat): return float(np.sqrt(np.mean((y - yhat)**2)))\n","def smape(y, yhat, eps=1e-8):\n","    denom = (np.abs(y) + np.abs(yhat) + eps)/2.0\n","    return float(np.mean(np.abs(y - yhat)/denom) * 100.0)\n","def dir_acc(y_prev, y_true, yhat):\n","    s_true = np.sign(y_true - y_prev); s_pred = np.sign(yhat - y_prev)\n","    return float(np.mean((s_true == s_pred).astype(np.float32)))\n","\n","def sample_minibatch(data_xy, arms, arm_idx, bs):\n","    a,h = arms[arm_idx]\n","    X,Y,P = data_xy[(a,\"train\",h)]\n","    if len(Y)==0:\n","        # very rare edge; fallback tiny batch\n","        idx = np.array([0])\n","    else:\n","        idx = np.random.randint(0,len(Y), size=(min(bs, len(Y)),))\n","    xb = torch.from_numpy(X[idx]).float().to(device)\n","    yb = torch.from_numpy(Y[idx]).float().to(device)\n","    pb = torch.from_numpy(P[idx]).float().to(device)\n","    return xb, yb, pb\n","\n","def eval_val_mse(model, data_xy, arms, arm_idx):\n","    a,h = arms[arm_idx]\n","    X,Y,_ = data_xy[(a,\"val\",h)]\n","    with torch.no_grad():\n","        xb = torch.from_numpy(X).float().to(device)\n","        mu, _ = model(xb)\n","        yhat = mu.squeeze(1).cpu().numpy()\n","    return float(np.mean((Y.squeeze(1) - yhat)**2))\n","\n","def eval_overall_val_mse(model, data_xy, arms):\n","    vals = []\n","    for i,_ in enumerate(arms):\n","        vals.append(eval_val_mse(model, data_xy, arms, i))\n","    return float(np.mean(vals)) if vals else np.nan\n","\n","def evaluate_test(model, data_xy, arms):\n","    # returns per-arm and horizon aggregates\n","    rows = []\n","    for (a,h) in arms:\n","        Xte,Yte,Prev = data_xy[(a,\"test\",h)]\n","        with torch.no_grad():\n","            xb = torch.from_numpy(Xte).float().to(device)\n","            mu,_ = model(xb)\n","            yhat = mu.squeeze(1).cpu().numpy()\n","        y = Yte.squeeze(1); p = Prev.squeeze(1)\n","        rows.append(dict(asset=a, horizon=h, MAE=mae(y,yhat), RMSE=rmse(y,yhat),\n","                         sMAPE=smape(y,yhat), DA=dir_acc(p,y,yhat)))\n","    per_arm = pd.DataFrame(rows).sort_values([\"horizon\",\"asset\"])\n","    agg = (per_arm.groupby(\"horizon\").agg(MAE=(\"MAE\",\"mean\"), RMSE=(\"RMSE\",\"mean\"),\n","                                          sMAPE=(\"sMAPE\",\"mean\"), DA=(\"DA\",\"mean\"))\n","           .reset_index().sort_values(\"horizon\"))\n","    return per_arm, agg\n","\n","def bandit_run_once(seed, df_feat, arms, data_xy, *,\n","                    hidden, layers, dropout, lr,\n","                    ucb_c, rounds, steps_per_pull,\n","                    batch_size=CFG.batch_size, patience=CFG.patience_rounds, eval_every=CFG.eval_every):\n","    set_seed(seed)\n","    input_dim = len([c for c in df_feat.columns if c in [\"z\",\"dz\",\"vol14\",\"rsi14\",\"macd_z\",\"dist_ma20_z\"]])\n","    model = PolicyLSTM(input_dim=input_dim, hidden=hidden, layers=layers, dropout=dropout).to(device)\n","    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n","    teacher = UCB1Teacher(len(arms), c=ucb_c)\n","\n","    def reinforce_step(xb, yb):\n","        model.train(); opt.zero_grad()\n","        mu, log_sigma = model(xb)\n","        sigma = torch.exp(log_sigma)\n","        dist = torch.distributions.Normal(mu, sigma)\n","        a = dist.rsample()\n","        r = - (a - yb)**2\n","        reinforce_step.baseline = 0.9*reinforce_step.baseline + 0.1*r.mean().detach() if hasattr(reinforce_step,\"baseline\") else r.mean().detach()\n","        adv = r - reinforce_step.baseline\n","        loss = - (dist.log_prob(a) * adv.detach()).mean()\n","        loss += - 1e-3 * dist.entropy().mean()\n","        loss += 0.1 * nn.MSELoss()(mu, yb)\n","        loss.backward()\n","        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        opt.step()\n","\n","    # warm\n","    for i in range(min(len(arms), 3)):\n","        xb,yb,_ = sample_minibatch(data_xy, arms, i, batch_size)\n","        reinforce_step(xb,yb)\n","\n","    best = (np.inf, None, 0)  # (global_val, state_dict, rounds_used)\n","    no_improve = 0\n","    for t in range(1, rounds+1):\n","        i = teacher.select()\n","        vb = eval_val_mse(model, data_xy, arms, i)\n","        for _ in range(steps_per_pull):\n","            xb,yb,_ = sample_minibatch(data_xy, arms, i, batch_size)\n","            reinforce_step(xb,yb)\n","        va = eval_val_mse(model, data_xy, arms, i)\n","        teacher.update(i, float(vb - va))\n","\n","        if t % eval_every == 0:\n","            gval = eval_overall_val_mse(model, data_xy, arms)\n","            if gval < best[0]:\n","                best = (gval, {k:v.cpu().clone() for k,v in model.state_dict().items()}, t)\n","                no_improve = 0\n","            else:\n","                no_improve += 1\n","            if no_improve >= patience:\n","                break\n","\n","    rounds_used = best[2] if best[1] is not None else rounds\n","    if best[1] is not None: model.load_state_dict(best[1])\n","\n","    per_arm, agg = evaluate_test(model, data_xy, arms)\n","    per_arm[\"rounds_used\"] = rounds_used\n","    agg[\"rounds_used\"] = rounds_used\n","    agg[\"params\"] = count_params(model)\n","    return per_arm, agg\n","\n","# -----------------------------\n","# Helper to prepare data per feature set / assets_max / win\n","# -----------------------------\n","def prepare_data(feature_set=\"price\", assets_max=CFG.assets_max_base, win=CFG.win_base, horizons=CFG.horizons_base):\n","    assets = top_assets(df_all, assets_max)\n","    df_feat = build_features(df_all, assets, augmented=(feature_set==\"aug\"))\n","    data_xy, arms, cov = build_arm_data(df_feat, horizons, win)\n","    return df_feat, data_xy, arms, cov\n","\n","# Baseline data for most sweeps (price-only)\n","df_price, data_xy_price, arms_price, cov_price = prepare_data(\"price\", CFG.assets_max_base, CFG.win_base, CFG.horizons_base)\n","assert len(arms_price)>0, \"No feasible arms for baseline (price).\"\n","# Augmented too (for §4.7.5)\n","df_aug, data_xy_aug, arms_aug, cov_aug = prepare_data(\"aug\", CFG.assets_max_base, CFG.win_base, CFG.horizons_base)\n","arms_common = sorted(list(set(arms_price).intersection(set(arms_aug))))\n","if len(arms_common)==0:\n","    arms_common = arms_price  # fallback\n","\n","print(\"Baseline feasible arms (price):\", arms_price)\n","print(\"Feasible arms (aug)          :\", arms_aug)\n","print(\"Arms used for shared comparisons:\", arms_common)\n","\n","# -----------------------------\n","# 4.7.1 Seeds & early stopping\n","# -----------------------------\n","def sweep_seeds():\n","    rows=[]\n","    for s in CFG.seeds:\n","        per_arm, agg = bandit_run_once(s, df_price, arms_price, data_xy_price,\n","                                       hidden=CFG.lstm_hidden_base, layers=CFG.lstm_layers_base,\n","                                       dropout=CFG.lstm_dropout_base, lr=CFG.lr_base,\n","                                       ucb_c=CFG.ucb_c_base, rounds=CFG.teacher_rounds_base,\n","                                       steps_per_pull=CFG.train_steps_per_pull_base)\n","        agg[\"seed\"]=s; rows.append(agg)\n","    out = pd.concat(rows, ignore_index=True)\n","    out.to_csv(f\"{TBL_DIR}/sens_471_seeds.csv\", index=False)\n","    # plot variance across seeds (MAE by horizon)\n","    plt.figure(figsize=(7.2,4))\n","    for h in CFG.horizons_base:\n","        tmp = out[out[\"horizon\"]==h]\n","        plt.plot(tmp[\"seed\"].astype(str), tmp[\"MAE\"].values, marker=\"o\", label=f\"H={h}\")\n","    plt.xlabel(\"Seed\"); plt.ylabel(\"MAE\"); plt.title(\"Seed sensitivity (MAE)\"); plt.legend()\n","    savefig(f\"{FIG_DIR}/sens_471_seeds_mae.png\")\n","    return out\n","\n","# -----------------------------\n","# 4.7.2 Teacher sensitivity (c, rounds, steps)\n","# -----------------------------\n","def sweep_teacher():\n","    rows=[]\n","    # c\n","    for c in CFG.ucb_c_vals:\n","        _, agg = bandit_run_once(CFG.seed_base, df_price, arms_price, data_xy_price,\n","                                 hidden=CFG.lstm_hidden_base, layers=CFG.lstm_layers_base,\n","                                 dropout=CFG.lstm_dropout_base, lr=CFG.lr_base,\n","                                 ucb_c=c, rounds=CFG.teacher_rounds_base,\n","                                 steps_per_pull=CFG.train_steps_per_pull_base)\n","        agg[\"sweep\"]=\"ucb_c\"; agg[\"value\"]=c; rows.append(agg)\n","    # rounds\n","    for r in CFG.rounds_vals:\n","        _, agg = bandit_run_once(CFG.seed_base, df_price, arms_price, data_xy_price,\n","                                 hidden=CFG.lstm_hidden_base, layers=CFG.lstm_layers_base,\n","                                 dropout=CFG.lstm_dropout_base, lr=CFG.lr_base,\n","                                 ucb_c=CFG.ucb_c_base, rounds=r,\n","                                 steps_per_pull=CFG.train_steps_per_pull_base)\n","        agg[\"sweep\"]=\"rounds\"; agg[\"value\"]=r; rows.append(agg)\n","    # steps\n","    for sp in CFG.steps_vals:\n","        _, agg = bandit_run_once(CFG.seed_base, df_price, arms_price, data_xy_price,\n","                                 hidden=CFG.lstm_hidden_base, layers=CFG.lstm_layers_base,\n","                                 dropout=CFG.lstm_dropout_base, lr=CFG.lr_base,\n","                                 ucb_c=CFG.ucb_c_base, rounds=CFG.teacher_rounds_base,\n","                                 steps_per_pull=sp)\n","        agg[\"sweep\"]=\"steps\"; agg[\"value\"]=sp; rows.append(agg)\n","    out = pd.concat(rows, ignore_index=True)\n","    out.to_csv(f\"{TBL_DIR}/sens_472_teacher.csv\", index=False)\n","    # figures\n","    def lineplot(sub, title, fname):\n","        plt.figure(figsize=(6.8,3.8))\n","        for h in CFG.horizons_base:\n","            tmp = sub[sub[\"horizon\"]==h].sort_values(\"value\")\n","            plt.plot(tmp[\"value\"].astype(float), tmp[\"MAE\"].values, marker=\"o\", label=f\"H={h}\")\n","        plt.xlabel(sub[\"sweep\"].iloc[0]); plt.ylabel(\"MAE\"); plt.title(title); plt.legend()\n","        savefig(f\"{FIG_DIR}/{fname}\")\n","    lineplot(out[out[\"sweep\"]==\"ucb_c\"], \"Teacher UCB c vs. MAE\", \"sens_472_ucb_c_vs_mae.png\")\n","    lineplot(out[out[\"sweep\"]==\"rounds\"], \"Teacher rounds vs. MAE\", \"sens_472_rounds_vs_mae.png\")\n","    lineplot(out[out[\"sweep\"]==\"steps\"],  \"Train-steps-per-pull vs. MAE\", \"sens_472_steps_vs_mae.png\")\n","    return out\n","\n","# -----------------------------\n","# 4.7.3 Student sensitivity (one-factor sweeps)\n","# -----------------------------\n","def sweep_student():\n","    outs=[]\n","    # hidden\n","    rows=[]\n","    for hdim in CFG.hidden_vals:\n","        _, agg = bandit_run_once(CFG.seed_base, df_price, arms_price, data_xy_price,\n","                                 hidden=hdim, layers=CFG.lstm_layers_base,\n","                                 dropout=CFG.lstm_dropout_base, lr=CFG.lr_base,\n","                                 ucb_c=CFG.ucb_c_base, rounds=CFG.teacher_rounds_base,\n","                                 steps_per_pull=CFG.train_steps_per_pull_base)\n","        agg[\"sweep\"]=\"hidden\"; agg[\"value\"]=hdim; rows.append(agg)\n","    out_h = pd.concat(rows, ignore_index=True); out_h.to_csv(f\"{TBL_DIR}/sens_473_student_hidden.csv\", index=False); outs.append(out_h)\n","\n","    # layers\n","    rows=[]\n","    for L in CFG.layers_vals:\n","        _, agg = bandit_run_once(CFG.seed_base, df_price, arms_price, data_xy_price,\n","                                 hidden=CFG.lstm_hidden_base, layers=L,\n","                                 dropout=CFG.lstm_dropout_base, lr=CFG.lr_base,\n","                                 ucb_c=CFG.ucb_c_base, rounds=CFG.teacher_rounds_base,\n","                                 steps_per_pull=CFG.train_steps_per_pull_base)\n","        agg[\"sweep\"]=\"layers\"; agg[\"value\"]=L; rows.append(agg)\n","    out_L = pd.concat(rows, ignore_index=True); out_L.to_csv(f\"{TBL_DIR}/sens_473_student_layers.csv\", index=False); outs.append(out_L)\n","\n","    # dropout\n","    rows=[]\n","    for dp in CFG.dropout_vals:\n","        _, agg = bandit_run_once(CFG.seed_base, df_price, arms_price, data_xy_price,\n","                                 hidden=CFG.lstm_hidden_base, layers=CFG.lstm_layers_base,\n","                                 dropout=dp, lr=CFG.lr_base,\n","                                 ucb_c=CFG.ucb_c_base, rounds=CFG.teacher_rounds_base,\n","                                 steps_per_pull=CFG.train_steps_per_pull_base)\n","        agg[\"sweep\"]=\"dropout\"; agg[\"value\"]=dp; rows.append(agg)\n","    out_D = pd.concat(rows, ignore_index=True); out_D.to_csv(f\"{TBL_DIR}/sens_473_student_dropout.csv\", index=False); outs.append(out_D)\n","\n","    # lr\n","    rows=[]\n","    for lr in CFG.lr_vals:\n","        _, agg = bandit_run_once(CFG.seed_base, df_price, arms_price, data_xy_price,\n","                                 hidden=CFG.lstm_hidden_base, layers=CFG.lstm_layers_base,\n","                                 dropout=CFG.lstm_dropout_base, lr=lr,\n","                                 ucb_c=CFG.ucb_c_base, rounds=CFG.teacher_rounds_base,\n","                                 steps_per_pull=CFG.train_steps_per_pull_base)\n","        agg[\"sweep\"]=\"lr\"; agg[\"value\"]=lr; rows.append(agg)\n","    out_R = pd.concat(rows, ignore_index=True); out_R.to_csv(f\"{TBL_DIR}/sens_473_student_lr.csv\", index=False); outs.append(out_R)\n","\n","    # plots\n","    def plot_one(sub, label, fname):\n","        plt.figure(figsize=(6.8,3.8))\n","        for h in CFG.horizons_base:\n","            tmp = sub[sub[\"horizon\"]==h].sort_values(\"value\")\n","            plt.plot(tmp[\"value\"].astype(float), tmp[\"MAE\"].values, marker=\"o\", label=f\"H={h}\")\n","        plt.xlabel(label); plt.ylabel(\"MAE\"); plt.title(f\"Student {label} vs. MAE\"); plt.legend()\n","        savefig(f\"{FIG_DIR}/{fname}\")\n","    plot_one(out_h, \"hidden\", \"sens_473_hidden_vs_mae.png\")\n","    plot_one(out_L, \"layers\", \"sens_473_layers_vs_mae.png\")\n","    plot_one(out_D, \"dropout\", \"sens_473_dropout_vs_mae.png\")\n","    plot_one(out_R, \"lr\",      \"sens_473_lr_vs_mae.png\")\n","\n","    return outs\n","\n","# -----------------------------\n","# 4.7.4 Window length w & horizon H\n","# -----------------------------\n","def sweep_window():\n","    rows=[]\n","    for w in CFG.win_vals:\n","        dfp, dxp, armsp, _ = prepare_data(\"price\", CFG.assets_max_base, w, CFG.horizons_base)\n","        _, agg = bandit_run_once(CFG.seed_base, dfp, armsp, dxp,\n","                                 hidden=CFG.lstm_hidden_base, layers=CFG.lstm_layers_base,\n","                                 dropout=CFG.lstm_dropout_base, lr=CFG.lr_base,\n","                                 ucb_c=CFG.ucb_c_base, rounds=CFG.teacher_rounds_base,\n","                                 steps_per_pull=CFG.train_steps_per_pull_base)\n","        agg[\"w\"]=w; rows.append(agg)\n","    out = pd.concat(rows, ignore_index=True); out.to_csv(f\"{TBL_DIR}/sens_474_win.csv\", index=False)\n","    # plot MAE vs w for each horizon\n","    plt.figure(figsize=(7.2,4))\n","    for h in CFG.horizons_base:\n","        tmp = out[out[\"horizon\"]==h].sort_values(\"w\")\n","        plt.plot(tmp[\"w\"], tmp[\"MAE\"], marker=\"o\", label=f\"H={h}\")\n","    plt.xlabel(\"Window length (w)\"); plt.ylabel(\"MAE\"); plt.title(\"Window length vs. MAE by horizon\"); plt.legend()\n","    savefig(f\"{FIG_DIR}/sens_474_w_by_horizon_mae.png\")\n","    return out\n","\n","# -----------------------------\n","# 4.7.5 Feature set sensitivity\n","# -----------------------------\n","def sweep_features():\n","    rows=[]\n","    # price\n","    _, agg_p = bandit_run_once(CFG.seed_base, df_price, arms_common, data_xy_price,\n","                               hidden=CFG.lstm_hidden_base, layers=CFG.lstm_layers_base,\n","                               dropout=CFG.lstm_dropout_base, lr=CFG.lr_base,\n","                               ucb_c=CFG.ucb_c_base, rounds=CFG.teacher_rounds_base,\n","                               steps_per_pull=CFG.train_steps_per_pull_base)\n","    agg_p[\"feature_set\"]=\"price\"; rows.append(agg_p)\n","    # aug\n","    _, agg_a = bandit_run_once(CFG.seed_base, df_aug, arms_common, data_xy_aug,\n","                               hidden=CFG.lstm_hidden_base, layers=CFG.lstm_layers_base,\n","                               dropout=CFG.lstm_dropout_base, lr=CFG.lr_base,\n","                               ucb_c=CFG.ucb_c_base, rounds=CFG.teacher_rounds_base,\n","                               steps_per_pull=CFG.train_steps_per_pull_base)\n","    agg_a[\"feature_set\"]=\"aug\"; rows.append(agg_a)\n","    out = pd.concat(rows, ignore_index=True); out.to_csv(f\"{TBL_DIR}/sens_475_features.csv\", index=False)\n","    # bar plot\n","    plt.figure(figsize=(6.8,3.8))\n","    for h in CFG.horizons_base:\n","        tmp = out[out[\"horizon\"]==h]\n","        x = np.arange(len(tmp[\"feature_set\"].unique()))\n","        vals = [tmp[tmp[\"feature_set\"]==\"price\"][\"MAE\"].values[0],\n","                tmp[tmp[\"feature_set\"]==\"aug\"][\"MAE\"].values[0]]\n","        plt.bar(x + (h-1)*0.25, vals, width=0.22, label=f\"H={h}\")\n","    plt.xticks(x + 0.25, [\"price\",\"aug\"])\n","    plt.ylabel(\"MAE\"); plt.title(\"Feature set sensitivity (MAE)\"); plt.legend()\n","    savefig(f\"{FIG_DIR}/sens_475_feature_set_bars.png\")\n","    return out\n","\n","# -----------------------------\n","# 4.7.6 Data coverage sensitivity\n","#   (i) assets_max sweep (3 vs 5)\n","#   (ii) downsample training windows (1.0, 0.5, 0.25)\n","# -----------------------------\n","def sweep_coverage():\n","    # (i) assets_max\n","    rows=[]\n","    for k in CFG.assets_max_vals:\n","        dfp, dxp, armsp, _ = prepare_data(\"price\", k, CFG.win_base, CFG.horizons_base)\n","        _, agg = bandit_run_once(CFG.seed_base, dfp, armsp, dxp,\n","                                 hidden=CFG.lstm_hidden_base, layers=CFG.lstm_layers_base,\n","                                 dropout=CFG.lstm_dropout_base, lr=CFG.lr_base,\n","                                 ucb_c=CFG.ucb_c_base, rounds=CFG.teacher_rounds_base,\n","                                 steps_per_pull=CFG.train_steps_per_pull_base)\n","        agg[\"assets_max\"]=k; rows.append(agg)\n","    out_assets = pd.concat(rows, ignore_index=True); out_assets.to_csv(f\"{TBL_DIR}/sens_476_coverage_assetsmax.csv\", index=False)\n","    plt.figure(figsize=(6.8,3.8))\n","    for h in CFG.horizons_base:\n","        tmp = out_assets[out_assets[\"horizon\"]==h].sort_values(\"assets_max\")\n","        plt.plot(tmp[\"assets_max\"], tmp[\"MAE\"], marker=\"o\", label=f\"H={h}\")\n","    plt.xlabel(\"assets_max\"); plt.ylabel(\"MAE\"); plt.title(\"assets_max vs. MAE\"); plt.legend()\n","    savefig(f\"{FIG_DIR}/sens_476_assetsmax_bars.png\")\n","\n","    # (ii) downsample training\n","    rows=[]\n","    for frac in CFG.downsample_fracs:\n","        # reuse baseline price data but downsample train\n","        dx = downsample_train(data_xy_price, arms_price, frac)\n","        _, agg = bandit_run_once(CFG.seed_base, df_price, arms_price, dx,\n","                                 hidden=CFG.lstm_hidden_base, layers=CFG.lstm_layers_base,\n","                                 dropout=CFG.lstm_dropout_base, lr=CFG.lr_base,\n","                                 ucb_c=CFG.ucb_c_base, rounds=CFG.teacher_rounds_base,\n","                                 steps_per_pull=CFG.train_steps_per_pull_base)\n","        agg[\"train_frac\"]=frac; rows.append(agg)\n","    out_down = pd.concat(rows, ignore_index=True); out_down.to_csv(f\"{TBL_DIR}/sens_476_coverage_downsample.csv\", index=False)\n","    # figure\n","    plt.figure(figsize=(6.8,3.8))\n","    for h in CFG.horizons_base:\n","        tmp = out_down[out_down[\"horizon\"]==h].sort_values(\"train_frac\")\n","        plt.plot(tmp[\"train_frac\"], tmp[\"MAE\"], marker=\"o\", label=f\"H={h}\")\n","    plt.xlabel(\"Train fraction kept\"); plt.ylabel(\"MAE\"); plt.title(\"Training coverage vs. MAE\"); plt.legend()\n","    savefig(f\"{FIG_DIR}/sens_476_downsample_line.png\")\n","\n","    return out_assets, out_down\n","\n","# -----------------------------\n","# RUN ALL SWEEPS\n","# -----------------------------\n","print(\"\\nRunning §4.7 sweeps... (this is a compact, CPU-friendly run)\")\n","res_471 = sweep_seeds()\n","res_472 = sweep_teacher()\n","res_473_hidden, res_473_layers, res_473_dropout, res_473_lr = sweep_student()\n","res_474 = sweep_window()\n","res_475 = sweep_features()\n","res_476_assets, res_476_down = sweep_coverage()\n","\n","# -----------------------------\n","# PRINT SUMMARY PATHS\n","# -----------------------------\n","print(\"\\n=== §4.7 SENSITIVITY — ARTIFACT SUMMARY ===\")\n","tbls = [f for f in sorted(os.listdir(TBL_DIR)) if f.startswith(\"sens_\")]\n","figs = [f for f in sorted(os.listdir(FIG_DIR)) if f.startswith(\"sens_\") and f.endswith(\".png\")]\n","print(\"Tables:\")\n","for t in tbls: print(\" -\", os.path.join(TBL_DIR, t))\n","print(\"Figures:\")\n","for f in figs[:24]: print(\" -\", os.path.join(FIG_DIR, f))\n","if len(figs)>24: print(f\" ... and {len(figs)-24} more figures\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":56346,"status":"aborted","timestamp":1761543222826,"user":{"displayName":"David Kibaara","userId":"07870126442739887481"},"user_tz":-180},"id":"wGF1SxfpzKOq"},"outputs":[],"source":["import shutil\n","import os\n","from google.colab import files\n","import time # Added for potential future use, but cleanup logic is removed for stability\n","\n","# --- Configuration ---\n","# The folder path you want to download (MUST exist)\n","FOLDER_PATH = \"/content/export\"\n","# The desired name for the downloaded ZIP file (without the .zip extension)\n","ZIP_NAME = \"export_content\"\n","# The final name of the archive file\n","ARCHIVE_PATH = f\"{ZIP_NAME}.zip\"\n","# ---------------------\n","\n","def download_folder(source_folder, output_zip_name):\n","    \"\"\"\n","    Compresses a directory into a ZIP file and triggers a browser download in Colab.\n","\n","    Args:\n","        source_folder (str): The path to the folder to be compressed and downloaded.\n","        output_zip_name (str): The desired name for the final ZIP file (without extension).\n","    \"\"\"\n","    if not os.path.isdir(source_folder):\n","        print(f\"Error: Folder not found at path: {source_folder}\")\n","        print(\"Please ensure the folder is created and populated before running the download.\")\n","        return\n","\n","    # Ensure ARCHIVE_PATH is correctly defined within the function scope or passed\n","    # It relies on the global configuration variables for simplicity here.\n","\n","    try:\n","        print(f\"Starting compression of '{source_folder}'...\")\n","\n","        # 1. Create the ZIP archive\n","        # Determine the parent directory and the base directory name\n","        root_dir = os.path.dirname(source_folder)\n","        base_dir_name = os.path.basename(source_folder)\n","\n","        # Create the ZIP file in the root_dir (usually /content/)\n","        shutil.make_archive(ZIP_NAME, 'zip', root_dir=root_dir, base_dir=base_dir_name)\n","\n","        print(f\"Successfully created archive: {ARCHIVE_PATH}\")\n","\n","        # 2. Trigger the download\n","        print(\"Triggering browser download...\")\n","        files.download(ARCHIVE_PATH)\n","        print(\"Download initiated. Check your browser's download panel.\")\n","\n","        # NOTE: The browser download is asynchronous. We rely on the download\n","        # completing after this script exits or before the cell finishes executing.\n","\n","    except Exception as e:\n","        print(f\"\\nAn error occurred during archiving or downloading: {e}\")\n","    finally:\n","        # --- CRITICAL FIX: Removed automatic cleanup ---\n","        # The cleanup step was causing a race condition, deleting the ZIP file\n","        # before the browser could finish fetching it.\n","        # The user must now manually delete the temporary ZIP file from Colab storage.\n","        print(\"\\nCleanup step skipped to ensure download completion.\")\n","        print(f\"The file '{ARCHIVE_PATH}' remains in the Colab environment until deleted manually.\")\n","\n","\n","# Execute the function\n","# NOTE: The browser download prompt may take a few seconds to appear\n","# after the compression is complete.\n","download_folder(FOLDER_PATH, ZIP_NAME)"]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"1MI3Sg0r71RlimJm2uxigWUzYe6L4DmBj","authorship_tag":"ABX9TyOc2HQiSTZEN/w+m4S8toF7"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}